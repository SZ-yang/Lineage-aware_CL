{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "d1e26884-2230-45af-914e-64211cabd5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFzoXTPCVub",
        "outputId": "e87da9e4-1513-433c-a3ac-67fccc48bbe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "c96fe66c-bb77-4b5b-cdd1-cf784d0ab1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.7 pytorch-lightning-2.4.0 torchmetrics-1.4.2\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
            "Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: array-api-compat, anndata\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.26.4)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.4.2)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.7)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.19.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.17.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (2.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.4.0\n",
            "    Uninstalling pytorch-lightning-2.4.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.4.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.10.9)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.5)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.9)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=f620ecb6bed8d1f0e0d9a09b5a2cd84a81b57885d27c66309f5c049aadc02e2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, session-info, pynndescent, umap-learn, scanpy\n",
            "Successfully installed legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "fcecff6f-cb8e-4f4d-963a-d9a16b1d02cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1728338005.6094522\n",
            "usage: scContrastiveLearning_Main_709_ckpt_epoch.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                                                    [--batch_size BATCH_SIZE]\n",
            "                                                    [--size_factor SIZE_FACTOR]\n",
            "                                                    [--temperature TEMPERATURE]\n",
            "                                                    [--patience PATIENCE] [--min_delta MIN_DELTA]\n",
            "                                                    [--max_epoch MAX_EPOCH] --output_dir\n",
            "                                                    OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                                                    [--hidden_dims HIDDEN_DIMS]\n",
            "                                                    [--embedding_size EMBEDDING_SIZE]\n",
            "                                                    [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3V30WCiJDs9I",
        "outputId": "cff48862-aad2-42a5-e232-20602690dd7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 16:  83% 14460/17473 [02:36<00:32, 92.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14480/17473 [02:36<00:32, 92.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14500/17473 [02:36<00:32, 92.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14520/17473 [02:36<00:31, 92.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14540/17473 [02:36<00:31, 92.76it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14560/17473 [02:36<00:31, 92.82it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  83% 14580/17473 [02:36<00:31, 92.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14600/17473 [02:37<00:30, 92.95it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14620/17473 [02:37<00:30, 93.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14640/17473 [02:37<00:30, 93.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14660/17473 [02:37<00:30, 93.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14680/17473 [02:37<00:29, 93.23it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14700/17473 [02:37<00:29, 93.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14720/17473 [02:37<00:29, 93.37it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14740/17473 [02:37<00:29, 93.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  84% 14760/17473 [02:37<00:29, 93.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14780/17473 [02:37<00:28, 93.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14800/17473 [02:38<00:28, 93.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14820/17473 [02:38<00:28, 93.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14840/17473 [02:38<00:28, 93.77it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14860/17473 [02:38<00:27, 93.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14880/17473 [02:38<00:27, 93.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14900/17473 [02:38<00:27, 93.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  85% 14920/17473 [02:38<00:27, 94.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 14940/17473 [02:38<00:26, 94.10it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 14960/17473 [02:38<00:26, 94.17it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 14980/17473 [02:38<00:26, 94.24it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15000/17473 [02:39<00:26, 94.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15020/17473 [02:39<00:25, 94.37it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15040/17473 [02:39<00:25, 94.43it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15060/17473 [02:39<00:25, 94.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15080/17473 [02:39<00:25, 94.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  86% 15100/17473 [02:39<00:25, 94.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15120/17473 [02:39<00:24, 94.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15140/17473 [02:39<00:24, 94.74it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15160/17473 [02:39<00:24, 94.80it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15180/17473 [02:40<00:24, 94.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15200/17473 [02:40<00:23, 94.92it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15220/17473 [02:40<00:23, 94.98it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15240/17473 [02:40<00:23, 95.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15260/17473 [02:40<00:23, 95.10it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  87% 15280/17473 [02:40<00:23, 95.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15300/17473 [02:40<00:22, 95.23it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15320/17473 [02:40<00:22, 95.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15340/17473 [02:40<00:22, 95.35it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15360/17473 [02:40<00:22, 95.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15380/17473 [02:41<00:21, 95.48it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15400/17473 [02:41<00:21, 95.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15420/17473 [02:41<00:21, 95.60it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15440/17473 [02:41<00:21, 95.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  88% 15460/17473 [02:41<00:21, 95.73it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15480/17473 [02:41<00:20, 95.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15500/17473 [02:41<00:20, 95.85it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15520/17473 [02:41<00:20, 95.92it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15540/17473 [02:41<00:20, 95.98it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15560/17473 [02:42<00:19, 96.04it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15580/17473 [02:42<00:19, 96.11it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15600/17473 [02:42<00:19, 96.17it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  89% 15620/17473 [02:42<00:19, 96.24it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15640/17473 [02:42<00:19, 96.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15660/17473 [02:42<00:18, 96.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15680/17473 [02:42<00:18, 96.43it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15700/17473 [02:42<00:18, 96.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15720/17473 [02:42<00:18, 96.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15740/17473 [02:42<00:17, 96.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15760/17473 [02:43<00:17, 96.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15780/17473 [02:43<00:17, 96.74it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  90% 15800/17473 [02:43<00:17, 96.80it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15820/17473 [02:43<00:17, 96.87it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15840/17473 [02:43<00:16, 96.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15860/17473 [02:43<00:16, 96.99it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15880/17473 [02:43<00:16, 97.06it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15900/17473 [02:43<00:16, 97.12it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15920/17473 [02:43<00:15, 97.19it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15940/17473 [02:43<00:15, 97.25it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15960/17473 [02:44<00:15, 97.31it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  91% 15980/17473 [02:44<00:15, 97.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16000/17473 [02:44<00:15, 97.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16020/17473 [02:44<00:14, 97.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16040/17473 [02:44<00:14, 97.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16060/17473 [02:44<00:14, 97.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16080/17473 [02:44<00:14, 97.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16100/17473 [02:44<00:14, 97.76it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16120/17473 [02:44<00:13, 97.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16140/17473 [02:44<00:13, 97.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  92% 16160/17473 [02:44<00:13, 97.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16180/17473 [02:45<00:13, 98.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16200/17473 [02:45<00:12, 98.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16220/17473 [02:45<00:12, 98.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16240/17473 [02:45<00:12, 98.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16260/17473 [02:45<00:12, 98.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16280/17473 [02:45<00:12, 98.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16300/17473 [02:45<00:11, 98.40it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  93% 16320/17473 [02:45<00:11, 98.46it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16340/17473 [02:45<00:11, 98.52it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16360/17473 [02:45<00:11, 98.58it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16380/17473 [02:46<00:11, 98.65it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16400/17473 [02:46<00:10, 98.71it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16420/17473 [02:46<00:10, 98.78it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16440/17473 [02:46<00:10, 98.84it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16460/17473 [02:46<00:10, 98.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16480/17473 [02:46<00:10, 98.97it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  94% 16500/17473 [02:46<00:09, 99.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16520/17473 [02:46<00:09, 99.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16540/17473 [02:46<00:09, 99.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16560/17473 [02:46<00:09, 99.22it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16580/17473 [02:46<00:08, 99.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16600/17473 [02:47<00:08, 99.35it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16620/17473 [02:47<00:08, 99.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16640/17473 [02:47<00:08, 99.47it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16660/17473 [02:47<00:08, 99.52it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  95% 16680/17473 [02:47<00:07, 99.58it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16700/17473 [02:47<00:07, 99.65it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16720/17473 [02:47<00:07, 99.71it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16740/17473 [02:47<00:07, 99.77it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16760/17473 [02:47<00:07, 99.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16780/17473 [02:47<00:06, 99.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16800/17473 [02:48<00:06, 99.95it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16820/17473 [02:48<00:06, 100.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16840/17473 [02:48<00:06, 100.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  96% 16860/17473 [02:48<00:06, 100.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16880/17473 [02:48<00:05, 100.20it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16900/17473 [02:48<00:05, 100.26it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16920/17473 [02:48<00:05, 100.33it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16940/17473 [02:48<00:05, 100.39it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16960/17473 [02:48<00:05, 100.45it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 16980/17473 [02:48<00:04, 100.51it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 17000/17473 [02:49<00:04, 100.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  97% 17020/17473 [02:49<00:04, 100.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17040/17473 [02:49<00:04, 100.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17060/17473 [02:49<00:04, 100.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17080/17473 [02:49<00:03, 100.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17100/17473 [02:49<00:03, 100.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17120/17473 [02:49<00:03, 100.92it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17140/17473 [02:49<00:03, 100.98it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17160/17473 [02:49<00:03, 101.04it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17180/17473 [02:49<00:02, 101.10it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  98% 17200/17473 [02:50<00:02, 101.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17220/17473 [02:50<00:02, 101.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17240/17473 [02:50<00:02, 101.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17260/17473 [02:50<00:02, 101.33it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17280/17473 [02:50<00:01, 101.39it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17300/17473 [02:50<00:01, 101.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17320/17473 [02:50<00:01, 101.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17340/17473 [02:50<00:01, 101.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17360/17473 [02:50<00:01, 101.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16:  99% 17380/17473 [02:50<00:00, 101.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16: 100% 17400/17473 [02:51<00:00, 101.72it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16: 100% 17420/17473 [02:51<00:00, 101.78it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16: 100% 17440/17473 [02:51<00:00, 101.84it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16: 100% 17460/17473 [02:51<00:00, 101.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 16: 100% 17473/17473 [02:51<00:00, 101.92it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  80% 13960/17473 [02:29<00:37, 93.37it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  80% 13980/17473 [02:34<00:38, 90.65it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  80% 14000/17473 [02:34<00:38, 90.72it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  80% 14020/17473 [02:34<00:38, 90.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  80% 14040/17473 [02:34<00:37, 90.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  80% 14060/17473 [02:34<00:37, 90.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14080/17473 [02:34<00:37, 91.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14100/17473 [02:34<00:37, 91.07it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14120/17473 [02:34<00:36, 91.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14140/17473 [02:35<00:36, 91.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14160/17473 [02:35<00:36, 91.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14180/17473 [02:35<00:36, 91.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14200/17473 [02:35<00:35, 91.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14220/17473 [02:35<00:35, 91.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  81% 14240/17473 [02:35<00:35, 91.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14260/17473 [02:35<00:35, 91.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14280/17473 [02:35<00:34, 91.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14300/17473 [02:35<00:34, 91.77it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14320/17473 [02:35<00:34, 91.84it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14340/17473 [02:36<00:34, 91.91it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14360/17473 [02:36<00:33, 91.98it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14380/17473 [02:36<00:33, 92.06it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  82% 14400/17473 [02:36<00:33, 92.13it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14420/17473 [02:36<00:33, 92.20it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14440/17473 [02:36<00:32, 92.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14460/17473 [02:36<00:32, 92.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14480/17473 [02:36<00:32, 92.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14500/17473 [02:36<00:32, 92.47it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14520/17473 [02:36<00:31, 92.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14540/17473 [02:37<00:31, 92.61it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14560/17473 [02:37<00:31, 92.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  83% 14580/17473 [02:37<00:31, 92.74it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14600/17473 [02:37<00:30, 92.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14620/17473 [02:37<00:30, 92.88it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14640/17473 [02:37<00:30, 92.94it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14660/17473 [02:37<00:30, 93.01it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14680/17473 [02:37<00:30, 93.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14700/17473 [02:37<00:29, 93.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14720/17473 [02:37<00:29, 93.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14740/17473 [02:38<00:29, 93.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  84% 14760/17473 [02:38<00:29, 93.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14780/17473 [02:38<00:28, 93.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14800/17473 [02:38<00:28, 93.48it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14820/17473 [02:38<00:28, 93.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14840/17473 [02:38<00:28, 93.61it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14860/17473 [02:38<00:27, 93.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14880/17473 [02:38<00:27, 93.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14900/17473 [02:38<00:27, 93.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  85% 14920/17473 [02:38<00:27, 93.88it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 14940/17473 [02:39<00:26, 93.94it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 14960/17473 [02:39<00:26, 94.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 14980/17473 [02:39<00:26, 94.06it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15000/17473 [02:39<00:26, 94.12it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15020/17473 [02:39<00:26, 94.18it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15040/17473 [02:39<00:25, 94.24it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15060/17473 [02:39<00:25, 94.31it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15080/17473 [02:39<00:25, 94.37it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  86% 15100/17473 [02:39<00:25, 94.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15120/17473 [02:39<00:24, 94.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15140/17473 [02:40<00:24, 94.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15160/17473 [02:40<00:24, 94.61it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15180/17473 [02:40<00:24, 94.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15200/17473 [02:40<00:23, 94.74it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15220/17473 [02:40<00:23, 94.80it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15240/17473 [02:40<00:23, 94.87it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15260/17473 [02:40<00:23, 94.94it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  87% 15280/17473 [02:40<00:23, 95.01it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15300/17473 [02:40<00:22, 95.07it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15320/17473 [02:41<00:22, 95.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15340/17473 [02:41<00:22, 95.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15360/17473 [02:41<00:22, 95.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15380/17473 [02:41<00:21, 95.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15400/17473 [02:41<00:21, 95.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15420/17473 [02:41<00:21, 95.47it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15440/17473 [02:41<00:21, 95.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  88% 15460/17473 [02:41<00:21, 95.60it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15480/17473 [02:41<00:20, 95.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15500/17473 [02:41<00:20, 95.74it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15520/17473 [02:42<00:20, 95.80it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15540/17473 [02:42<00:20, 95.87it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15560/17473 [02:42<00:19, 95.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15580/17473 [02:42<00:19, 96.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15600/17473 [02:42<00:19, 96.07it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  89% 15620/17473 [02:42<00:19, 96.13it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15640/17473 [02:42<00:19, 96.20it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15660/17473 [02:42<00:18, 96.26it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15680/17473 [02:42<00:18, 96.33it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15700/17473 [02:42<00:18, 96.39it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15720/17473 [02:42<00:18, 96.45it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15740/17473 [02:43<00:17, 96.52it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15760/17473 [02:43<00:17, 96.58it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15780/17473 [02:43<00:17, 96.64it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  90% 15800/17473 [02:43<00:17, 96.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15820/17473 [02:43<00:17, 96.76it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15840/17473 [02:43<00:16, 96.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15860/17473 [02:43<00:16, 96.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15880/17473 [02:43<00:16, 96.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15900/17473 [02:43<00:16, 97.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15920/17473 [02:43<00:15, 97.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15940/17473 [02:44<00:15, 97.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15960/17473 [02:44<00:15, 97.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  91% 15980/17473 [02:44<00:15, 97.26it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16000/17473 [02:44<00:15, 97.32it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16020/17473 [02:44<00:14, 97.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16040/17473 [02:44<00:14, 97.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16060/17473 [02:44<00:14, 97.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16080/17473 [02:44<00:14, 97.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16100/17473 [02:44<00:14, 97.60it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16120/17473 [02:45<00:13, 97.66it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16140/17473 [02:45<00:13, 97.72it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  92% 16160/17473 [02:45<00:13, 97.78it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16180/17473 [02:45<00:13, 97.84it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16200/17473 [02:45<00:13, 97.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16220/17473 [02:45<00:12, 97.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16240/17473 [02:45<00:12, 98.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16260/17473 [02:45<00:12, 98.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16280/17473 [02:45<00:12, 98.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16300/17473 [02:45<00:11, 98.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  93% 16320/17473 [02:46<00:11, 98.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16340/17473 [02:46<00:11, 98.32it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16360/17473 [02:46<00:11, 98.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16380/17473 [02:46<00:11, 98.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16400/17473 [02:46<00:10, 98.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16420/17473 [02:46<00:10, 98.55it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16440/17473 [02:46<00:10, 98.61it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16460/17473 [02:46<00:10, 98.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16480/17473 [02:46<00:10, 98.73it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  94% 16500/17473 [02:47<00:09, 98.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16520/17473 [02:47<00:09, 98.85it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16540/17473 [02:47<00:09, 98.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16560/17473 [02:47<00:09, 98.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16580/17473 [02:47<00:09, 99.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16600/17473 [02:47<00:08, 99.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16620/17473 [02:47<00:08, 99.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16640/17473 [02:47<00:08, 99.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16660/17473 [02:47<00:08, 99.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  95% 16680/17473 [02:50<00:08, 98.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16700/17473 [02:50<00:07, 98.06it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16720/17473 [02:50<00:07, 98.11it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16740/17473 [02:50<00:07, 98.18it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16760/17473 [02:50<00:07, 98.24it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16780/17473 [02:50<00:07, 98.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16800/17473 [02:50<00:06, 98.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16820/17473 [02:50<00:06, 98.42it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16840/17473 [02:50<00:06, 98.48it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  96% 16860/17473 [02:51<00:06, 98.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16880/17473 [02:51<00:06, 98.60it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16900/17473 [02:51<00:05, 98.66it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16920/17473 [02:51<00:05, 98.72it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16940/17473 [02:51<00:05, 98.77it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16960/17473 [02:51<00:05, 98.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 16980/17473 [02:51<00:04, 98.88it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 17000/17473 [02:51<00:04, 98.94it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  97% 17020/17473 [02:51<00:04, 98.99it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17040/17473 [02:52<00:04, 99.05it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17060/17473 [02:52<00:04, 99.10it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17080/17473 [02:52<00:03, 99.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17100/17473 [02:52<00:03, 99.22it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17120/17473 [02:52<00:03, 99.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17140/17473 [02:52<00:03, 99.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17160/17473 [02:52<00:03, 99.40it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17180/17473 [02:52<00:02, 99.46it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  98% 17200/17473 [02:52<00:02, 99.51it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17220/17473 [02:52<00:02, 99.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17240/17473 [02:53<00:02, 99.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17260/17473 [02:53<00:02, 99.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17280/17473 [02:53<00:01, 99.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17300/17473 [02:53<00:01, 99.80it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17320/17473 [02:53<00:01, 99.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17340/17473 [02:53<00:01, 99.92it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17360/17473 [02:53<00:01, 99.98it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17:  99% 17380/17473 [02:53<00:00, 100.04it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17: 100% 17400/17473 [02:53<00:00, 100.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17: 100% 17420/17473 [02:53<00:00, 100.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17: 100% 17440/17473 [02:54<00:00, 100.20it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17: 100% 17460/17473 [02:54<00:00, 100.26it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 17: 100% 17473/17473 [02:54<00:00, 100.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.385\n",
            "Epoch 18:  80% 13960/17473 [02:30<00:37, 92.97it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  80% 13980/17473 [02:34<00:38, 90.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  80% 14000/17473 [02:34<00:38, 90.45it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  80% 14020/17473 [02:34<00:38, 90.52it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  80% 14040/17473 [02:34<00:37, 90.59it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  80% 14060/17473 [02:35<00:37, 90.66it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14080/17473 [02:35<00:37, 90.73it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14100/17473 [02:35<00:37, 90.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14120/17473 [02:35<00:36, 90.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14140/17473 [02:35<00:36, 90.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14160/17473 [02:35<00:36, 91.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14180/17473 [02:35<00:36, 91.07it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14200/17473 [02:35<00:35, 91.14it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14220/17473 [02:35<00:35, 91.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  81% 14240/17473 [02:35<00:35, 91.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14260/17473 [02:36<00:35, 91.35it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14280/17473 [02:36<00:34, 91.42it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14300/17473 [02:36<00:34, 91.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14320/17473 [02:36<00:34, 91.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14340/17473 [02:36<00:34, 91.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14360/17473 [02:36<00:33, 91.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14380/17473 [02:36<00:33, 91.77it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  82% 14400/17473 [02:36<00:33, 91.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14420/17473 [02:36<00:33, 91.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14440/17473 [02:37<00:32, 91.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14460/17473 [02:37<00:32, 92.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14480/17473 [02:37<00:32, 92.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14500/17473 [02:37<00:32, 92.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14520/17473 [02:37<00:32, 92.22it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14540/17473 [02:37<00:31, 92.29it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14560/17473 [02:37<00:31, 92.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  83% 14580/17473 [02:37<00:31, 92.42it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14600/17473 [02:37<00:31, 92.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14620/17473 [02:37<00:30, 92.55it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14640/17473 [02:38<00:30, 92.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14660/17473 [02:38<00:30, 92.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14680/17473 [02:38<00:30, 92.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14700/17473 [02:38<00:29, 92.82it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14720/17473 [02:38<00:29, 92.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14740/17473 [02:38<00:29, 92.95it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  84% 14760/17473 [02:38<00:29, 93.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14780/17473 [02:38<00:28, 93.08it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14800/17473 [02:38<00:28, 93.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14820/17473 [02:38<00:28, 93.23it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14840/17473 [02:39<00:28, 93.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14860/17473 [02:39<00:27, 93.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14880/17473 [02:39<00:27, 93.43it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14900/17473 [02:39<00:27, 93.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  85% 14920/17473 [02:39<00:27, 93.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 14940/17473 [02:39<00:27, 93.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 14960/17473 [02:39<00:26, 93.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 14980/17473 [02:39<00:26, 93.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15000/17473 [02:39<00:26, 93.82it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15020/17473 [02:39<00:26, 93.89it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15040/17473 [02:40<00:25, 93.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15060/17473 [02:40<00:25, 94.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15080/17473 [02:40<00:25, 94.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  86% 15100/17473 [02:40<00:25, 94.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15120/17473 [02:40<00:24, 94.23it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15140/17473 [02:40<00:24, 94.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15160/17473 [02:40<00:24, 94.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15180/17473 [02:40<00:24, 94.43it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15200/17473 [02:40<00:24, 94.49it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15220/17473 [02:40<00:23, 94.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15240/17473 [02:41<00:23, 94.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15260/17473 [02:41<00:23, 94.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  87% 15280/17473 [02:41<00:23, 94.76it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15300/17473 [02:41<00:22, 94.83it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15320/17473 [02:41<00:22, 94.88it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15340/17473 [02:41<00:22, 94.94it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15360/17473 [02:41<00:22, 95.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15380/17473 [02:41<00:22, 95.06it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15400/17473 [02:41<00:21, 95.12it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15420/17473 [02:42<00:21, 95.18it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15440/17473 [02:42<00:21, 95.25it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  88% 15460/17473 [02:42<00:21, 95.31it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15480/17473 [02:42<00:20, 95.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15500/17473 [02:42<00:20, 95.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15520/17473 [02:42<00:20, 95.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15540/17473 [02:42<00:20, 95.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15560/17473 [02:42<00:20, 95.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15580/17473 [02:42<00:19, 95.68it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15600/17473 [02:42<00:19, 95.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  89% 15620/17473 [02:43<00:19, 95.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15640/17473 [02:43<00:19, 95.87it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15660/17473 [02:43<00:18, 95.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15680/17473 [02:43<00:18, 96.00it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15700/17473 [02:43<00:18, 96.05it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15720/17473 [02:43<00:18, 96.12it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15740/17473 [02:43<00:18, 96.19it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15760/17473 [02:43<00:17, 96.25it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15780/17473 [02:43<00:17, 96.32it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  90% 15800/17473 [02:43<00:17, 96.38it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15820/17473 [02:44<00:17, 96.44it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15840/17473 [02:44<00:16, 96.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15860/17473 [02:44<00:16, 96.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15880/17473 [02:44<00:16, 96.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15900/17473 [02:44<00:16, 96.69it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15920/17473 [02:44<00:16, 96.75it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15940/17473 [02:44<00:15, 96.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15960/17473 [02:44<00:15, 96.86it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  91% 15980/17473 [02:44<00:15, 96.91it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16000/17473 [02:45<00:15, 96.97it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16020/17473 [02:45<00:14, 97.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16040/17473 [02:45<00:14, 97.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16060/17473 [02:45<00:14, 97.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16080/17473 [02:45<00:14, 97.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16100/17473 [02:45<00:14, 97.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16120/17473 [02:45<00:13, 97.34it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16140/17473 [02:45<00:13, 97.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  92% 16160/17473 [02:45<00:13, 97.47it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16180/17473 [02:45<00:13, 97.53it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16200/17473 [02:45<00:13, 97.60it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16220/17473 [02:46<00:12, 97.66it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16240/17473 [02:46<00:12, 97.72it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16260/17473 [02:46<00:12, 97.78it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16280/17473 [02:46<00:12, 97.85it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16300/17473 [02:46<00:11, 97.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  93% 16320/17473 [02:46<00:11, 97.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16340/17473 [02:46<00:11, 98.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16360/17473 [02:46<00:11, 98.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16380/17473 [02:46<00:11, 98.16it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16400/17473 [02:46<00:10, 98.22it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16420/17473 [02:47<00:10, 98.28it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16440/17473 [02:47<00:10, 98.35it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16460/17473 [02:47<00:10, 98.41it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16480/17473 [02:47<00:10, 98.48it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  94% 16500/17473 [02:47<00:09, 98.54it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16520/17473 [02:47<00:09, 98.61it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16540/17473 [02:47<00:09, 98.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16560/17473 [02:47<00:09, 98.73it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16580/17473 [02:47<00:09, 98.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16600/17473 [02:47<00:08, 98.85it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16620/17473 [02:48<00:08, 98.91it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16640/17473 [02:48<00:08, 98.97it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16660/17473 [02:48<00:08, 99.03it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  95% 16680/17473 [02:48<00:08, 99.09it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16700/17473 [02:48<00:07, 99.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16720/17473 [02:48<00:07, 99.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16740/17473 [02:48<00:07, 99.27it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16760/17473 [02:48<00:07, 99.33it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16780/17473 [02:48<00:06, 99.39it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16800/17473 [02:48<00:06, 99.45it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16820/17473 [02:49<00:06, 99.51it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16840/17473 [02:49<00:06, 99.57it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  96% 16860/17473 [02:49<00:06, 99.63it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16880/17473 [02:49<00:05, 99.70it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16900/17473 [02:49<00:05, 99.76it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16920/17473 [02:49<00:05, 99.81it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16940/17473 [02:49<00:05, 99.87it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16960/17473 [02:49<00:05, 99.93it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 16980/17473 [02:49<00:04, 99.99it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 17000/17473 [02:49<00:04, 100.04it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  97% 17020/17473 [02:50<00:04, 100.10it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17040/17473 [02:50<00:04, 100.15it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17060/17473 [02:50<00:04, 100.21it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17080/17473 [02:50<00:03, 100.26it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17100/17473 [02:50<00:03, 100.32it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17120/17473 [02:50<00:03, 100.37it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17140/17473 [02:50<00:03, 100.43it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17160/17473 [02:50<00:03, 100.50it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17180/17473 [02:50<00:02, 100.56it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  98% 17200/17473 [02:50<00:02, 100.62it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17220/17473 [02:51<00:02, 100.67it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17240/17473 [02:51<00:02, 100.73it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17260/17473 [02:51<00:02, 100.79it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17280/17473 [02:51<00:01, 100.84it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17300/17473 [02:51<00:01, 100.90it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17320/17473 [02:51<00:01, 100.96it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17340/17473 [02:51<00:01, 101.02it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17360/17473 [02:51<00:01, 101.07it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18:  99% 17380/17473 [02:51<00:00, 101.13it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18: 100% 17400/17473 [02:51<00:00, 101.19it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18: 100% 17420/17473 [02:52<00:00, 101.24it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18: 100% 17440/17473 [02:52<00:00, 101.30it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18: 100% 17460/17473 [02:52<00:00, 101.36it/s, loss=3.37, v_num=24, val_loss=3.390, avg_val_loss=3.390, train_loss=3.370]\n",
            "Epoch 18: 100% 17473/17473 [02:52<00:00, 101.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  80% 13960/17473 [02:32<00:38, 91.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  80% 13980/17473 [02:36<00:39, 89.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  80% 14000/17473 [02:37<00:38, 89.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  80% 14020/17473 [02:37<00:38, 89.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  80% 14040/17473 [02:37<00:38, 89.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  80% 14060/17473 [02:37<00:38, 89.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14080/17473 [02:37<00:37, 89.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14100/17473 [02:37<00:37, 89.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14120/17473 [02:37<00:37, 89.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14140/17473 [02:37<00:37, 89.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14160/17473 [02:37<00:36, 89.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14180/17473 [02:37<00:36, 89.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14200/17473 [02:38<00:36, 89.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14220/17473 [02:38<00:36, 89.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  81% 14240/17473 [02:38<00:35, 89.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14260/17473 [02:38<00:35, 90.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14280/17473 [02:38<00:35, 90.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14300/17473 [02:38<00:35, 90.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14320/17473 [02:38<00:34, 90.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14340/17473 [02:38<00:34, 90.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14360/17473 [02:38<00:34, 90.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14380/17473 [02:38<00:34, 90.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  82% 14400/17473 [02:39<00:33, 90.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14420/17473 [02:39<00:33, 90.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14440/17473 [02:39<00:33, 90.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14460/17473 [02:39<00:33, 90.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14480/17473 [02:39<00:32, 90.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14500/17473 [02:39<00:32, 90.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14520/17473 [02:39<00:32, 90.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14540/17473 [02:39<00:32, 91.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14560/17473 [02:39<00:31, 91.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  83% 14580/17473 [02:39<00:31, 91.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14600/17473 [02:40<00:31, 91.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14620/17473 [02:40<00:31, 91.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14640/17473 [02:40<00:31, 91.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14660/17473 [02:40<00:30, 91.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14680/17473 [02:40<00:30, 91.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14700/17473 [02:40<00:30, 91.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14720/17473 [02:40<00:30, 91.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14740/17473 [02:40<00:29, 91.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  84% 14760/17473 [02:40<00:29, 91.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14780/17473 [02:41<00:29, 91.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14800/17473 [02:41<00:29, 91.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14820/17473 [02:41<00:28, 91.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14840/17473 [02:41<00:28, 91.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14860/17473 [02:41<00:28, 92.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14880/17473 [02:41<00:28, 92.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14900/17473 [02:41<00:27, 92.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  85% 14920/17473 [02:41<00:27, 92.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 14940/17473 [02:41<00:27, 92.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 14960/17473 [02:42<00:27, 92.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 14980/17473 [02:42<00:26, 92.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15000/17473 [02:42<00:26, 92.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15020/17473 [02:42<00:26, 92.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15040/17473 [02:42<00:26, 92.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15060/17473 [02:42<00:26, 92.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15080/17473 [02:42<00:25, 92.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  86% 15100/17473 [02:42<00:25, 92.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15120/17473 [02:42<00:25, 92.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15140/17473 [02:43<00:25, 92.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15160/17473 [02:43<00:24, 92.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15180/17473 [02:43<00:24, 92.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15200/17473 [02:43<00:24, 93.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15220/17473 [02:43<00:24, 93.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15240/17473 [02:43<00:23, 93.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15260/17473 [02:43<00:23, 93.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  87% 15280/17473 [02:43<00:23, 93.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15300/17473 [02:43<00:23, 93.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15320/17473 [02:43<00:23, 93.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15340/17473 [02:44<00:22, 93.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15360/17473 [02:44<00:22, 93.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15380/17473 [02:44<00:22, 93.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15400/17473 [02:44<00:22, 93.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15420/17473 [02:44<00:21, 93.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15440/17473 [02:44<00:21, 93.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  88% 15460/17473 [02:44<00:21, 93.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15480/17473 [02:44<00:21, 93.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15500/17473 [02:44<00:20, 93.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15520/17473 [02:45<00:20, 94.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15540/17473 [02:45<00:20, 94.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15560/17473 [02:45<00:20, 94.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15580/17473 [02:45<00:20, 94.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15600/17473 [02:45<00:19, 94.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  89% 15620/17473 [02:45<00:19, 94.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15640/17473 [02:45<00:19, 94.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15660/17473 [02:45<00:19, 94.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15680/17473 [02:45<00:18, 94.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15700/17473 [02:45<00:18, 94.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15720/17473 [02:46<00:18, 94.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15740/17473 [02:46<00:18, 94.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15760/17473 [02:46<00:18, 94.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15780/17473 [02:46<00:17, 94.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  90% 15800/17473 [02:46<00:17, 94.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15820/17473 [02:46<00:17, 95.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15840/17473 [02:46<00:17, 95.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15860/17473 [02:46<00:16, 95.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15880/17473 [02:46<00:16, 95.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15900/17473 [02:46<00:16, 95.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15920/17473 [02:47<00:16, 95.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15940/17473 [02:47<00:16, 95.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15960/17473 [02:47<00:15, 95.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  91% 15980/17473 [02:47<00:15, 95.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16000/17473 [02:47<00:15, 95.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16020/17473 [02:47<00:15, 95.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16040/17473 [02:47<00:14, 95.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16060/17473 [02:47<00:14, 95.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16080/17473 [02:47<00:14, 95.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16100/17473 [02:47<00:14, 95.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16120/17473 [02:48<00:14, 95.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16140/17473 [02:48<00:13, 95.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  92% 16160/17473 [02:48<00:13, 96.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16180/17473 [02:48<00:13, 96.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16200/17473 [02:48<00:13, 96.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16220/17473 [02:48<00:13, 96.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16240/17473 [02:48<00:12, 96.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16260/17473 [02:48<00:12, 96.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16280/17473 [02:48<00:12, 96.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16300/17473 [02:48<00:12, 96.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  93% 16320/17473 [02:49<00:11, 96.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16340/17473 [02:49<00:11, 96.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16360/17473 [02:49<00:11, 96.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16380/17473 [02:49<00:11, 96.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16400/17473 [02:49<00:11, 96.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16420/17473 [02:49<00:10, 96.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16440/17473 [02:49<00:10, 96.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16460/17473 [02:49<00:10, 96.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16480/17473 [02:49<00:10, 97.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  94% 16500/17473 [02:49<00:10, 97.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16520/17473 [02:50<00:09, 97.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16540/17473 [02:50<00:09, 97.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16560/17473 [02:50<00:09, 97.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16580/17473 [02:50<00:09, 97.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16600/17473 [02:50<00:08, 97.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16620/17473 [02:50<00:08, 97.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16640/17473 [02:50<00:08, 97.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16660/17473 [02:50<00:08, 97.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  95% 16680/17473 [02:50<00:08, 97.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16700/17473 [02:50<00:07, 97.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16720/17473 [02:51<00:07, 97.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16740/17473 [02:51<00:07, 97.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16760/17473 [02:51<00:07, 97.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16780/17473 [02:51<00:07, 97.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16800/17473 [02:51<00:06, 97.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16820/17473 [02:51<00:06, 98.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16840/17473 [02:51<00:06, 98.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  96% 16860/17473 [02:51<00:06, 98.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16880/17473 [02:51<00:06, 98.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16900/17473 [02:52<00:05, 98.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16920/17473 [02:52<00:05, 98.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16940/17473 [02:52<00:05, 98.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16960/17473 [02:52<00:05, 98.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 16980/17473 [02:52<00:05, 98.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 17000/17473 [02:52<00:04, 98.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  97% 17020/17473 [02:52<00:04, 98.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17040/17473 [02:52<00:04, 98.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17060/17473 [02:52<00:04, 98.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17080/17473 [02:52<00:03, 98.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17100/17473 [02:53<00:03, 98.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17120/17473 [02:53<00:03, 98.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17140/17473 [02:53<00:03, 98.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17160/17473 [02:53<00:03, 99.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17180/17473 [02:53<00:02, 99.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  98% 17200/17473 [02:53<00:02, 99.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17220/17473 [02:53<00:02, 99.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17240/17473 [02:53<00:02, 99.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17260/17473 [02:53<00:02, 99.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17280/17473 [02:53<00:01, 99.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17300/17473 [02:53<00:01, 99.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17320/17473 [02:54<00:01, 99.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17340/17473 [02:54<00:01, 99.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17360/17473 [02:54<00:01, 99.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19:  99% 17380/17473 [02:54<00:00, 99.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19: 100% 17400/17473 [02:54<00:00, 99.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19: 100% 17420/17473 [02:54<00:00, 99.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19: 100% 17440/17473 [02:54<00:00, 99.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19: 100% 17460/17473 [02:54<00:00, 99.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 19: 100% 17473/17473 [02:54<00:00, 99.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 3.383\n",
            "Epoch 20:  80% 13960/17473 [02:31<00:38, 91.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  80% 13980/17473 [02:36<00:39, 89.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  80% 14000/17473 [02:36<00:38, 89.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  80% 14020/17473 [02:36<00:38, 89.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  80% 14040/17473 [02:36<00:38, 89.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  80% 14060/17473 [02:37<00:38, 89.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14080/17473 [02:37<00:37, 89.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14100/17473 [02:37<00:37, 89.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14120/17473 [02:37<00:37, 89.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14140/17473 [02:37<00:37, 89.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14160/17473 [02:37<00:36, 89.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14180/17473 [02:37<00:36, 89.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14200/17473 [02:37<00:36, 90.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14220/17473 [02:37<00:36, 90.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  81% 14240/17473 [02:37<00:35, 90.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14260/17473 [02:38<00:35, 90.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14280/17473 [02:38<00:35, 90.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14300/17473 [02:38<00:35, 90.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14320/17473 [02:38<00:34, 90.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14340/17473 [02:38<00:34, 90.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14360/17473 [02:38<00:34, 90.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14380/17473 [02:38<00:34, 90.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  82% 14400/17473 [02:38<00:33, 90.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14420/17473 [02:38<00:33, 90.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14440/17473 [02:38<00:33, 90.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14460/17473 [02:39<00:33, 90.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14480/17473 [02:39<00:32, 91.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14500/17473 [02:39<00:32, 91.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14520/17473 [02:39<00:32, 91.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14540/17473 [02:39<00:32, 91.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14560/17473 [02:39<00:31, 91.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  83% 14580/17473 [02:39<00:31, 91.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14600/17473 [02:39<00:31, 91.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14620/17473 [02:39<00:31, 91.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14640/17473 [02:39<00:30, 91.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14660/17473 [02:40<00:30, 91.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14680/17473 [02:40<00:30, 91.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14700/17473 [02:40<00:30, 91.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14720/17473 [02:40<00:29, 91.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14740/17473 [02:40<00:29, 91.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  84% 14760/17473 [02:40<00:29, 91.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14780/17473 [02:40<00:29, 92.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14800/17473 [02:40<00:29, 92.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14820/17473 [02:40<00:28, 92.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14840/17473 [02:40<00:28, 92.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14860/17473 [02:41<00:28, 92.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14880/17473 [02:41<00:28, 92.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14900/17473 [02:41<00:27, 92.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  85% 14920/17473 [02:41<00:27, 92.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 14940/17473 [02:41<00:27, 92.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 14960/17473 [02:41<00:27, 92.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 14980/17473 [02:41<00:26, 92.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15000/17473 [02:41<00:26, 92.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15020/17473 [02:41<00:26, 92.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15040/17473 [02:41<00:26, 92.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15060/17473 [02:42<00:25, 92.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15080/17473 [02:42<00:25, 93.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  86% 15100/17473 [02:42<00:25, 93.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15120/17473 [02:42<00:25, 93.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15140/17473 [02:42<00:25, 93.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15160/17473 [02:42<00:24, 93.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15180/17473 [02:42<00:24, 93.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15200/17473 [02:42<00:24, 93.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15220/17473 [02:42<00:24, 93.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15240/17473 [02:42<00:23, 93.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15260/17473 [02:43<00:23, 93.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  87% 15280/17473 [02:43<00:23, 93.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15300/17473 [02:43<00:23, 93.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15320/17473 [02:43<00:22, 93.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15340/17473 [02:43<00:22, 93.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15360/17473 [02:43<00:22, 93.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15380/17473 [02:43<00:22, 93.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15400/17473 [02:43<00:22, 94.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15420/17473 [02:43<00:21, 94.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15440/17473 [02:43<00:21, 94.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  88% 15460/17473 [02:44<00:21, 94.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15480/17473 [02:44<00:21, 94.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15500/17473 [02:44<00:20, 94.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15520/17473 [02:44<00:20, 94.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15540/17473 [02:44<00:20, 94.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15560/17473 [02:44<00:20, 94.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15580/17473 [02:44<00:20, 94.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15600/17473 [02:44<00:19, 94.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  89% 15620/17473 [02:44<00:19, 94.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15640/17473 [02:44<00:19, 94.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15660/17473 [02:45<00:19, 94.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15680/17473 [02:45<00:18, 94.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15700/17473 [02:45<00:18, 95.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15720/17473 [02:45<00:18, 95.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15740/17473 [02:45<00:18, 95.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15760/17473 [02:45<00:17, 95.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15780/17473 [02:45<00:17, 95.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  90% 15800/17473 [02:45<00:17, 95.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15820/17473 [02:45<00:17, 95.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15840/17473 [02:45<00:17, 95.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15860/17473 [02:45<00:16, 95.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15880/17473 [02:46<00:16, 95.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15900/17473 [02:46<00:16, 95.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15920/17473 [02:46<00:16, 95.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15940/17473 [02:46<00:16, 95.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15960/17473 [02:46<00:15, 95.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  91% 15980/17473 [02:46<00:15, 95.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16000/17473 [02:46<00:15, 95.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16020/17473 [02:46<00:15, 96.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16040/17473 [02:46<00:14, 96.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16060/17473 [02:47<00:14, 96.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16080/17473 [02:47<00:14, 96.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16100/17473 [02:47<00:14, 96.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16120/17473 [02:47<00:14, 96.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16140/17473 [02:47<00:13, 96.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  92% 16160/17473 [02:47<00:13, 96.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16180/17473 [02:47<00:13, 96.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16200/17473 [02:47<00:13, 96.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16220/17473 [02:47<00:12, 96.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16240/17473 [02:47<00:12, 96.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16260/17473 [02:48<00:12, 96.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16280/17473 [02:48<00:12, 96.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16300/17473 [02:48<00:12, 96.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  93% 16320/17473 [02:48<00:11, 96.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16340/17473 [02:48<00:11, 97.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16360/17473 [02:48<00:11, 97.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16380/17473 [02:48<00:11, 97.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16400/17473 [02:48<00:11, 97.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16420/17473 [02:48<00:10, 97.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16440/17473 [02:48<00:10, 97.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16460/17473 [02:49<00:10, 97.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16480/17473 [02:49<00:10, 97.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  94% 16500/17473 [02:49<00:09, 97.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16520/17473 [02:49<00:09, 97.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16540/17473 [02:49<00:09, 97.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16560/17473 [02:49<00:09, 97.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16580/17473 [02:49<00:09, 97.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16600/17473 [02:49<00:08, 97.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16620/17473 [02:49<00:08, 97.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16640/17473 [02:49<00:08, 97.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16660/17473 [02:50<00:08, 97.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  95% 16680/17473 [02:50<00:08, 98.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16700/17473 [02:50<00:07, 98.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16720/17473 [02:50<00:07, 98.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16740/17473 [02:50<00:07, 98.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16760/17473 [02:50<00:07, 98.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16780/17473 [02:50<00:07, 98.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16800/17473 [02:50<00:06, 98.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16820/17473 [02:50<00:06, 98.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16840/17473 [02:50<00:06, 98.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  96% 16860/17473 [02:51<00:06, 98.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16880/17473 [02:51<00:06, 98.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16900/17473 [02:51<00:05, 98.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16920/17473 [02:51<00:05, 98.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16940/17473 [02:51<00:05, 98.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16960/17473 [02:51<00:05, 98.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 16980/17473 [02:51<00:04, 98.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 17000/17473 [02:51<00:04, 98.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  97% 17020/17473 [02:51<00:04, 99.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17040/17473 [02:51<00:04, 99.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17060/17473 [02:52<00:04, 99.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17080/17473 [02:52<00:03, 99.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17100/17473 [02:52<00:03, 99.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17120/17473 [02:52<00:03, 99.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17140/17473 [02:52<00:03, 99.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17160/17473 [02:52<00:03, 99.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17180/17473 [02:52<00:02, 99.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  98% 17200/17473 [02:52<00:02, 99.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17220/17473 [02:52<00:02, 99.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17240/17473 [02:53<00:02, 99.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17260/17473 [02:53<00:02, 99.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17280/17473 [02:53<00:01, 99.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17300/17473 [02:53<00:01, 99.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17320/17473 [02:53<00:01, 99.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17340/17473 [02:53<00:01, 99.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17360/17473 [02:53<00:01, 99.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20:  99% 17380/17473 [02:53<00:00, 100.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20: 100% 17400/17473 [02:53<00:00, 100.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20: 100% 17420/17473 [02:53<00:00, 100.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20: 100% 17440/17473 [02:54<00:00, 100.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20: 100% 17460/17473 [02:54<00:00, 100.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 20: 100% 17473/17473 [02:54<00:00, 100.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  80% 13960/17473 [02:31<00:38, 92.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  80% 13980/17473 [02:36<00:39, 89.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  80% 14000/17473 [02:36<00:38, 89.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  80% 14020/17473 [02:36<00:38, 89.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  80% 14040/17473 [02:36<00:38, 89.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  80% 14060/17473 [02:36<00:37, 89.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14080/17473 [02:36<00:37, 89.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14100/17473 [02:36<00:37, 89.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14120/17473 [02:36<00:37, 90.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14140/17473 [02:36<00:36, 90.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14160/17473 [02:37<00:36, 90.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14180/17473 [02:37<00:36, 90.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14200/17473 [02:37<00:36, 90.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14220/17473 [02:37<00:35, 90.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  81% 14240/17473 [02:37<00:35, 90.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14260/17473 [02:37<00:35, 90.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14280/17473 [02:37<00:35, 90.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14300/17473 [02:37<00:35, 90.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14320/17473 [02:37<00:34, 90.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14340/17473 [02:37<00:34, 90.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14360/17473 [02:38<00:34, 90.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14380/17473 [02:38<00:34, 90.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  82% 14400/17473 [02:38<00:33, 90.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14420/17473 [02:38<00:33, 91.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14440/17473 [02:38<00:33, 91.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14460/17473 [02:38<00:33, 91.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14480/17473 [02:38<00:32, 91.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14500/17473 [02:38<00:32, 91.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14520/17473 [02:38<00:32, 91.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14540/17473 [02:39<00:32, 91.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14560/17473 [02:39<00:31, 91.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  83% 14580/17473 [02:39<00:31, 91.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14600/17473 [02:39<00:31, 91.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14620/17473 [02:39<00:31, 91.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14640/17473 [02:39<00:30, 91.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14660/17473 [02:39<00:30, 91.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14680/17473 [02:39<00:30, 91.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14700/17473 [02:39<00:30, 91.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14720/17473 [02:39<00:29, 92.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14740/17473 [02:40<00:29, 92.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  84% 14760/17473 [02:40<00:29, 92.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14780/17473 [02:40<00:29, 92.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14800/17473 [02:40<00:28, 92.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14820/17473 [02:40<00:28, 92.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14840/17473 [02:40<00:28, 92.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14860/17473 [02:40<00:28, 92.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14880/17473 [02:40<00:28, 92.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14900/17473 [02:40<00:27, 92.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  85% 14920/17473 [02:41<00:27, 92.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 14940/17473 [02:41<00:27, 92.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 14960/17473 [02:41<00:27, 92.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 14980/17473 [02:41<00:26, 92.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15000/17473 [02:41<00:26, 92.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15020/17473 [02:41<00:26, 92.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15040/17473 [02:41<00:26, 93.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15060/17473 [02:41<00:25, 93.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15080/17473 [02:41<00:25, 93.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  86% 15100/17473 [02:41<00:25, 93.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15120/17473 [02:42<00:25, 93.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15140/17473 [02:42<00:24, 93.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15160/17473 [02:42<00:24, 93.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15180/17473 [02:42<00:24, 93.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15200/17473 [02:42<00:24, 93.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15220/17473 [02:42<00:24, 93.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15240/17473 [02:42<00:23, 93.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15260/17473 [02:42<00:23, 93.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  87% 15280/17473 [02:42<00:23, 93.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15300/17473 [02:43<00:23, 93.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15320/17473 [02:43<00:22, 93.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15340/17473 [02:43<00:22, 93.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15360/17473 [02:43<00:22, 94.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15380/17473 [02:43<00:22, 94.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15400/17473 [02:43<00:22, 94.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15420/17473 [02:43<00:21, 94.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15440/17473 [02:43<00:21, 94.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  88% 15460/17473 [02:43<00:21, 94.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15480/17473 [02:43<00:21, 94.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15500/17473 [02:44<00:20, 94.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15520/17473 [02:44<00:20, 94.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15540/17473 [02:44<00:20, 94.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15560/17473 [02:44<00:20, 94.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15580/17473 [02:44<00:19, 94.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15600/17473 [02:44<00:19, 94.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  89% 15620/17473 [02:44<00:19, 94.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15640/17473 [02:44<00:19, 94.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15660/17473 [02:44<00:19, 95.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15680/17473 [02:44<00:18, 95.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15700/17473 [02:44<00:18, 95.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15720/17473 [02:45<00:18, 95.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15740/17473 [02:45<00:18, 95.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15760/17473 [02:45<00:17, 95.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15780/17473 [02:45<00:17, 95.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  90% 15800/17473 [02:45<00:17, 95.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15820/17473 [02:45<00:17, 95.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15840/17473 [02:45<00:17, 95.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15860/17473 [02:45<00:16, 95.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15880/17473 [02:45<00:16, 95.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15900/17473 [02:45<00:16, 95.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15920/17473 [02:46<00:16, 95.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15940/17473 [02:46<00:15, 95.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15960/17473 [02:46<00:15, 95.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  91% 15980/17473 [02:46<00:15, 96.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16000/17473 [02:46<00:15, 96.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16020/17473 [02:46<00:15, 96.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16040/17473 [02:46<00:14, 96.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16060/17473 [02:46<00:14, 96.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16080/17473 [02:46<00:14, 96.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16100/17473 [02:46<00:14, 96.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16120/17473 [02:47<00:14, 96.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16140/17473 [02:47<00:13, 96.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  92% 16160/17473 [02:47<00:13, 96.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16180/17473 [02:47<00:13, 96.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16200/17473 [02:47<00:13, 96.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16220/17473 [02:47<00:12, 96.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16240/17473 [02:47<00:12, 96.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16260/17473 [02:47<00:12, 96.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16280/17473 [02:47<00:12, 96.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16300/17473 [02:47<00:12, 97.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  93% 16320/17473 [02:48<00:11, 97.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16340/17473 [02:48<00:11, 97.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16360/17473 [02:48<00:11, 97.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16380/17473 [02:48<00:11, 97.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16400/17473 [02:48<00:11, 97.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16420/17473 [02:48<00:10, 97.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16440/17473 [02:48<00:10, 97.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16460/17473 [02:48<00:10, 97.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16480/17473 [02:48<00:10, 97.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  94% 16500/17473 [02:48<00:09, 97.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16520/17473 [02:49<00:09, 97.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16540/17473 [02:49<00:09, 97.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16560/17473 [02:49<00:09, 97.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16580/17473 [02:49<00:09, 97.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16600/17473 [02:49<00:08, 97.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16620/17473 [02:49<00:08, 98.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16640/17473 [02:49<00:08, 98.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16660/17473 [02:49<00:08, 98.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  95% 16680/17473 [02:49<00:08, 98.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16700/17473 [02:49<00:07, 98.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16720/17473 [02:50<00:07, 98.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16740/17473 [02:50<00:07, 98.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16760/17473 [02:50<00:07, 98.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16780/17473 [02:50<00:07, 98.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16800/17473 [02:50<00:06, 98.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16820/17473 [02:50<00:06, 98.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16840/17473 [02:50<00:06, 98.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  96% 16860/17473 [02:50<00:06, 98.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16880/17473 [02:50<00:06, 98.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16900/17473 [02:50<00:05, 98.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16920/17473 [02:51<00:05, 98.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16940/17473 [02:51<00:05, 98.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16960/17473 [02:51<00:05, 99.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 16980/17473 [02:51<00:04, 99.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 17000/17473 [02:51<00:04, 99.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  97% 17020/17473 [02:51<00:04, 99.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17040/17473 [02:51<00:04, 99.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17060/17473 [02:51<00:04, 99.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17080/17473 [02:51<00:03, 99.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17100/17473 [02:51<00:03, 99.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17120/17473 [02:51<00:03, 99.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17140/17473 [02:52<00:03, 99.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17160/17473 [02:52<00:03, 99.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17180/17473 [02:52<00:02, 99.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  98% 17200/17473 [02:52<00:02, 99.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17220/17473 [02:52<00:02, 99.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17240/17473 [02:52<00:02, 99.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17260/17473 [02:52<00:02, 99.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17280/17473 [02:52<00:01, 100.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17300/17473 [02:52<00:01, 100.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17320/17473 [02:52<00:01, 100.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17340/17473 [02:53<00:01, 100.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17360/17473 [02:53<00:01, 100.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21:  99% 17380/17473 [02:53<00:00, 100.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21: 100% 17400/17473 [02:53<00:00, 100.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21: 100% 17420/17473 [02:53<00:00, 100.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21: 100% 17440/17473 [02:53<00:00, 100.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21: 100% 17460/17473 [02:53<00:00, 100.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 21: 100% 17473/17473 [02:53<00:00, 100.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  80% 13960/17473 [02:31<00:38, 92.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  80% 13980/17473 [02:35<00:38, 89.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  80% 14000/17473 [02:35<00:38, 89.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  80% 14020/17473 [02:35<00:38, 90.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  80% 14040/17473 [02:35<00:38, 90.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  80% 14060/17473 [02:35<00:37, 90.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14080/17473 [02:36<00:37, 90.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14100/17473 [02:36<00:37, 90.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14120/17473 [02:36<00:37, 90.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14140/17473 [02:36<00:36, 90.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14160/17473 [02:36<00:36, 90.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14180/17473 [02:36<00:36, 90.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14200/17473 [02:36<00:36, 90.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14220/17473 [02:36<00:35, 90.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  81% 14240/17473 [02:36<00:35, 90.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14260/17473 [02:37<00:35, 90.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14280/17473 [02:37<00:35, 90.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14300/17473 [02:37<00:34, 90.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14320/17473 [02:37<00:34, 91.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14340/17473 [02:37<00:34, 91.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14360/17473 [02:37<00:34, 91.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14380/17473 [02:37<00:33, 91.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  82% 14400/17473 [02:37<00:33, 91.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14420/17473 [02:37<00:33, 91.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14440/17473 [02:37<00:33, 91.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14460/17473 [02:37<00:32, 91.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14480/17473 [02:38<00:32, 91.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14500/17473 [02:38<00:32, 91.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14520/17473 [02:38<00:32, 91.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14540/17473 [02:38<00:31, 91.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14560/17473 [02:38<00:31, 91.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  83% 14580/17473 [02:38<00:31, 91.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14600/17473 [02:38<00:31, 92.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14620/17473 [02:38<00:30, 92.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14640/17473 [02:38<00:30, 92.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14660/17473 [02:38<00:30, 92.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14680/17473 [02:39<00:30, 92.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14700/17473 [02:39<00:30, 92.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14720/17473 [02:39<00:29, 92.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14740/17473 [02:39<00:29, 92.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  84% 14760/17473 [02:39<00:29, 92.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14780/17473 [02:39<00:29, 92.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14800/17473 [02:39<00:28, 92.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14820/17473 [02:39<00:28, 92.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14840/17473 [02:39<00:28, 92.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14860/17473 [02:39<00:28, 92.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14880/17473 [02:40<00:27, 92.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14900/17473 [02:40<00:27, 93.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  85% 14920/17473 [02:40<00:27, 93.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 14940/17473 [02:40<00:27, 93.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 14960/17473 [02:40<00:26, 93.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 14980/17473 [02:40<00:26, 93.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15000/17473 [02:40<00:26, 93.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15020/17473 [02:40<00:26, 93.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15040/17473 [02:40<00:26, 93.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15060/17473 [02:40<00:25, 93.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15080/17473 [02:41<00:25, 93.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  86% 15100/17473 [02:41<00:25, 93.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15120/17473 [02:41<00:25, 93.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15140/17473 [02:41<00:24, 93.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15160/17473 [02:41<00:24, 93.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15180/17473 [02:41<00:24, 93.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15200/17473 [02:41<00:24, 94.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15220/17473 [02:41<00:23, 94.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15240/17473 [02:41<00:23, 94.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15260/17473 [02:42<00:23, 94.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  87% 15280/17473 [02:42<00:23, 94.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15300/17473 [02:42<00:23, 94.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15320/17473 [02:42<00:22, 94.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15340/17473 [02:42<00:22, 94.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15360/17473 [02:42<00:22, 94.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15380/17473 [02:42<00:22, 94.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15400/17473 [02:42<00:21, 94.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15420/17473 [02:42<00:21, 94.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15440/17473 [02:42<00:21, 94.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  88% 15460/17473 [02:43<00:21, 94.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15480/17473 [02:43<00:21, 94.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15500/17473 [02:43<00:20, 94.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15520/17473 [02:43<00:20, 95.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15540/17473 [02:43<00:20, 95.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15560/17473 [02:43<00:20, 95.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15580/17473 [02:43<00:19, 95.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15600/17473 [02:43<00:19, 95.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  89% 15620/17473 [02:43<00:19, 95.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15640/17473 [02:43<00:19, 95.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15660/17473 [02:44<00:18, 95.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15680/17473 [02:44<00:18, 95.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15700/17473 [02:44<00:18, 95.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15720/17473 [02:44<00:18, 95.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15740/17473 [02:44<00:18, 95.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15760/17473 [02:44<00:17, 95.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15780/17473 [02:44<00:17, 95.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  90% 15800/17473 [02:44<00:17, 95.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15820/17473 [02:44<00:17, 95.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15840/17473 [02:45<00:17, 95.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15860/17473 [02:45<00:16, 96.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15880/17473 [02:45<00:16, 96.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15900/17473 [02:45<00:16, 96.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15920/17473 [02:45<00:16, 96.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15940/17473 [02:45<00:15, 96.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15960/17473 [02:45<00:15, 96.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  91% 15980/17473 [02:45<00:15, 96.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16000/17473 [02:45<00:15, 96.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16020/17473 [02:45<00:15, 96.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16040/17473 [02:46<00:14, 96.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16060/17473 [02:46<00:14, 96.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16080/17473 [02:46<00:14, 96.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16100/17473 [02:46<00:14, 96.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16120/17473 [02:46<00:13, 96.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16140/17473 [02:46<00:13, 96.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  92% 16160/17473 [02:46<00:13, 96.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16180/17473 [02:46<00:13, 97.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16200/17473 [02:46<00:13, 97.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16220/17473 [02:46<00:12, 97.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16240/17473 [02:47<00:12, 97.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16260/17473 [02:47<00:12, 97.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16280/17473 [02:47<00:12, 97.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16300/17473 [02:47<00:12, 97.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  93% 16320/17473 [02:47<00:11, 97.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16340/17473 [02:47<00:11, 97.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16360/17473 [02:47<00:11, 97.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16380/17473 [02:47<00:11, 97.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16400/17473 [02:47<00:10, 97.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16420/17473 [02:47<00:10, 97.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16440/17473 [02:48<00:10, 97.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16460/17473 [02:48<00:10, 97.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16480/17473 [02:48<00:10, 97.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  94% 16500/17473 [02:48<00:09, 98.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16520/17473 [02:48<00:09, 98.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16540/17473 [02:48<00:09, 98.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16560/17473 [02:48<00:09, 98.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16580/17473 [02:48<00:09, 98.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16600/17473 [02:48<00:08, 98.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16620/17473 [02:48<00:08, 98.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16640/17473 [02:49<00:08, 98.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16660/17473 [02:49<00:08, 98.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  95% 16680/17473 [02:49<00:08, 98.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16700/17473 [02:49<00:07, 98.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16720/17473 [02:49<00:07, 98.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16740/17473 [02:49<00:07, 98.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16760/17473 [02:49<00:07, 98.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16780/17473 [02:49<00:07, 98.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16800/17473 [02:49<00:06, 98.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16820/17473 [02:49<00:06, 99.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16840/17473 [02:50<00:06, 99.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  96% 16860/17473 [02:50<00:06, 99.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16880/17473 [02:50<00:05, 99.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16900/17473 [02:50<00:05, 99.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16920/17473 [02:50<00:05, 99.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16940/17473 [02:50<00:05, 99.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16960/17473 [02:50<00:05, 99.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 16980/17473 [02:50<00:04, 99.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 17000/17473 [02:50<00:04, 99.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  97% 17020/17473 [02:50<00:04, 99.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17040/17473 [02:50<00:04, 99.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17060/17473 [02:51<00:04, 99.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17080/17473 [02:51<00:03, 99.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17100/17473 [02:51<00:03, 99.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17120/17473 [02:51<00:03, 99.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17140/17473 [02:51<00:03, 99.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17160/17473 [02:51<00:03, 100.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17180/17473 [02:51<00:02, 100.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  98% 17200/17473 [02:51<00:02, 100.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17220/17473 [02:51<00:02, 100.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17240/17473 [02:51<00:02, 100.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17260/17473 [02:52<00:02, 100.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17280/17473 [02:52<00:01, 100.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17300/17473 [02:52<00:01, 100.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17320/17473 [02:52<00:01, 100.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17340/17473 [02:52<00:01, 100.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17360/17473 [02:52<00:01, 100.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22:  99% 17380/17473 [02:52<00:00, 100.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22: 100% 17400/17473 [02:52<00:00, 100.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22: 100% 17420/17473 [02:52<00:00, 100.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22: 100% 17440/17473 [02:53<00:00, 100.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22: 100% 17460/17473 [02:53<00:00, 100.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 22: 100% 17473/17473 [02:53<00:00, 100.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.382\n",
            "Epoch 23:  80% 13960/17473 [02:30<00:37, 92.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  80% 13980/17473 [02:35<00:38, 89.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  80% 14000/17473 [02:35<00:38, 90.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  80% 14020/17473 [02:35<00:38, 90.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  80% 14040/17473 [02:35<00:38, 90.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  80% 14060/17473 [02:35<00:37, 90.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14080/17473 [02:35<00:37, 90.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14100/17473 [02:36<00:37, 90.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14120/17473 [02:36<00:37, 90.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14140/17473 [02:36<00:36, 90.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14160/17473 [02:36<00:36, 90.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14180/17473 [02:36<00:36, 90.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14200/17473 [02:36<00:36, 90.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14220/17473 [02:36<00:35, 90.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  81% 14240/17473 [02:36<00:35, 90.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14260/17473 [02:36<00:35, 90.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14280/17473 [02:36<00:35, 90.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14300/17473 [02:37<00:34, 91.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14320/17473 [02:37<00:34, 91.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14340/17473 [02:37<00:34, 91.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14360/17473 [02:37<00:34, 91.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14380/17473 [02:37<00:33, 91.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  82% 14400/17473 [02:37<00:33, 91.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14420/17473 [02:37<00:33, 91.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14440/17473 [02:37<00:33, 91.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14460/17473 [02:37<00:32, 91.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14480/17473 [02:37<00:32, 91.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14500/17473 [02:38<00:32, 91.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14520/17473 [02:38<00:32, 91.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14540/17473 [02:38<00:31, 91.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14560/17473 [02:38<00:31, 91.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  83% 14580/17473 [02:38<00:31, 92.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14600/17473 [02:38<00:31, 92.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14620/17473 [02:38<00:30, 92.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14640/17473 [02:38<00:30, 92.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14660/17473 [02:38<00:30, 92.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14680/17473 [02:38<00:30, 92.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14700/17473 [02:39<00:30, 92.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14720/17473 [02:39<00:29, 92.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14740/17473 [02:39<00:29, 92.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  84% 14760/17473 [02:39<00:29, 92.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14780/17473 [02:39<00:29, 92.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14800/17473 [02:39<00:28, 92.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14820/17473 [02:39<00:28, 92.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14840/17473 [02:39<00:28, 92.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14860/17473 [02:39<00:28, 92.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14880/17473 [02:39<00:27, 93.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14900/17473 [02:40<00:27, 93.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  85% 14920/17473 [02:40<00:27, 93.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 14940/17473 [02:40<00:27, 93.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 14960/17473 [02:40<00:26, 93.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 14980/17473 [02:40<00:26, 93.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15000/17473 [02:40<00:26, 93.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15020/17473 [02:40<00:26, 93.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15040/17473 [02:40<00:26, 93.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15060/17473 [02:40<00:25, 93.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15080/17473 [02:41<00:25, 93.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  86% 15100/17473 [02:41<00:25, 93.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15120/17473 [02:41<00:25, 93.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15140/17473 [02:41<00:24, 93.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15160/17473 [02:41<00:24, 93.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15180/17473 [02:41<00:24, 94.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15200/17473 [02:41<00:24, 94.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15220/17473 [02:41<00:23, 94.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15240/17473 [02:41<00:23, 94.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15260/17473 [02:41<00:23, 94.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  87% 15280/17473 [02:42<00:23, 94.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15300/17473 [02:42<00:23, 94.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15320/17473 [02:42<00:22, 94.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15340/17473 [02:42<00:22, 94.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15360/17473 [02:42<00:22, 94.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15380/17473 [02:42<00:22, 94.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15400/17473 [02:42<00:21, 94.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15420/17473 [02:42<00:21, 94.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15440/17473 [02:42<00:21, 94.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  88% 15460/17473 [02:42<00:21, 94.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15480/17473 [02:42<00:20, 94.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15500/17473 [02:43<00:20, 95.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15520/17473 [02:43<00:20, 95.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15540/17473 [02:43<00:20, 95.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15560/17473 [02:43<00:20, 95.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15580/17473 [02:43<00:19, 95.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15600/17473 [02:43<00:19, 95.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  89% 15620/17473 [02:43<00:19, 95.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15640/17473 [02:43<00:19, 95.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15660/17473 [02:43<00:18, 95.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15680/17473 [02:43<00:18, 95.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15700/17473 [02:44<00:18, 95.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15720/17473 [02:44<00:18, 95.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15740/17473 [02:44<00:18, 95.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15760/17473 [02:44<00:17, 95.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15780/17473 [02:44<00:17, 95.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  90% 15800/17473 [02:44<00:17, 96.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15820/17473 [02:44<00:17, 96.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15840/17473 [02:44<00:16, 96.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15860/17473 [02:44<00:16, 96.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15880/17473 [02:44<00:16, 96.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15900/17473 [02:45<00:16, 96.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15920/17473 [02:45<00:16, 96.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15940/17473 [02:45<00:15, 96.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15960/17473 [02:45<00:15, 96.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  91% 15980/17473 [02:45<00:15, 96.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16000/17473 [02:45<00:15, 96.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16020/17473 [02:45<00:15, 96.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16040/17473 [02:45<00:14, 96.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16060/17473 [02:45<00:14, 96.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16080/17473 [02:45<00:14, 96.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16100/17473 [02:46<00:14, 96.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16120/17473 [02:46<00:13, 97.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16140/17473 [02:46<00:13, 97.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  92% 16160/17473 [02:46<00:13, 97.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16180/17473 [02:46<00:13, 97.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16200/17473 [02:46<00:13, 97.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16220/17473 [02:46<00:12, 97.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16240/17473 [02:46<00:12, 97.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16260/17473 [02:46<00:12, 97.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16280/17473 [02:46<00:12, 97.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16300/17473 [02:47<00:12, 97.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  93% 16320/17473 [02:47<00:11, 97.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16340/17473 [02:47<00:11, 97.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16360/17473 [02:47<00:11, 97.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16380/17473 [02:47<00:11, 97.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16400/17473 [02:47<00:10, 97.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16420/17473 [02:47<00:10, 97.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16440/17473 [02:47<00:10, 98.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16460/17473 [02:47<00:10, 98.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16480/17473 [02:47<00:10, 98.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  94% 16500/17473 [02:48<00:09, 98.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16520/17473 [02:48<00:09, 98.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16540/17473 [02:48<00:09, 98.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16560/17473 [02:48<00:09, 98.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16580/17473 [02:48<00:09, 98.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16600/17473 [02:48<00:08, 98.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16620/17473 [02:48<00:08, 98.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16640/17473 [02:48<00:08, 98.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16660/17473 [02:48<00:08, 98.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  95% 16680/17473 [02:48<00:08, 98.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16700/17473 [02:49<00:07, 98.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16720/17473 [02:49<00:07, 98.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16740/17473 [02:49<00:07, 98.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16760/17473 [02:49<00:07, 98.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16780/17473 [02:49<00:06, 99.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16800/17473 [02:49<00:06, 99.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16820/17473 [02:49<00:06, 99.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16840/17473 [02:49<00:06, 99.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  96% 16860/17473 [02:49<00:06, 99.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16880/17473 [02:50<00:05, 99.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16900/17473 [02:50<00:05, 99.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16920/17473 [02:50<00:05, 99.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16940/17473 [02:50<00:05, 99.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16960/17473 [02:50<00:05, 99.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 16980/17473 [02:50<00:04, 99.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 17000/17473 [02:50<00:04, 99.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  97% 17020/17473 [02:50<00:04, 99.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17040/17473 [02:50<00:04, 99.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17060/17473 [02:50<00:04, 99.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17080/17473 [02:50<00:03, 99.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17100/17473 [02:51<00:03, 99.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17120/17473 [02:51<00:03, 100.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17140/17473 [02:51<00:03, 100.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17160/17473 [02:51<00:03, 100.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17180/17473 [02:51<00:02, 100.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  98% 17200/17473 [02:51<00:02, 100.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17220/17473 [02:51<00:02, 100.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17240/17473 [02:51<00:02, 100.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17260/17473 [02:51<00:02, 100.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17280/17473 [02:51<00:01, 100.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17300/17473 [02:52<00:01, 100.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17320/17473 [02:52<00:01, 100.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17340/17473 [02:52<00:01, 100.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17360/17473 [02:52<00:01, 100.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23:  99% 17380/17473 [02:52<00:00, 100.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23: 100% 17400/17473 [02:52<00:00, 100.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23: 100% 17420/17473 [02:52<00:00, 100.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23: 100% 17440/17473 [02:52<00:00, 100.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23: 100% 17460/17473 [02:52<00:00, 101.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 23: 100% 17473/17473 [02:52<00:00, 101.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.370]\n",
            "Epoch 24:  80% 13960/17473 [02:30<00:37, 92.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  80% 13980/17473 [02:35<00:38, 89.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  80% 14000/17473 [02:35<00:38, 90.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  80% 14020/17473 [02:35<00:38, 90.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  80% 14040/17473 [02:35<00:38, 90.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  80% 14060/17473 [02:35<00:37, 90.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14080/17473 [02:35<00:37, 90.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14100/17473 [02:35<00:37, 90.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14120/17473 [02:36<00:37, 90.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14140/17473 [02:36<00:36, 90.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14160/17473 [02:36<00:36, 90.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14180/17473 [02:36<00:36, 90.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14200/17473 [02:36<00:36, 90.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14220/17473 [02:36<00:35, 90.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  81% 14240/17473 [02:36<00:35, 90.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14260/17473 [02:36<00:35, 90.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14280/17473 [02:36<00:35, 91.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14300/17473 [02:37<00:34, 91.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14320/17473 [02:37<00:34, 91.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14340/17473 [02:37<00:34, 91.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14360/17473 [02:37<00:34, 91.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14380/17473 [02:37<00:33, 91.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  82% 14400/17473 [02:37<00:33, 91.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14420/17473 [02:37<00:33, 91.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14440/17473 [02:37<00:33, 91.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14460/17473 [02:37<00:32, 91.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14480/17473 [02:37<00:32, 91.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14500/17473 [02:38<00:32, 91.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14520/17473 [02:38<00:32, 91.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14540/17473 [02:38<00:31, 91.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14560/17473 [02:38<00:31, 91.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  83% 14580/17473 [02:38<00:31, 92.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14600/17473 [02:38<00:31, 92.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14620/17473 [02:38<00:30, 92.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14640/17473 [02:38<00:30, 92.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14660/17473 [02:38<00:30, 92.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14680/17473 [02:38<00:30, 92.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14700/17473 [02:39<00:30, 92.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14720/17473 [02:39<00:29, 92.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14740/17473 [02:39<00:29, 92.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  84% 14760/17473 [02:39<00:29, 92.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14780/17473 [02:39<00:29, 92.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14800/17473 [02:39<00:28, 92.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14820/17473 [02:39<00:28, 92.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14840/17473 [02:39<00:28, 92.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14860/17473 [02:39<00:28, 92.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14880/17473 [02:39<00:27, 93.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14900/17473 [02:40<00:27, 93.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  85% 14920/17473 [02:40<00:27, 93.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 14940/17473 [02:40<00:27, 93.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 14960/17473 [02:40<00:26, 93.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 14980/17473 [02:40<00:26, 93.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15000/17473 [02:40<00:26, 93.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15020/17473 [02:40<00:26, 93.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15040/17473 [02:40<00:26, 93.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15060/17473 [02:40<00:25, 93.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15080/17473 [02:40<00:25, 93.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  86% 15100/17473 [02:41<00:25, 93.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15120/17473 [02:41<00:25, 93.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15140/17473 [02:41<00:24, 93.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15160/17473 [02:41<00:24, 93.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15180/17473 [02:41<00:24, 94.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15200/17473 [02:41<00:24, 94.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15220/17473 [02:41<00:23, 94.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15240/17473 [02:41<00:23, 94.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15260/17473 [02:41<00:23, 94.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  87% 15280/17473 [02:41<00:23, 94.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15300/17473 [02:42<00:23, 94.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15320/17473 [02:42<00:22, 94.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15340/17473 [02:42<00:22, 94.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15360/17473 [02:42<00:22, 94.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15380/17473 [02:42<00:22, 94.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15400/17473 [02:42<00:21, 94.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15420/17473 [02:42<00:21, 94.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15440/17473 [02:42<00:21, 94.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  88% 15460/17473 [02:42<00:21, 94.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15480/17473 [02:43<00:20, 94.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15500/17473 [02:43<00:20, 95.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15520/17473 [02:43<00:20, 95.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15540/17473 [02:43<00:20, 95.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15560/17473 [02:43<00:20, 95.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15580/17473 [02:43<00:19, 95.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15600/17473 [02:43<00:19, 95.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  89% 15620/17473 [02:43<00:19, 95.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15640/17473 [02:43<00:19, 95.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15660/17473 [02:43<00:18, 95.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15680/17473 [02:44<00:18, 95.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15700/17473 [02:44<00:18, 95.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15720/17473 [02:44<00:18, 95.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15740/17473 [02:44<00:18, 95.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15760/17473 [02:44<00:17, 95.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15780/17473 [02:44<00:17, 95.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  90% 15800/17473 [02:44<00:17, 95.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15820/17473 [02:44<00:17, 96.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15840/17473 [02:44<00:16, 96.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15860/17473 [02:44<00:16, 96.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15880/17473 [02:45<00:16, 96.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15900/17473 [02:45<00:16, 96.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15920/17473 [02:45<00:16, 96.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15940/17473 [02:45<00:15, 96.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15960/17473 [02:45<00:15, 96.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  91% 15980/17473 [02:45<00:15, 96.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16000/17473 [02:45<00:15, 96.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16020/17473 [02:45<00:15, 96.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16040/17473 [02:45<00:14, 96.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16060/17473 [02:45<00:14, 96.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16080/17473 [02:46<00:14, 96.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16100/17473 [02:46<00:14, 96.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16120/17473 [02:46<00:13, 96.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16140/17473 [02:46<00:13, 97.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  92% 16160/17473 [02:46<00:13, 97.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16180/17473 [02:46<00:13, 97.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16200/17473 [02:46<00:13, 97.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16220/17473 [02:46<00:12, 97.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16240/17473 [02:46<00:12, 97.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16260/17473 [02:47<00:12, 97.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16280/17473 [02:47<00:12, 97.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16300/17473 [02:47<00:12, 97.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  93% 16320/17473 [02:47<00:11, 97.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16340/17473 [02:47<00:11, 97.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16360/17473 [02:47<00:11, 97.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16380/17473 [02:47<00:11, 97.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16400/17473 [02:47<00:10, 97.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16420/17473 [02:47<00:10, 97.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16440/17473 [02:47<00:10, 97.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16460/17473 [02:48<00:10, 97.97it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16480/17473 [02:48<00:10, 98.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  94% 16500/17473 [02:48<00:09, 98.09it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16520/17473 [02:48<00:09, 98.16it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16540/17473 [02:48<00:09, 98.22it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16560/17473 [02:48<00:09, 98.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16580/17473 [02:48<00:09, 98.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16600/17473 [02:48<00:08, 98.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16620/17473 [02:48<00:08, 98.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16640/17473 [02:48<00:08, 98.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16660/17473 [02:48<00:08, 98.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  95% 16680/17473 [02:49<00:08, 98.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16700/17473 [02:49<00:07, 98.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16720/17473 [02:49<00:07, 98.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16740/17473 [02:49<00:07, 98.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16760/17473 [02:49<00:07, 98.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16780/17473 [02:49<00:07, 98.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16800/17473 [02:49<00:06, 99.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16820/17473 [02:49<00:06, 99.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16840/17473 [02:49<00:06, 99.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  96% 16860/17473 [02:50<00:06, 99.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16880/17473 [02:50<00:05, 99.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16900/17473 [02:50<00:05, 99.29it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16920/17473 [02:50<00:05, 99.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16940/17473 [02:50<00:05, 99.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16960/17473 [02:50<00:05, 99.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 16980/17473 [02:50<00:04, 99.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 17000/17473 [02:50<00:04, 99.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  97% 17020/17473 [02:50<00:04, 99.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17040/17473 [02:50<00:04, 99.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17060/17473 [02:50<00:04, 99.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17080/17473 [02:51<00:03, 99.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17100/17473 [02:51<00:03, 99.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17120/17473 [02:51<00:03, 99.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17140/17473 [02:51<00:03, 100.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17160/17473 [02:51<00:03, 100.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17180/17473 [02:51<00:02, 100.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  98% 17200/17473 [02:51<00:02, 100.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17220/17473 [02:51<00:02, 100.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17240/17473 [02:51<00:02, 100.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17260/17473 [02:51<00:02, 100.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17280/17473 [02:52<00:01, 100.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17300/17473 [02:52<00:01, 100.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17320/17473 [02:52<00:01, 100.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17340/17473 [02:52<00:01, 100.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17360/17473 [02:52<00:01, 100.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24:  99% 17380/17473 [02:52<00:00, 100.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24: 100% 17400/17473 [02:52<00:00, 100.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24: 100% 17420/17473 [02:52<00:00, 100.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24: 100% 17440/17473 [02:52<00:00, 100.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24: 100% 17460/17473 [02:52<00:00, 100.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 24: 100% 17473/17473 [02:53<00:00, 100.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  80% 13960/17473 [02:30<00:37, 92.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  80% 13980/17473 [02:35<00:38, 90.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  80% 14000/17473 [02:35<00:38, 90.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  80% 14020/17473 [02:35<00:38, 90.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  80% 14040/17473 [02:35<00:38, 90.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  80% 14060/17473 [02:35<00:37, 90.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14080/17473 [02:35<00:37, 90.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14100/17473 [02:35<00:37, 90.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14120/17473 [02:35<00:37, 90.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14140/17473 [02:36<00:36, 90.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14160/17473 [02:36<00:36, 90.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14180/17473 [02:36<00:36, 90.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14200/17473 [02:36<00:36, 90.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14220/17473 [02:36<00:35, 90.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  81% 14240/17473 [02:36<00:35, 90.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14260/17473 [02:36<00:35, 91.03it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14280/17473 [02:36<00:35, 91.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14300/17473 [02:36<00:34, 91.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14320/17473 [02:36<00:34, 91.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14340/17473 [02:37<00:34, 91.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14360/17473 [02:37<00:34, 91.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14380/17473 [02:37<00:33, 91.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  82% 14400/17473 [02:37<00:33, 91.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14420/17473 [02:37<00:33, 91.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14440/17473 [02:37<00:33, 91.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14460/17473 [02:37<00:32, 91.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14480/17473 [02:37<00:32, 91.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14500/17473 [02:37<00:32, 91.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14520/17473 [02:37<00:32, 91.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14540/17473 [02:38<00:31, 91.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14560/17473 [02:38<00:31, 92.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  83% 14580/17473 [02:38<00:31, 92.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14600/17473 [02:38<00:31, 92.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14620/17473 [02:38<00:30, 92.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14640/17473 [02:38<00:30, 92.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14660/17473 [02:38<00:30, 92.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14680/17473 [02:38<00:30, 92.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14700/17473 [02:38<00:29, 92.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14720/17473 [02:38<00:29, 92.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14740/17473 [02:39<00:29, 92.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  84% 14760/17473 [02:39<00:29, 92.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14780/17473 [02:39<00:29, 92.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14800/17473 [02:39<00:28, 92.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14820/17473 [02:39<00:28, 92.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14840/17473 [02:39<00:28, 93.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14860/17473 [02:39<00:28, 93.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14880/17473 [02:39<00:27, 93.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14900/17473 [02:39<00:27, 93.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  85% 14920/17473 [02:39<00:27, 93.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 14940/17473 [02:40<00:27, 93.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 14960/17473 [02:40<00:26, 93.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 14980/17473 [02:40<00:26, 93.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15000/17473 [02:40<00:26, 93.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15020/17473 [02:40<00:26, 93.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15040/17473 [02:40<00:25, 93.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15060/17473 [02:40<00:25, 93.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15080/17473 [02:40<00:25, 93.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  86% 15100/17473 [02:40<00:25, 93.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15120/17473 [02:40<00:25, 93.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15140/17473 [02:41<00:24, 93.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15160/17473 [02:41<00:24, 94.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15180/17473 [02:41<00:24, 94.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15200/17473 [02:41<00:24, 94.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15220/17473 [02:41<00:23, 94.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15240/17473 [02:41<00:23, 94.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15260/17473 [02:41<00:23, 94.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  87% 15280/17473 [02:41<00:23, 94.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15300/17473 [02:41<00:22, 94.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15320/17473 [02:41<00:22, 94.58it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15340/17473 [02:42<00:22, 94.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15360/17473 [02:42<00:22, 94.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15380/17473 [02:42<00:22, 94.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15400/17473 [02:42<00:21, 94.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15420/17473 [02:42<00:21, 94.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15440/17473 [02:42<00:21, 94.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  88% 15460/17473 [02:42<00:21, 95.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15480/17473 [02:42<00:20, 95.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15500/17473 [02:42<00:20, 95.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15520/17473 [02:43<00:20, 95.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15540/17473 [02:43<00:20, 95.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15560/17473 [02:43<00:20, 95.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15580/17473 [02:43<00:19, 95.40it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15600/17473 [02:43<00:19, 95.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  89% 15620/17473 [02:43<00:19, 95.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15640/17473 [02:43<00:19, 95.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15660/17473 [02:43<00:18, 95.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15680/17473 [02:43<00:18, 95.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15700/17473 [02:43<00:18, 95.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15720/17473 [02:44<00:18, 95.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15740/17473 [02:44<00:18, 95.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15760/17473 [02:44<00:17, 95.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15780/17473 [02:44<00:17, 96.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  90% 15800/17473 [02:44<00:17, 96.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15820/17473 [02:44<00:17, 96.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15840/17473 [02:44<00:16, 96.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15860/17473 [02:44<00:16, 96.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15880/17473 [02:44<00:16, 96.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15900/17473 [02:44<00:16, 96.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15920/17473 [02:45<00:16, 96.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15940/17473 [02:45<00:15, 96.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15960/17473 [02:45<00:15, 96.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  91% 15980/17473 [02:45<00:15, 96.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16000/17473 [02:45<00:15, 96.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16020/17473 [02:45<00:15, 96.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16040/17473 [02:45<00:14, 96.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16060/17473 [02:45<00:14, 96.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16080/17473 [02:45<00:14, 96.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16100/17473 [02:45<00:14, 97.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16120/17473 [02:46<00:13, 97.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16140/17473 [02:46<00:13, 97.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  92% 16160/17473 [02:46<00:13, 97.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16180/17473 [02:46<00:13, 97.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16200/17473 [02:46<00:13, 97.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16220/17473 [02:46<00:12, 97.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16240/17473 [02:46<00:12, 97.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16260/17473 [02:46<00:12, 97.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16280/17473 [02:46<00:12, 97.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16300/17473 [02:46<00:12, 97.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  93% 16320/17473 [02:47<00:11, 97.70it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16340/17473 [02:47<00:11, 97.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16360/17473 [02:47<00:11, 97.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16380/17473 [02:47<00:11, 97.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16400/17473 [02:47<00:10, 97.95it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16420/17473 [02:47<00:10, 98.01it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16440/17473 [02:47<00:10, 98.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16460/17473 [02:47<00:10, 98.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16480/17473 [02:47<00:10, 98.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  94% 16500/17473 [02:47<00:09, 98.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16520/17473 [02:48<00:09, 98.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16540/17473 [02:48<00:09, 98.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16560/17473 [02:48<00:09, 98.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16580/17473 [02:48<00:09, 98.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16600/17473 [02:48<00:08, 98.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16620/17473 [02:48<00:08, 98.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16640/17473 [02:48<00:08, 98.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16660/17473 [02:48<00:08, 98.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  95% 16680/17473 [02:48<00:08, 98.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16700/17473 [02:48<00:07, 98.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16720/17473 [02:49<00:07, 98.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16740/17473 [02:49<00:07, 98.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16760/17473 [02:49<00:07, 99.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16780/17473 [02:49<00:06, 99.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16800/17473 [02:49<00:06, 99.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16820/17473 [02:49<00:06, 99.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16840/17473 [02:49<00:06, 99.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  96% 16860/17473 [02:49<00:06, 99.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16880/17473 [02:49<00:05, 99.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16900/17473 [02:49<00:05, 99.41it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16920/17473 [02:50<00:05, 99.47it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16940/17473 [02:50<00:05, 99.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16960/17473 [02:50<00:05, 99.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 16980/17473 [02:50<00:04, 99.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 17000/17473 [02:50<00:04, 99.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  97% 17020/17473 [02:50<00:04, 99.76it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17040/17473 [02:50<00:04, 99.82it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17060/17473 [02:50<00:04, 99.88it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17080/17473 [02:50<00:03, 99.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17100/17473 [02:50<00:03, 100.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17120/17473 [02:51<00:03, 100.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17140/17473 [02:51<00:03, 100.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17160/17473 [02:51<00:03, 100.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17180/17473 [02:51<00:02, 100.23it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  98% 17200/17473 [02:51<00:02, 100.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17220/17473 [02:51<00:02, 100.34it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17240/17473 [02:51<00:02, 100.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17260/17473 [02:51<00:02, 100.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17280/17473 [02:51<00:01, 100.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17300/17473 [02:52<00:01, 100.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17320/17473 [02:52<00:01, 100.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17340/17473 [02:52<00:01, 100.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17360/17473 [02:52<00:01, 100.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25:  99% 17380/17473 [02:52<00:00, 100.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25: 100% 17400/17473 [02:52<00:00, 100.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25: 100% 17420/17473 [02:52<00:00, 100.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25: 100% 17440/17473 [02:52<00:00, 100.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25: 100% 17460/17473 [02:52<00:00, 101.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 25: 100% 17473/17473 [02:52<00:00, 101.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  80% 13960/17473 [02:30<00:37, 92.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  80% 13980/17473 [02:34<00:38, 90.28it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  80% 14000/17473 [02:34<00:38, 90.35it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  80% 14020/17473 [02:35<00:38, 90.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  80% 14040/17473 [02:35<00:37, 90.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  80% 14060/17473 [02:35<00:37, 90.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14080/17473 [02:35<00:37, 90.64it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14100/17473 [02:35<00:37, 90.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14120/17473 [02:35<00:36, 90.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14140/17473 [02:35<00:36, 90.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14160/17473 [02:35<00:36, 90.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14180/17473 [02:35<00:36, 90.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14200/17473 [02:35<00:35, 91.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14220/17473 [02:36<00:35, 91.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  81% 14240/17473 [02:36<00:35, 91.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14260/17473 [02:36<00:35, 91.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14280/17473 [02:36<00:34, 91.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14300/17473 [02:36<00:34, 91.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14320/17473 [02:36<00:34, 91.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14340/17473 [02:36<00:34, 91.52it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14360/17473 [02:36<00:33, 91.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14380/17473 [02:36<00:33, 91.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  82% 14400/17473 [02:36<00:33, 91.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14420/17473 [02:37<00:33, 91.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14440/17473 [02:37<00:33, 91.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14460/17473 [02:37<00:32, 91.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14480/17473 [02:37<00:32, 91.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14500/17473 [02:37<00:32, 92.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14520/17473 [02:37<00:32, 92.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14540/17473 [02:37<00:31, 92.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14560/17473 [02:37<00:31, 92.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  83% 14580/17473 [02:37<00:31, 92.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14600/17473 [02:38<00:31, 92.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14620/17473 [02:38<00:30, 92.46it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14640/17473 [02:38<00:30, 92.53it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14660/17473 [02:38<00:30, 92.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14680/17473 [02:38<00:30, 92.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14700/17473 [02:38<00:29, 92.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14720/17473 [02:38<00:29, 92.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14740/17473 [02:38<00:29, 92.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  84% 14760/17473 [02:38<00:29, 92.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14780/17473 [02:38<00:28, 92.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14800/17473 [02:39<00:28, 93.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14820/17473 [02:39<00:28, 93.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14840/17473 [02:39<00:28, 93.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14860/17473 [02:39<00:28, 93.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14880/17473 [02:39<00:27, 93.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14900/17473 [02:39<00:27, 93.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  85% 14920/17473 [02:39<00:27, 93.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 14940/17473 [02:39<00:27, 93.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 14960/17473 [02:39<00:26, 93.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 14980/17473 [02:39<00:26, 93.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15000/17473 [02:40<00:26, 93.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15020/17473 [02:40<00:26, 93.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15040/17473 [02:40<00:25, 93.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15060/17473 [02:40<00:25, 93.85it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15080/17473 [02:40<00:25, 93.91it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  86% 15100/17473 [02:40<00:25, 93.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15120/17473 [02:40<00:25, 94.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15140/17473 [02:40<00:24, 94.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15160/17473 [02:40<00:24, 94.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15180/17473 [02:41<00:24, 94.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15200/17473 [02:41<00:24, 94.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15220/17473 [02:41<00:23, 94.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15240/17473 [02:41<00:23, 94.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15260/17473 [02:41<00:23, 94.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  87% 15280/17473 [02:41<00:23, 94.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15300/17473 [02:41<00:22, 94.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15320/17473 [02:41<00:22, 94.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15340/17473 [02:41<00:22, 94.74it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15360/17473 [02:42<00:22, 94.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15380/17473 [02:42<00:22, 94.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15400/17473 [02:42<00:21, 94.93it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15420/17473 [02:42<00:21, 94.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15440/17473 [02:42<00:21, 95.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  88% 15460/17473 [02:42<00:21, 95.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15480/17473 [02:42<00:20, 95.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15500/17473 [02:42<00:20, 95.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15520/17473 [02:42<00:20, 95.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15540/17473 [02:42<00:20, 95.38it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15560/17473 [02:43<00:20, 95.44it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15580/17473 [02:43<00:19, 95.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15600/17473 [02:43<00:19, 95.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  89% 15620/17473 [02:43<00:19, 95.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15640/17473 [02:43<00:19, 95.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15660/17473 [02:43<00:18, 95.73it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15680/17473 [02:43<00:18, 95.79it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15700/17473 [02:43<00:18, 95.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15720/17473 [02:43<00:18, 95.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15740/17473 [02:43<00:18, 95.99it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15760/17473 [02:44<00:17, 96.05it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15780/17473 [02:44<00:17, 96.11it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  90% 15800/17473 [02:44<00:17, 96.17it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15820/17473 [02:44<00:17, 96.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15840/17473 [02:44<00:16, 96.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15860/17473 [02:44<00:16, 96.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15880/17473 [02:44<00:16, 96.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15900/17473 [02:44<00:16, 96.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15920/17473 [02:44<00:16, 96.55it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15940/17473 [02:44<00:15, 96.62it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15960/17473 [02:45<00:15, 96.68it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  91% 15980/17473 [02:45<00:15, 96.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16000/17473 [02:45<00:15, 96.81it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16020/17473 [02:45<00:14, 96.87it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16040/17473 [02:45<00:14, 96.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16060/17473 [02:45<00:14, 97.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16080/17473 [02:45<00:14, 97.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16100/17473 [02:45<00:14, 97.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16120/17473 [02:45<00:13, 97.18it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16140/17473 [02:45<00:13, 97.24it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  92% 16160/17473 [02:46<00:13, 97.30it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16180/17473 [02:46<00:13, 97.36it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16200/17473 [02:46<00:13, 97.42it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16220/17473 [02:46<00:12, 97.48it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16240/17473 [02:46<00:12, 97.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16260/17473 [02:46<00:12, 97.59it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16280/17473 [02:46<00:12, 97.65it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16300/17473 [02:46<00:12, 97.71it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  93% 16320/17473 [02:46<00:11, 97.77it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16340/17473 [02:47<00:11, 97.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16360/17473 [02:47<00:11, 97.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16380/17473 [02:47<00:11, 97.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16400/17473 [02:47<00:10, 98.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16420/17473 [02:47<00:10, 98.08it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16440/17473 [02:47<00:10, 98.14it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16460/17473 [02:47<00:10, 98.20it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16480/17473 [02:47<00:10, 98.26it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  94% 16500/17473 [02:47<00:09, 98.32it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16520/17473 [02:47<00:09, 98.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16540/17473 [02:48<00:09, 98.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16560/17473 [02:48<00:09, 98.51it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16580/17473 [02:48<00:09, 98.57it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16600/17473 [02:48<00:08, 98.63it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16620/17473 [02:48<00:08, 98.69it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16640/17473 [02:48<00:08, 98.75it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16660/17473 [02:48<00:08, 98.80it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  95% 16680/17473 [02:48<00:08, 98.86it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16700/17473 [02:48<00:07, 98.92it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16720/17473 [02:48<00:07, 98.98it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16740/17473 [02:49<00:07, 99.04it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16760/17473 [02:49<00:07, 99.10it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16780/17473 [02:49<00:06, 99.15it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16800/17473 [02:49<00:06, 99.21it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16820/17473 [02:49<00:06, 99.27it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16840/17473 [02:49<00:06, 99.33it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  96% 16860/17473 [02:49<00:06, 99.39it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16880/17473 [02:49<00:05, 99.45it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16900/17473 [02:49<00:05, 99.50it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16920/17473 [02:49<00:05, 99.56it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16940/17473 [02:50<00:05, 99.61it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16960/17473 [02:50<00:05, 99.67it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 16980/17473 [02:50<00:04, 99.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 17000/17473 [02:50<00:04, 99.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  97% 17020/17473 [02:50<00:04, 99.84it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17040/17473 [02:50<00:04, 99.90it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17060/17473 [02:50<00:04, 99.96it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17080/17473 [02:50<00:03, 100.02it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17100/17473 [02:50<00:03, 100.07it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17120/17473 [02:50<00:03, 100.13it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17140/17473 [02:51<00:03, 100.19it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17160/17473 [02:51<00:03, 100.25it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17180/17473 [02:51<00:02, 100.31it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  98% 17200/17473 [02:51<00:02, 100.37it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17220/17473 [02:51<00:02, 100.43it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17240/17473 [02:51<00:02, 100.49it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17260/17473 [02:51<00:02, 100.54it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17280/17473 [02:51<00:01, 100.60it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17300/17473 [02:51<00:01, 100.66it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17320/17473 [02:51<00:01, 100.72it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17340/17473 [02:52<00:01, 100.78it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17360/17473 [02:52<00:01, 100.83it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26:  99% 17380/17473 [02:52<00:00, 100.89it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26: 100% 17400/17473 [02:52<00:00, 100.94it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26: 100% 17420/17473 [02:52<00:00, 101.00it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26: 100% 17440/17473 [02:52<00:00, 101.06it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26: 100% 17460/17473 [02:52<00:00, 101.12it/s, loss=3.37, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 26: 100% 17473/17473 [02:52<00:00, 101.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  80% 13960/17473 [02:33<00:38, 90.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  80% 13980/17473 [02:38<00:39, 88.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  80% 14000/17473 [02:38<00:39, 88.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  80% 14020/17473 [02:38<00:39, 88.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  80% 14040/17473 [02:38<00:38, 88.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  80% 14060/17473 [02:38<00:38, 88.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14080/17473 [02:38<00:38, 88.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14100/17473 [02:38<00:38, 88.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14120/17473 [02:39<00:37, 88.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14140/17473 [02:39<00:37, 88.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14160/17473 [02:39<00:37, 88.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14180/17473 [02:39<00:37, 88.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14200/17473 [02:39<00:36, 89.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14220/17473 [02:39<00:36, 89.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  81% 14240/17473 [02:39<00:36, 89.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14260/17473 [02:39<00:36, 89.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14280/17473 [02:39<00:35, 89.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14300/17473 [02:40<00:35, 89.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14320/17473 [02:40<00:35, 89.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14340/17473 [02:40<00:35, 89.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14360/17473 [02:40<00:34, 89.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14380/17473 [02:40<00:34, 89.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  82% 14400/17473 [02:40<00:34, 89.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14420/17473 [02:40<00:34, 89.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14440/17473 [02:40<00:33, 89.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14460/17473 [02:40<00:33, 89.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14480/17473 [02:40<00:33, 89.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14500/17473 [02:41<00:33, 90.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14520/17473 [02:41<00:32, 90.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14540/17473 [02:41<00:32, 90.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14560/17473 [02:41<00:32, 90.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  83% 14580/17473 [02:41<00:32, 90.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14600/17473 [02:41<00:31, 90.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14620/17473 [02:41<00:31, 90.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14640/17473 [02:41<00:31, 90.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14660/17473 [02:41<00:31, 90.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14680/17473 [02:41<00:30, 90.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14700/17473 [02:42<00:30, 90.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14720/17473 [02:42<00:30, 90.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14740/17473 [02:42<00:30, 90.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  84% 14760/17473 [02:42<00:29, 90.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14780/17473 [02:42<00:29, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14800/17473 [02:42<00:29, 91.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14820/17473 [02:42<00:29, 91.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14840/17473 [02:42<00:28, 91.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14860/17473 [02:42<00:28, 91.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14880/17473 [02:42<00:28, 91.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14900/17473 [02:43<00:28, 91.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  85% 14920/17473 [02:43<00:27, 91.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 14940/17473 [02:43<00:27, 91.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 14960/17473 [02:43<00:27, 91.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 14980/17473 [02:43<00:27, 91.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15000/17473 [02:43<00:26, 91.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15020/17473 [02:43<00:26, 91.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15040/17473 [02:43<00:26, 91.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15060/17473 [02:43<00:26, 91.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15080/17473 [02:43<00:26, 91.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  86% 15100/17473 [02:44<00:25, 92.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15120/17473 [02:44<00:25, 92.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15140/17473 [02:44<00:25, 92.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15160/17473 [02:44<00:25, 92.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15180/17473 [02:44<00:24, 92.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15200/17473 [02:44<00:24, 92.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15220/17473 [02:44<00:24, 92.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15240/17473 [02:44<00:24, 92.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15260/17473 [02:44<00:23, 92.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  87% 15280/17473 [02:44<00:23, 92.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15300/17473 [02:45<00:23, 92.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15320/17473 [02:45<00:23, 92.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15340/17473 [02:45<00:22, 92.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15360/17473 [02:45<00:22, 92.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15380/17473 [02:45<00:22, 92.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15400/17473 [02:45<00:22, 93.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15420/17473 [02:45<00:22, 93.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15440/17473 [02:45<00:21, 93.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  88% 15460/17473 [02:45<00:21, 93.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15480/17473 [02:46<00:21, 93.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15500/17473 [02:46<00:21, 93.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15520/17473 [02:46<00:20, 93.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15540/17473 [02:46<00:20, 93.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15560/17473 [02:46<00:20, 93.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15580/17473 [02:46<00:20, 93.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15600/17473 [02:46<00:20, 93.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  89% 15620/17473 [02:46<00:19, 93.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15640/17473 [02:46<00:19, 93.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15660/17473 [02:46<00:19, 93.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15680/17473 [02:47<00:19, 93.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15700/17473 [02:47<00:18, 93.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15720/17473 [02:47<00:18, 93.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15740/17473 [02:47<00:18, 94.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15760/17473 [02:47<00:18, 94.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15780/17473 [02:47<00:17, 94.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  90% 15800/17473 [02:47<00:17, 94.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15820/17473 [02:47<00:17, 94.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15840/17473 [02:47<00:17, 94.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15860/17473 [02:48<00:17, 94.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15880/17473 [02:48<00:16, 94.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15900/17473 [02:48<00:16, 94.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15920/17473 [02:48<00:16, 94.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15940/17473 [02:48<00:16, 94.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15960/17473 [02:48<00:15, 94.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  91% 15980/17473 [02:48<00:15, 94.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16000/17473 [02:48<00:15, 94.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16020/17473 [02:48<00:15, 94.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16040/17473 [02:48<00:15, 94.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16060/17473 [02:49<00:14, 95.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16080/17473 [02:49<00:14, 95.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16100/17473 [02:49<00:14, 95.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16120/17473 [02:49<00:14, 95.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16140/17473 [02:49<00:13, 95.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  92% 16160/17473 [02:49<00:13, 95.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16180/17473 [02:49<00:13, 95.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16200/17473 [02:49<00:13, 95.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16220/17473 [02:49<00:13, 95.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16240/17473 [02:49<00:12, 95.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16260/17473 [02:50<00:12, 95.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16280/17473 [02:50<00:12, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16300/17473 [02:50<00:12, 95.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  93% 16320/17473 [02:50<00:12, 95.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16340/17473 [02:50<00:11, 95.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16360/17473 [02:50<00:11, 95.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16380/17473 [02:50<00:11, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16400/17473 [02:50<00:11, 96.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16420/17473 [02:50<00:10, 96.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16440/17473 [02:50<00:10, 96.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16460/17473 [02:51<00:10, 96.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16480/17473 [02:51<00:10, 96.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  94% 16500/17473 [02:51<00:10, 96.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16520/17473 [02:51<00:09, 96.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16540/17473 [02:51<00:09, 96.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16560/17473 [02:51<00:09, 96.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16580/17473 [02:51<00:09, 96.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16600/17473 [02:51<00:09, 96.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16620/17473 [02:51<00:08, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16640/17473 [02:51<00:08, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16660/17473 [02:52<00:08, 96.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  95% 16680/17473 [02:52<00:08, 96.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16700/17473 [02:52<00:07, 96.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16720/17473 [02:52<00:07, 97.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16740/17473 [02:52<00:07, 97.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16760/17473 [02:52<00:07, 97.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16780/17473 [02:52<00:07, 97.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16800/17473 [02:52<00:06, 97.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16820/17473 [02:52<00:06, 97.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16840/17473 [02:52<00:06, 97.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  96% 16860/17473 [02:53<00:06, 97.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16880/17473 [02:53<00:06, 97.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16900/17473 [02:53<00:05, 97.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16920/17473 [02:53<00:05, 97.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16940/17473 [02:53<00:05, 97.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16960/17473 [02:53<00:05, 97.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 16980/17473 [02:53<00:05, 97.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 17000/17473 [02:53<00:04, 97.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  97% 17020/17473 [02:53<00:04, 97.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17040/17473 [02:53<00:04, 97.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17060/17473 [02:54<00:04, 97.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17080/17473 [02:54<00:04, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17100/17473 [02:54<00:03, 98.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17120/17473 [02:54<00:03, 98.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17140/17473 [02:54<00:03, 98.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17160/17473 [02:54<00:03, 98.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17180/17473 [02:54<00:02, 98.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  98% 17200/17473 [02:54<00:02, 98.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17220/17473 [02:54<00:02, 98.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17240/17473 [02:55<00:02, 98.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17260/17473 [02:55<00:02, 98.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17280/17473 [02:55<00:01, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17300/17473 [02:55<00:01, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17320/17473 [02:55<00:01, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17340/17473 [02:55<00:01, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17360/17473 [02:55<00:01, 98.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27:  99% 17380/17473 [02:55<00:00, 98.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27: 100% 17400/17473 [02:55<00:00, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27: 100% 17420/17473 [02:55<00:00, 99.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27: 100% 17440/17473 [02:56<00:00, 99.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27: 100% 17460/17473 [02:56<00:00, 99.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 27: 100% 17473/17473 [02:56<00:00, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.381\n",
            "Epoch 28:  80% 13960/17473 [02:32<00:38, 91.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  80% 13980/17473 [02:37<00:39, 88.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  80% 14000/17473 [02:37<00:39, 88.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  80% 14020/17473 [02:37<00:38, 89.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  80% 14040/17473 [02:37<00:38, 89.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  80% 14060/17473 [02:37<00:38, 89.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14080/17473 [02:37<00:38, 89.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14100/17473 [02:37<00:37, 89.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14120/17473 [02:38<00:37, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14140/17473 [02:38<00:37, 89.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14160/17473 [02:38<00:37, 89.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14180/17473 [02:38<00:36, 89.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14200/17473 [02:38<00:36, 89.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14220/17473 [02:38<00:36, 89.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  81% 14240/17473 [02:38<00:36, 89.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14260/17473 [02:38<00:35, 89.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14280/17473 [02:38<00:35, 89.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14300/17473 [02:38<00:35, 89.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14320/17473 [02:39<00:35, 90.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14340/17473 [02:39<00:34, 90.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14360/17473 [02:39<00:34, 90.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14380/17473 [02:39<00:34, 90.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  82% 14400/17473 [02:39<00:34, 90.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14420/17473 [02:39<00:33, 90.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14440/17473 [02:39<00:33, 90.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14460/17473 [02:39<00:33, 90.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14480/17473 [02:39<00:33, 90.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14500/17473 [02:39<00:32, 90.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14520/17473 [02:40<00:32, 90.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14540/17473 [02:40<00:32, 90.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14560/17473 [02:40<00:32, 90.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  83% 14580/17473 [02:40<00:31, 90.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14600/17473 [02:40<00:31, 90.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14620/17473 [02:40<00:31, 91.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14640/17473 [02:40<00:31, 91.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14660/17473 [02:40<00:30, 91.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14680/17473 [02:40<00:30, 91.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14700/17473 [02:40<00:30, 91.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14720/17473 [02:41<00:30, 91.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14740/17473 [02:41<00:29, 91.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  84% 14760/17473 [02:41<00:29, 91.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14780/17473 [02:41<00:29, 91.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14800/17473 [02:41<00:29, 91.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14820/17473 [02:41<00:28, 91.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14840/17473 [02:41<00:28, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14860/17473 [02:41<00:28, 91.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14880/17473 [02:41<00:28, 91.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14900/17473 [02:41<00:27, 91.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  85% 14920/17473 [02:42<00:27, 92.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 14940/17473 [02:42<00:27, 92.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 14960/17473 [02:42<00:27, 92.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 14980/17473 [02:42<00:27, 92.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15000/17473 [02:42<00:26, 92.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15020/17473 [02:42<00:26, 92.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15040/17473 [02:42<00:26, 92.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15060/17473 [02:42<00:26, 92.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15080/17473 [02:42<00:25, 92.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  86% 15100/17473 [02:43<00:25, 92.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15120/17473 [02:43<00:25, 92.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15140/17473 [02:43<00:25, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15160/17473 [02:43<00:24, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15180/17473 [02:43<00:24, 92.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15200/17473 [02:43<00:24, 92.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15220/17473 [02:43<00:24, 93.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15240/17473 [02:43<00:23, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15260/17473 [02:43<00:23, 93.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  87% 15280/17473 [02:43<00:23, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15300/17473 [02:43<00:23, 93.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15320/17473 [02:44<00:23, 93.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15340/17473 [02:44<00:22, 93.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15360/17473 [02:44<00:22, 93.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15380/17473 [02:44<00:22, 93.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15400/17473 [02:44<00:22, 93.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15420/17473 [02:44<00:21, 93.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15440/17473 [02:44<00:21, 93.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  88% 15460/17473 [02:44<00:21, 93.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15480/17473 [02:44<00:21, 93.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15500/17473 [02:45<00:21, 93.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15520/17473 [02:45<00:20, 93.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15540/17473 [02:45<00:20, 94.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15560/17473 [02:45<00:20, 94.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15580/17473 [02:45<00:20, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15600/17473 [02:45<00:19, 94.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  89% 15620/17473 [02:45<00:19, 94.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15640/17473 [02:45<00:19, 94.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15660/17473 [02:45<00:19, 94.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15680/17473 [02:45<00:18, 94.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15700/17473 [02:46<00:18, 94.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15720/17473 [02:46<00:18, 94.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15740/17473 [02:46<00:18, 94.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15760/17473 [02:46<00:18, 94.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15780/17473 [02:46<00:17, 94.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  90% 15800/17473 [02:46<00:17, 94.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15820/17473 [02:46<00:17, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15840/17473 [02:46<00:17, 94.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15860/17473 [02:46<00:16, 95.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15880/17473 [02:46<00:16, 95.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15900/17473 [02:47<00:16, 95.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15920/17473 [02:47<00:16, 95.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15940/17473 [02:47<00:16, 95.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15960/17473 [02:47<00:15, 95.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  91% 15980/17473 [02:47<00:15, 95.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16000/17473 [02:47<00:15, 95.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16020/17473 [02:47<00:15, 95.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16040/17473 [02:47<00:14, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16060/17473 [02:47<00:14, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16080/17473 [02:47<00:14, 95.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16100/17473 [02:48<00:14, 95.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16120/17473 [02:48<00:14, 95.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16140/17473 [02:48<00:13, 95.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  92% 16160/17473 [02:48<00:13, 95.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16180/17473 [02:48<00:13, 96.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16200/17473 [02:48<00:13, 96.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16220/17473 [02:48<00:13, 96.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16240/17473 [02:48<00:12, 96.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16260/17473 [02:48<00:12, 96.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16280/17473 [02:48<00:12, 96.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16300/17473 [02:49<00:12, 96.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  93% 16320/17473 [02:49<00:11, 96.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16340/17473 [02:49<00:11, 96.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16360/17473 [02:49<00:11, 96.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16380/17473 [02:49<00:11, 96.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16400/17473 [02:49<00:11, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16420/17473 [02:49<00:10, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16440/17473 [02:49<00:10, 96.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16460/17473 [02:49<00:10, 96.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16480/17473 [02:50<00:10, 96.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  94% 16500/17473 [02:50<00:10, 96.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16520/17473 [02:50<00:09, 97.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16540/17473 [02:50<00:09, 97.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16560/17473 [02:50<00:09, 97.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16580/17473 [02:50<00:09, 97.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16600/17473 [02:50<00:08, 97.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16620/17473 [02:50<00:08, 97.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16640/17473 [02:50<00:08, 97.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16660/17473 [02:50<00:08, 97.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  95% 16680/17473 [02:51<00:08, 97.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16700/17473 [02:51<00:07, 97.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16720/17473 [02:51<00:07, 97.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16740/17473 [02:51<00:07, 97.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16760/17473 [02:51<00:07, 97.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16780/17473 [02:51<00:07, 97.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16800/17473 [02:51<00:06, 97.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16820/17473 [02:51<00:06, 97.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16840/17473 [02:51<00:06, 98.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  96% 16860/17473 [02:51<00:06, 98.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16880/17473 [02:52<00:06, 98.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16900/17473 [02:52<00:05, 98.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16920/17473 [02:52<00:05, 98.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16940/17473 [02:52<00:05, 98.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16960/17473 [02:52<00:05, 98.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 16980/17473 [02:52<00:05, 98.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 17000/17473 [02:52<00:04, 98.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  97% 17020/17473 [02:52<00:04, 98.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17040/17473 [02:52<00:04, 98.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17060/17473 [02:53<00:04, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17080/17473 [02:53<00:03, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17100/17473 [02:53<00:03, 98.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17120/17473 [02:53<00:03, 98.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17140/17473 [02:53<00:03, 98.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17160/17473 [02:53<00:03, 98.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17180/17473 [02:53<00:02, 98.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  98% 17200/17473 [02:53<00:02, 98.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17220/17473 [02:53<00:02, 99.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17240/17473 [02:53<00:02, 99.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17260/17473 [02:54<00:02, 99.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17280/17473 [02:54<00:01, 99.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17300/17473 [02:54<00:01, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17320/17473 [02:54<00:01, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17340/17473 [02:54<00:01, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17360/17473 [02:54<00:01, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28:  99% 17380/17473 [02:54<00:00, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28: 100% 17400/17473 [02:54<00:00, 99.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28: 100% 17420/17473 [02:54<00:00, 99.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28: 100% 17440/17473 [02:55<00:00, 99.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28: 100% 17460/17473 [02:55<00:00, 99.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 28: 100% 17473/17473 [02:55<00:00, 99.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  80% 13960/17473 [02:32<00:38, 91.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  80% 13980/17473 [02:37<00:39, 88.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  80% 14000/17473 [02:37<00:39, 89.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  80% 14020/17473 [02:37<00:38, 89.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  80% 14040/17473 [02:37<00:38, 89.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  80% 14060/17473 [02:37<00:38, 89.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14080/17473 [02:37<00:38, 89.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14100/17473 [02:37<00:37, 89.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14120/17473 [02:37<00:37, 89.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14140/17473 [02:38<00:37, 89.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14160/17473 [02:38<00:36, 89.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14180/17473 [02:38<00:36, 89.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14200/17473 [02:38<00:36, 89.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14220/17473 [02:38<00:36, 89.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  81% 14240/17473 [02:38<00:35, 89.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14260/17473 [02:38<00:35, 89.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14280/17473 [02:38<00:35, 89.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14300/17473 [02:38<00:35, 90.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14320/17473 [02:38<00:34, 90.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14340/17473 [02:39<00:34, 90.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14360/17473 [02:39<00:34, 90.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14380/17473 [02:39<00:34, 90.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  82% 14400/17473 [02:39<00:34, 90.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14420/17473 [02:39<00:33, 90.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14440/17473 [02:39<00:33, 90.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14460/17473 [02:39<00:33, 90.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14480/17473 [02:39<00:33, 90.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14500/17473 [02:39<00:32, 90.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14520/17473 [02:39<00:32, 90.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14540/17473 [02:40<00:32, 90.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14560/17473 [02:40<00:32, 90.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  83% 14580/17473 [02:40<00:31, 90.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14600/17473 [02:40<00:31, 91.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14620/17473 [02:40<00:31, 91.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14640/17473 [02:40<00:31, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14660/17473 [02:40<00:30, 91.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14680/17473 [02:40<00:30, 91.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14700/17473 [02:40<00:30, 91.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14720/17473 [02:41<00:30, 91.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14740/17473 [02:41<00:29, 91.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  84% 14760/17473 [02:41<00:29, 91.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14780/17473 [02:41<00:29, 91.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14800/17473 [02:41<00:29, 91.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14820/17473 [02:41<00:28, 91.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14840/17473 [02:41<00:28, 91.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14860/17473 [02:41<00:28, 91.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14880/17473 [02:41<00:28, 91.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14900/17473 [02:41<00:27, 92.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  85% 14920/17473 [02:42<00:27, 92.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 14940/17473 [02:42<00:27, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 14960/17473 [02:42<00:27, 92.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 14980/17473 [02:42<00:27, 92.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15000/17473 [02:42<00:26, 92.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15020/17473 [02:42<00:26, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15040/17473 [02:42<00:26, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15060/17473 [02:42<00:26, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15080/17473 [02:42<00:25, 92.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  86% 15100/17473 [02:42<00:25, 92.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15120/17473 [02:43<00:25, 92.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15140/17473 [02:43<00:25, 92.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15160/17473 [02:43<00:24, 92.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15180/17473 [02:43<00:24, 92.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15200/17473 [02:43<00:24, 92.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15220/17473 [02:43<00:24, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15240/17473 [02:43<00:23, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15260/17473 [02:43<00:23, 93.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  87% 15280/17473 [02:43<00:23, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15300/17473 [02:44<00:23, 93.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15320/17473 [02:44<00:23, 93.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15340/17473 [02:44<00:22, 93.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15360/17473 [02:44<00:22, 93.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15380/17473 [02:44<00:22, 93.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15400/17473 [02:44<00:22, 93.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15420/17473 [02:44<00:21, 93.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15440/17473 [02:44<00:21, 93.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  88% 15460/17473 [02:44<00:21, 93.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15480/17473 [02:44<00:21, 93.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15500/17473 [02:45<00:21, 93.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15520/17473 [02:45<00:20, 93.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15540/17473 [02:45<00:20, 94.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15560/17473 [02:45<00:20, 94.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15580/17473 [02:45<00:20, 94.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15600/17473 [02:45<00:19, 94.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  89% 15620/17473 [02:45<00:19, 94.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15640/17473 [02:45<00:19, 94.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15660/17473 [02:45<00:19, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15680/17473 [02:45<00:18, 94.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15700/17473 [02:46<00:18, 94.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15720/17473 [02:46<00:18, 94.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15740/17473 [02:46<00:18, 94.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15760/17473 [02:46<00:18, 94.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15780/17473 [02:46<00:17, 94.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  90% 15800/17473 [02:46<00:17, 94.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15820/17473 [02:46<00:17, 94.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15840/17473 [02:46<00:17, 95.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15860/17473 [02:46<00:16, 95.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15880/17473 [02:46<00:16, 95.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15900/17473 [02:47<00:16, 95.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15920/17473 [02:47<00:16, 95.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15940/17473 [02:47<00:16, 95.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15960/17473 [02:47<00:15, 95.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  91% 15980/17473 [02:47<00:15, 95.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16000/17473 [02:47<00:15, 95.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16020/17473 [02:47<00:15, 95.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16040/17473 [02:47<00:14, 95.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16060/17473 [02:47<00:14, 95.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16080/17473 [02:47<00:14, 95.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16100/17473 [02:48<00:14, 95.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16120/17473 [02:48<00:14, 95.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16140/17473 [02:48<00:13, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  92% 16160/17473 [02:48<00:13, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16180/17473 [02:48<00:13, 96.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16200/17473 [02:48<00:13, 96.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16220/17473 [02:48<00:13, 96.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16240/17473 [02:48<00:12, 96.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16260/17473 [02:48<00:12, 96.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16280/17473 [02:48<00:12, 96.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16300/17473 [02:49<00:12, 96.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  93% 16320/17473 [02:49<00:11, 96.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16340/17473 [02:49<00:11, 96.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16360/17473 [02:49<00:11, 96.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16380/17473 [02:49<00:11, 96.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16400/17473 [02:49<00:11, 96.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16420/17473 [02:49<00:10, 96.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16440/17473 [02:49<00:10, 96.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16460/17473 [02:49<00:10, 96.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16480/17473 [02:49<00:10, 96.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  94% 16500/17473 [02:50<00:10, 97.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16520/17473 [02:50<00:09, 97.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16540/17473 [02:50<00:09, 97.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16560/17473 [02:50<00:09, 97.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16580/17473 [02:50<00:09, 97.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16600/17473 [02:50<00:08, 97.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16620/17473 [02:50<00:08, 97.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16640/17473 [02:50<00:08, 97.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16660/17473 [02:50<00:08, 97.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  95% 16680/17473 [02:50<00:08, 97.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16700/17473 [02:51<00:07, 97.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16720/17473 [02:51<00:07, 97.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16740/17473 [02:51<00:07, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16760/17473 [02:51<00:07, 97.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16780/17473 [02:51<00:07, 97.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16800/17473 [02:51<00:06, 97.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16820/17473 [02:51<00:06, 97.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16840/17473 [02:51<00:06, 98.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  96% 16860/17473 [02:51<00:06, 98.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16880/17473 [02:52<00:06, 98.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16900/17473 [02:52<00:05, 98.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16920/17473 [02:52<00:05, 98.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16940/17473 [02:52<00:05, 98.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16960/17473 [02:52<00:05, 98.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 16980/17473 [02:52<00:05, 98.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 17000/17473 [02:52<00:04, 98.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  97% 17020/17473 [02:52<00:04, 98.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17040/17473 [02:52<00:04, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17060/17473 [02:52<00:04, 98.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17080/17473 [02:53<00:03, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17100/17473 [02:53<00:03, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17120/17473 [02:53<00:03, 98.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17140/17473 [02:53<00:03, 98.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17160/17473 [02:53<00:03, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17180/17473 [02:53<00:02, 99.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  98% 17200/17473 [02:53<00:02, 99.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17220/17473 [02:53<00:02, 99.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17240/17473 [02:53<00:02, 99.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17260/17473 [02:53<00:02, 99.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17280/17473 [02:54<00:01, 99.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17300/17473 [02:54<00:01, 99.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17320/17473 [02:54<00:01, 99.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17340/17473 [02:54<00:01, 99.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17360/17473 [02:54<00:01, 99.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29:  99% 17380/17473 [02:54<00:00, 99.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29: 100% 17400/17473 [02:54<00:00, 99.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29: 100% 17420/17473 [02:54<00:00, 99.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29: 100% 17440/17473 [02:54<00:00, 99.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29: 100% 17460/17473 [02:54<00:00, 99.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 29: 100% 17473/17473 [02:54<00:00, 99.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  80% 13960/17473 [02:32<00:38, 91.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  80% 13980/17473 [02:37<00:39, 88.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  80% 14000/17473 [02:37<00:39, 89.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  80% 14020/17473 [02:37<00:38, 89.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  80% 14040/17473 [02:37<00:38, 89.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  80% 14060/17473 [02:37<00:38, 89.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14080/17473 [02:37<00:37, 89.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14100/17473 [02:37<00:37, 89.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14120/17473 [02:37<00:37, 89.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14140/17473 [02:37<00:37, 89.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14160/17473 [02:38<00:36, 89.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14180/17473 [02:38<00:36, 89.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14200/17473 [02:38<00:36, 89.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14220/17473 [02:38<00:36, 89.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  81% 14240/17473 [02:38<00:35, 89.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14260/17473 [02:38<00:35, 89.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14280/17473 [02:38<00:35, 89.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14300/17473 [02:38<00:35, 90.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14320/17473 [02:38<00:34, 90.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14340/17473 [02:39<00:34, 90.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14360/17473 [02:39<00:34, 90.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14380/17473 [02:39<00:34, 90.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  82% 14400/17473 [02:39<00:33, 90.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14420/17473 [02:39<00:33, 90.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14440/17473 [02:39<00:33, 90.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14460/17473 [02:39<00:33, 90.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14480/17473 [02:39<00:33, 90.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14500/17473 [02:39<00:32, 90.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14520/17473 [02:39<00:32, 90.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14540/17473 [02:40<00:32, 90.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14560/17473 [02:40<00:32, 90.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  83% 14580/17473 [02:40<00:31, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14600/17473 [02:40<00:31, 91.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14620/17473 [02:40<00:31, 91.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14640/17473 [02:40<00:31, 91.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14660/17473 [02:40<00:30, 91.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14680/17473 [02:40<00:30, 91.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14700/17473 [02:40<00:30, 91.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14720/17473 [02:40<00:30, 91.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14740/17473 [02:41<00:29, 91.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  84% 14760/17473 [02:41<00:29, 91.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14780/17473 [02:41<00:29, 91.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14800/17473 [02:41<00:29, 91.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14820/17473 [02:41<00:28, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14840/17473 [02:41<00:28, 91.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14860/17473 [02:41<00:28, 91.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14880/17473 [02:41<00:28, 91.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14900/17473 [02:41<00:27, 92.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  85% 14920/17473 [02:41<00:27, 92.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 14940/17473 [02:42<00:27, 92.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 14960/17473 [02:42<00:27, 92.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 14980/17473 [02:42<00:27, 92.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15000/17473 [02:42<00:26, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15020/17473 [02:42<00:26, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15040/17473 [02:42<00:26, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15060/17473 [02:42<00:26, 92.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15080/17473 [02:42<00:25, 92.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  86% 15100/17473 [02:42<00:25, 92.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15120/17473 [02:42<00:25, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15140/17473 [02:43<00:25, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15160/17473 [02:43<00:24, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15180/17473 [02:43<00:24, 92.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15200/17473 [02:43<00:24, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15220/17473 [02:43<00:24, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15240/17473 [02:43<00:23, 93.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15260/17473 [02:43<00:23, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  87% 15280/17473 [02:43<00:23, 93.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15300/17473 [02:43<00:23, 93.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15320/17473 [02:43<00:23, 93.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15340/17473 [02:44<00:22, 93.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15360/17473 [02:44<00:22, 93.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15380/17473 [02:44<00:22, 93.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15400/17473 [02:44<00:22, 93.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15420/17473 [02:44<00:21, 93.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15440/17473 [02:44<00:21, 93.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  88% 15460/17473 [02:44<00:21, 93.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15480/17473 [02:44<00:21, 93.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15500/17473 [02:44<00:20, 94.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15520/17473 [02:44<00:20, 94.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15540/17473 [02:45<00:20, 94.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15560/17473 [02:45<00:20, 94.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15580/17473 [02:45<00:20, 94.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15600/17473 [02:45<00:19, 94.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  89% 15620/17473 [02:45<00:19, 94.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15640/17473 [02:45<00:19, 94.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15660/17473 [02:45<00:19, 94.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15680/17473 [02:45<00:18, 94.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15700/17473 [02:45<00:18, 94.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15720/17473 [02:45<00:18, 94.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15740/17473 [02:46<00:18, 94.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15760/17473 [02:46<00:18, 94.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15780/17473 [02:46<00:17, 94.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  90% 15800/17473 [02:46<00:17, 94.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15820/17473 [02:46<00:17, 95.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15840/17473 [02:46<00:17, 95.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15860/17473 [02:46<00:16, 95.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15880/17473 [02:46<00:16, 95.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15900/17473 [02:46<00:16, 95.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15920/17473 [02:47<00:16, 95.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15940/17473 [02:47<00:16, 95.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15960/17473 [02:47<00:15, 95.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  91% 15980/17473 [02:47<00:15, 95.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16000/17473 [02:47<00:15, 95.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16020/17473 [02:47<00:15, 95.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16040/17473 [02:47<00:14, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16060/17473 [02:47<00:14, 95.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16080/17473 [02:47<00:14, 95.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16100/17473 [02:47<00:14, 95.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16120/17473 [02:48<00:14, 95.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16140/17473 [02:48<00:13, 96.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  92% 16160/17473 [02:48<00:13, 96.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16180/17473 [02:48<00:13, 96.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16200/17473 [02:48<00:13, 96.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16220/17473 [02:48<00:13, 96.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16240/17473 [02:48<00:12, 96.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16260/17473 [02:48<00:12, 96.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16280/17473 [02:48<00:12, 96.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16300/17473 [02:48<00:12, 96.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  93% 16320/17473 [02:49<00:11, 96.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16340/17473 [02:49<00:11, 96.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16360/17473 [02:49<00:11, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16380/17473 [02:49<00:11, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16400/17473 [02:49<00:11, 96.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16420/17473 [02:49<00:10, 96.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16440/17473 [02:49<00:10, 96.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16460/17473 [02:49<00:10, 97.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16480/17473 [02:49<00:10, 97.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  94% 16500/17473 [02:49<00:10, 97.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16520/17473 [02:49<00:09, 97.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16540/17473 [02:50<00:09, 97.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16560/17473 [02:50<00:09, 97.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16580/17473 [02:50<00:09, 97.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16600/17473 [02:50<00:08, 97.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16620/17473 [02:50<00:08, 97.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16640/17473 [02:50<00:08, 97.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16660/17473 [02:50<00:08, 97.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  95% 16680/17473 [02:50<00:08, 97.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16700/17473 [02:50<00:07, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16720/17473 [02:50<00:07, 97.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16740/17473 [02:51<00:07, 97.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16760/17473 [02:51<00:07, 97.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16780/17473 [02:51<00:07, 97.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16800/17473 [02:51<00:06, 98.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16820/17473 [02:51<00:06, 98.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16840/17473 [02:51<00:06, 98.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  96% 16860/17473 [02:51<00:06, 98.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16880/17473 [02:51<00:06, 98.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16900/17473 [02:51<00:05, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16920/17473 [02:52<00:05, 98.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16940/17473 [02:52<00:05, 98.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16960/17473 [02:52<00:05, 98.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 16980/17473 [02:52<00:05, 98.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 17000/17473 [02:52<00:04, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  97% 17020/17473 [02:52<00:04, 98.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17040/17473 [02:52<00:04, 98.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17060/17473 [02:52<00:04, 98.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17080/17473 [02:52<00:03, 98.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17100/17473 [02:52<00:03, 98.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17120/17473 [02:53<00:03, 98.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17140/17473 [02:53<00:03, 98.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17160/17473 [02:53<00:03, 99.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17180/17473 [02:53<00:02, 99.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  98% 17200/17473 [02:53<00:02, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17220/17473 [02:53<00:02, 99.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17240/17473 [02:53<00:02, 99.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17260/17473 [02:53<00:02, 99.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17280/17473 [02:53<00:01, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17300/17473 [02:53<00:01, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17320/17473 [02:54<00:01, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17340/17473 [02:54<00:01, 99.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17360/17473 [02:54<00:01, 99.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30:  99% 17380/17473 [02:54<00:00, 99.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30: 100% 17400/17473 [02:54<00:00, 99.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30: 100% 17420/17473 [02:54<00:00, 99.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30: 100% 17440/17473 [02:54<00:00, 99.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30: 100% 17460/17473 [02:54<00:00, 99.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 30: 100% 17473/17473 [02:54<00:00, 99.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  80% 13960/17473 [02:31<00:38, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  80% 13980/17473 [02:36<00:39, 89.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  80% 14000/17473 [02:36<00:38, 89.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  80% 14020/17473 [02:36<00:38, 89.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  80% 14040/17473 [02:36<00:38, 89.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  80% 14060/17473 [02:36<00:38, 89.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14080/17473 [02:37<00:37, 89.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14100/17473 [02:37<00:37, 89.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14120/17473 [02:37<00:37, 89.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14140/17473 [02:37<00:37, 89.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14160/17473 [02:37<00:36, 89.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14180/17473 [02:37<00:36, 90.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14200/17473 [02:37<00:36, 90.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14220/17473 [02:37<00:36, 90.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  81% 14240/17473 [02:37<00:35, 90.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14260/17473 [02:37<00:35, 90.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14280/17473 [02:38<00:35, 90.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14300/17473 [02:38<00:35, 90.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14320/17473 [02:38<00:34, 90.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14340/17473 [02:38<00:34, 90.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14360/17473 [02:38<00:34, 90.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14380/17473 [02:38<00:34, 90.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  82% 14400/17473 [02:38<00:33, 90.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14420/17473 [02:38<00:33, 90.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14440/17473 [02:38<00:33, 90.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14460/17473 [02:38<00:33, 90.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14480/17473 [02:39<00:32, 91.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14500/17473 [02:39<00:32, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14520/17473 [02:39<00:32, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14540/17473 [02:39<00:32, 91.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14560/17473 [02:39<00:31, 91.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  83% 14580/17473 [02:39<00:31, 91.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14600/17473 [02:39<00:31, 91.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14620/17473 [02:39<00:31, 91.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14640/17473 [02:39<00:30, 91.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14660/17473 [02:39<00:30, 91.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14680/17473 [02:40<00:30, 91.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14700/17473 [02:40<00:30, 91.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14720/17473 [02:40<00:29, 91.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14740/17473 [02:40<00:29, 91.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  84% 14760/17473 [02:40<00:29, 91.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14780/17473 [02:40<00:29, 92.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14800/17473 [02:40<00:29, 92.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14820/17473 [02:40<00:28, 92.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14840/17473 [02:40<00:28, 92.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14860/17473 [02:40<00:28, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14880/17473 [02:41<00:28, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14900/17473 [02:41<00:27, 92.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  85% 14920/17473 [02:41<00:27, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 14940/17473 [02:41<00:27, 92.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 14960/17473 [02:41<00:27, 92.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 14980/17473 [02:41<00:26, 92.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15000/17473 [02:41<00:26, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15020/17473 [02:41<00:26, 92.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15040/17473 [02:41<00:26, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15060/17473 [02:41<00:25, 92.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15080/17473 [02:42<00:25, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  86% 15100/17473 [02:42<00:25, 93.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15120/17473 [02:42<00:25, 93.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15140/17473 [02:42<00:25, 93.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15160/17473 [02:42<00:24, 93.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15180/17473 [02:42<00:24, 93.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15200/17473 [02:42<00:24, 93.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15220/17473 [02:42<00:24, 93.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15240/17473 [02:42<00:23, 93.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15260/17473 [02:43<00:23, 93.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  87% 15280/17473 [02:43<00:23, 93.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15300/17473 [02:43<00:23, 93.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15320/17473 [02:43<00:22, 93.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15340/17473 [02:43<00:22, 93.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15360/17473 [02:43<00:22, 93.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15380/17473 [02:43<00:22, 94.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15400/17473 [02:43<00:22, 94.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15420/17473 [02:43<00:21, 94.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15440/17473 [02:43<00:21, 94.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  88% 15460/17473 [02:44<00:21, 94.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15480/17473 [02:44<00:21, 94.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15500/17473 [02:44<00:20, 94.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15520/17473 [02:44<00:20, 94.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15540/17473 [02:44<00:20, 94.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15560/17473 [02:44<00:20, 94.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15580/17473 [02:44<00:20, 94.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15600/17473 [02:44<00:19, 94.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  89% 15620/17473 [02:44<00:19, 94.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15640/17473 [02:44<00:19, 94.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15660/17473 [02:45<00:19, 94.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15680/17473 [02:45<00:18, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15700/17473 [02:45<00:18, 94.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15720/17473 [02:45<00:18, 95.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15740/17473 [02:45<00:18, 95.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15760/17473 [02:45<00:18, 95.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15780/17473 [02:45<00:17, 95.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  90% 15800/17473 [02:45<00:17, 95.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15820/17473 [02:45<00:17, 95.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15840/17473 [02:45<00:17, 95.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15860/17473 [02:46<00:16, 95.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15880/17473 [02:46<00:16, 95.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15900/17473 [02:46<00:16, 95.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15920/17473 [02:46<00:16, 95.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15940/17473 [02:46<00:16, 95.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15960/17473 [02:46<00:15, 95.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  91% 15980/17473 [02:46<00:15, 95.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16000/17473 [02:46<00:15, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16020/17473 [02:46<00:15, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16040/17473 [02:47<00:14, 96.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16060/17473 [02:47<00:14, 96.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16080/17473 [02:47<00:14, 96.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16100/17473 [02:47<00:14, 96.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16120/17473 [02:47<00:14, 96.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16140/17473 [02:47<00:13, 96.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  92% 16160/17473 [02:47<00:13, 96.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16180/17473 [02:47<00:13, 96.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16200/17473 [02:47<00:13, 96.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16220/17473 [02:47<00:12, 96.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16240/17473 [02:48<00:12, 96.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16260/17473 [02:48<00:12, 96.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16280/17473 [02:48<00:12, 96.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16300/17473 [02:48<00:12, 96.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  93% 16320/17473 [02:48<00:11, 96.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16340/17473 [02:48<00:11, 96.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16360/17473 [02:48<00:11, 97.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16380/17473 [02:48<00:11, 97.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16400/17473 [02:48<00:11, 97.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16420/17473 [02:48<00:10, 97.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16440/17473 [02:48<00:10, 97.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16460/17473 [02:49<00:10, 97.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16480/17473 [02:49<00:10, 97.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  94% 16500/17473 [02:49<00:09, 97.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16520/17473 [02:49<00:09, 97.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16540/17473 [02:49<00:09, 97.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16560/17473 [02:49<00:09, 97.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16580/17473 [02:49<00:09, 97.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16600/17473 [02:49<00:08, 97.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16620/17473 [02:49<00:08, 97.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16640/17473 [02:49<00:08, 97.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16660/17473 [02:50<00:08, 97.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  95% 16680/17473 [02:50<00:08, 98.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16700/17473 [02:50<00:07, 98.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16720/17473 [02:50<00:07, 98.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16740/17473 [02:50<00:07, 98.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16760/17473 [02:50<00:07, 98.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16780/17473 [02:50<00:07, 98.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16800/17473 [02:50<00:06, 98.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16820/17473 [02:50<00:06, 98.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16840/17473 [02:50<00:06, 98.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  96% 16860/17473 [02:51<00:06, 98.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16880/17473 [02:51<00:06, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16900/17473 [02:51<00:05, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16920/17473 [02:51<00:05, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16940/17473 [02:51<00:05, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16960/17473 [02:51<00:05, 98.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 16980/17473 [02:51<00:04, 98.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 17000/17473 [02:51<00:04, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  97% 17020/17473 [02:51<00:04, 99.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17040/17473 [02:52<00:04, 99.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17060/17473 [02:52<00:04, 99.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17080/17473 [02:52<00:03, 99.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17100/17473 [02:52<00:03, 99.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17120/17473 [02:52<00:03, 99.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17140/17473 [02:52<00:03, 99.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17160/17473 [02:52<00:03, 99.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17180/17473 [02:52<00:02, 99.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  98% 17200/17473 [02:52<00:02, 99.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17220/17473 [02:52<00:02, 99.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17240/17473 [02:53<00:02, 99.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17260/17473 [02:53<00:02, 99.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17280/17473 [02:53<00:01, 99.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17300/17473 [02:53<00:01, 99.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17320/17473 [02:53<00:01, 99.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17340/17473 [02:53<00:01, 99.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17360/17473 [02:53<00:01, 99.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31:  99% 17380/17473 [02:53<00:00, 100.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31: 100% 17400/17473 [02:53<00:00, 100.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31: 100% 17420/17473 [02:53<00:00, 100.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31: 100% 17440/17473 [02:54<00:00, 100.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31: 100% 17460/17473 [02:54<00:00, 100.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 31: 100% 17473/17473 [02:54<00:00, 100.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  80% 13960/17473 [02:32<00:38, 91.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  80% 13980/17473 [02:36<00:39, 89.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  80% 14000/17473 [02:36<00:38, 89.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  80% 14020/17473 [02:37<00:38, 89.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  80% 14040/17473 [02:37<00:38, 89.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  80% 14060/17473 [02:37<00:38, 89.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14080/17473 [02:37<00:37, 89.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14100/17473 [02:37<00:37, 89.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14120/17473 [02:37<00:37, 89.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14140/17473 [02:37<00:37, 89.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14160/17473 [02:37<00:36, 89.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14180/17473 [02:37<00:36, 89.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14200/17473 [02:37<00:36, 89.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14220/17473 [02:38<00:36, 89.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  81% 14240/17473 [02:38<00:35, 90.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14260/17473 [02:38<00:35, 90.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14280/17473 [02:38<00:35, 90.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14300/17473 [02:38<00:35, 90.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14320/17473 [02:38<00:34, 90.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14340/17473 [02:38<00:34, 90.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14360/17473 [02:38<00:34, 90.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14380/17473 [02:38<00:34, 90.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  82% 14400/17473 [02:38<00:33, 90.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14420/17473 [02:39<00:33, 90.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14440/17473 [02:39<00:33, 90.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14460/17473 [02:39<00:33, 90.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14480/17473 [02:39<00:32, 90.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14500/17473 [02:39<00:32, 90.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14520/17473 [02:39<00:32, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14540/17473 [02:39<00:32, 91.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14560/17473 [02:39<00:31, 91.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  83% 14580/17473 [02:39<00:31, 91.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14600/17473 [02:39<00:31, 91.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14620/17473 [02:40<00:31, 91.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14640/17473 [02:40<00:31, 91.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14660/17473 [02:40<00:30, 91.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14680/17473 [02:40<00:30, 91.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14700/17473 [02:40<00:30, 91.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14720/17473 [02:40<00:30, 91.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14740/17473 [02:40<00:29, 91.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  84% 14760/17473 [02:40<00:29, 91.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14780/17473 [02:40<00:29, 91.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14800/17473 [02:41<00:29, 91.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14820/17473 [02:41<00:28, 91.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14840/17473 [02:41<00:28, 92.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14860/17473 [02:41<00:28, 92.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14880/17473 [02:41<00:28, 92.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14900/17473 [02:41<00:27, 92.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  85% 14920/17473 [02:41<00:27, 92.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 14940/17473 [02:41<00:27, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 14960/17473 [02:41<00:27, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 14980/17473 [02:41<00:26, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15000/17473 [02:42<00:26, 92.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15020/17473 [02:42<00:26, 92.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15040/17473 [02:42<00:26, 92.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15060/17473 [02:42<00:26, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15080/17473 [02:42<00:25, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  86% 15100/17473 [02:42<00:25, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15120/17473 [02:42<00:25, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15140/17473 [02:42<00:25, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15160/17473 [02:42<00:24, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15180/17473 [02:42<00:24, 93.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15200/17473 [02:43<00:24, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15220/17473 [02:43<00:24, 93.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15240/17473 [02:43<00:23, 93.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15260/17473 [02:43<00:23, 93.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  87% 15280/17473 [02:43<00:23, 93.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15300/17473 [02:43<00:23, 93.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15320/17473 [02:43<00:22, 93.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15340/17473 [02:43<00:22, 93.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15360/17473 [02:43<00:22, 93.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15380/17473 [02:43<00:22, 93.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15400/17473 [02:44<00:22, 93.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15420/17473 [02:44<00:21, 93.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15440/17473 [02:44<00:21, 94.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  88% 15460/17473 [02:44<00:21, 94.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15480/17473 [02:44<00:21, 94.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15500/17473 [02:44<00:20, 94.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15520/17473 [02:44<00:20, 94.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15540/17473 [02:44<00:20, 94.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15560/17473 [02:44<00:20, 94.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15580/17473 [02:44<00:20, 94.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15600/17473 [02:45<00:19, 94.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  89% 15620/17473 [02:45<00:19, 94.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15640/17473 [02:45<00:19, 94.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15660/17473 [02:45<00:19, 94.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15680/17473 [02:45<00:18, 94.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15700/17473 [02:45<00:18, 94.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15720/17473 [02:45<00:18, 94.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15740/17473 [02:45<00:18, 94.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15760/17473 [02:45<00:18, 95.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15780/17473 [02:45<00:17, 95.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  90% 15800/17473 [02:46<00:17, 95.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15820/17473 [02:46<00:17, 95.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15840/17473 [02:46<00:17, 95.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15860/17473 [02:46<00:16, 95.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15880/17473 [02:46<00:16, 95.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15900/17473 [02:46<00:16, 95.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15920/17473 [02:46<00:16, 95.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15940/17473 [02:46<00:16, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15960/17473 [02:46<00:15, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  91% 15980/17473 [02:46<00:15, 95.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16000/17473 [02:47<00:15, 95.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16020/17473 [02:47<00:15, 95.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16040/17473 [02:47<00:14, 95.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16060/17473 [02:47<00:14, 95.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16080/17473 [02:47<00:14, 96.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16100/17473 [02:47<00:14, 96.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16120/17473 [02:47<00:14, 96.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16140/17473 [02:47<00:13, 96.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  92% 16160/17473 [02:47<00:13, 96.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16180/17473 [02:47<00:13, 96.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16200/17473 [02:48<00:13, 96.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16220/17473 [02:48<00:12, 96.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16240/17473 [02:48<00:12, 96.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16260/17473 [02:48<00:12, 96.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16280/17473 [02:48<00:12, 96.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16300/17473 [02:48<00:12, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  93% 16320/17473 [02:48<00:11, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16340/17473 [02:48<00:11, 96.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16360/17473 [02:48<00:11, 96.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16380/17473 [02:48<00:11, 96.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16400/17473 [02:49<00:11, 97.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16420/17473 [02:49<00:10, 97.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16440/17473 [02:49<00:10, 97.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16460/17473 [02:49<00:10, 97.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16480/17473 [02:49<00:10, 97.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  94% 16500/17473 [02:49<00:09, 97.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16520/17473 [02:49<00:09, 97.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16540/17473 [02:49<00:09, 97.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16560/17473 [02:49<00:09, 97.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16580/17473 [02:49<00:09, 97.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16600/17473 [02:50<00:08, 97.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16620/17473 [02:50<00:08, 97.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16640/17473 [02:50<00:08, 97.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16660/17473 [02:50<00:08, 97.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  95% 16680/17473 [02:50<00:08, 97.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16700/17473 [02:50<00:07, 97.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16720/17473 [02:50<00:07, 98.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16740/17473 [02:50<00:07, 98.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16760/17473 [02:50<00:07, 98.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16780/17473 [02:50<00:07, 98.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16800/17473 [02:51<00:06, 98.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16820/17473 [02:51<00:06, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16840/17473 [02:51<00:06, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  96% 16860/17473 [02:51<00:06, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16880/17473 [02:51<00:06, 98.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16900/17473 [02:51<00:05, 98.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16920/17473 [02:51<00:05, 98.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16940/17473 [02:51<00:05, 98.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16960/17473 [02:51<00:05, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 16980/17473 [02:51<00:04, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 17000/17473 [02:51<00:04, 98.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  97% 17020/17473 [02:52<00:04, 98.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17040/17473 [02:52<00:04, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17060/17473 [02:52<00:04, 99.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17080/17473 [02:52<00:03, 99.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17100/17473 [02:52<00:03, 99.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17120/17473 [02:52<00:03, 99.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17140/17473 [02:52<00:03, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17160/17473 [02:52<00:03, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17180/17473 [02:52<00:02, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  98% 17200/17473 [02:52<00:02, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17220/17473 [02:53<00:02, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17240/17473 [02:53<00:02, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17260/17473 [02:53<00:02, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17280/17473 [02:53<00:01, 99.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17300/17473 [02:53<00:01, 99.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17320/17473 [02:53<00:01, 99.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17340/17473 [02:53<00:01, 99.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17360/17473 [02:53<00:01, 99.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32:  99% 17380/17473 [02:53<00:00, 99.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32: 100% 17400/17473 [02:53<00:00, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32: 100% 17420/17473 [02:54<00:00, 100.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32: 100% 17440/17473 [02:54<00:00, 100.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32: 100% 17460/17473 [02:54<00:00, 100.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 32: 100% 17473/17473 [02:54<00:00, 100.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  80% 13960/17473 [02:32<00:38, 91.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  80% 13980/17473 [02:36<00:39, 89.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  80% 14000/17473 [02:36<00:38, 89.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  80% 14020/17473 [02:36<00:38, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  80% 14040/17473 [02:37<00:38, 89.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  80% 14060/17473 [02:37<00:38, 89.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14080/17473 [02:37<00:37, 89.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14100/17473 [02:37<00:37, 89.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14120/17473 [02:37<00:37, 89.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14140/17473 [02:37<00:37, 89.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14160/17473 [02:37<00:36, 89.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14180/17473 [02:37<00:36, 89.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14200/17473 [02:37<00:36, 89.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14220/17473 [02:37<00:36, 90.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  81% 14240/17473 [02:38<00:35, 90.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14260/17473 [02:38<00:35, 90.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14280/17473 [02:38<00:35, 90.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14300/17473 [02:38<00:35, 90.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14320/17473 [02:38<00:34, 90.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14340/17473 [02:38<00:34, 90.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14360/17473 [02:38<00:34, 90.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14380/17473 [02:38<00:34, 90.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  82% 14400/17473 [02:38<00:33, 90.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14420/17473 [02:39<00:33, 90.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14440/17473 [02:39<00:33, 90.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14460/17473 [02:39<00:33, 90.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14480/17473 [02:39<00:32, 90.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14500/17473 [02:39<00:32, 90.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14520/17473 [02:39<00:32, 91.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14540/17473 [02:39<00:32, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14560/17473 [02:39<00:31, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  83% 14580/17473 [02:39<00:31, 91.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14600/17473 [02:39<00:31, 91.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14620/17473 [02:40<00:31, 91.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14640/17473 [02:40<00:30, 91.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14660/17473 [02:40<00:30, 91.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14680/17473 [02:40<00:30, 91.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14700/17473 [02:40<00:30, 91.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14720/17473 [02:40<00:30, 91.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14740/17473 [02:40<00:29, 91.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  84% 14760/17473 [02:40<00:29, 91.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14780/17473 [02:40<00:29, 91.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14800/17473 [02:40<00:29, 91.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14820/17473 [02:40<00:28, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14840/17473 [02:41<00:28, 92.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14860/17473 [02:41<00:28, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14880/17473 [02:41<00:28, 92.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14900/17473 [02:41<00:27, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  85% 14920/17473 [02:41<00:27, 92.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 14940/17473 [02:41<00:27, 92.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 14960/17473 [02:41<00:27, 92.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 14980/17473 [02:41<00:26, 92.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15000/17473 [02:41<00:26, 92.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15020/17473 [02:41<00:26, 92.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15040/17473 [02:42<00:26, 92.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15060/17473 [02:42<00:25, 92.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15080/17473 [02:42<00:25, 92.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  86% 15100/17473 [02:42<00:25, 93.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15120/17473 [02:42<00:25, 93.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15140/17473 [02:42<00:25, 93.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15160/17473 [02:42<00:24, 93.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15180/17473 [02:42<00:24, 93.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15200/17473 [02:42<00:24, 93.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15220/17473 [02:42<00:24, 93.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15240/17473 [02:43<00:23, 93.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15260/17473 [02:43<00:23, 93.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  87% 15280/17473 [02:43<00:23, 93.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15300/17473 [02:43<00:23, 93.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15320/17473 [02:43<00:22, 93.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15340/17473 [02:43<00:22, 93.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15360/17473 [02:43<00:22, 93.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15380/17473 [02:43<00:22, 93.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15400/17473 [02:43<00:22, 94.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15420/17473 [02:43<00:21, 94.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15440/17473 [02:44<00:21, 94.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  88% 15460/17473 [02:44<00:21, 94.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15480/17473 [02:44<00:21, 94.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15500/17473 [02:44<00:20, 94.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15520/17473 [02:44<00:20, 94.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15540/17473 [02:44<00:20, 94.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15560/17473 [02:44<00:20, 94.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15580/17473 [02:44<00:20, 94.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15600/17473 [02:44<00:19, 94.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  89% 15620/17473 [02:44<00:19, 94.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15640/17473 [02:45<00:19, 94.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15660/17473 [02:45<00:19, 94.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15680/17473 [02:45<00:18, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15700/17473 [02:45<00:18, 94.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15720/17473 [02:45<00:18, 95.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15740/17473 [02:45<00:18, 95.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15760/17473 [02:45<00:17, 95.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15780/17473 [02:45<00:17, 95.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  90% 15800/17473 [02:45<00:17, 95.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15820/17473 [02:45<00:17, 95.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15840/17473 [02:45<00:17, 95.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15860/17473 [02:46<00:16, 95.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15880/17473 [02:46<00:16, 95.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15900/17473 [02:46<00:16, 95.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15920/17473 [02:46<00:16, 95.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15940/17473 [02:46<00:16, 95.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15960/17473 [02:46<00:15, 95.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  91% 15980/17473 [02:46<00:15, 95.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16000/17473 [02:46<00:15, 95.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16020/17473 [02:46<00:15, 96.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16040/17473 [02:46<00:14, 96.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16060/17473 [02:47<00:14, 96.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16080/17473 [02:47<00:14, 96.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16100/17473 [02:47<00:14, 96.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16120/17473 [02:47<00:14, 96.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16140/17473 [02:47<00:13, 96.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  92% 16160/17473 [02:47<00:13, 96.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16180/17473 [02:47<00:13, 96.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16200/17473 [02:47<00:13, 96.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16220/17473 [02:47<00:12, 96.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16240/17473 [02:47<00:12, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16260/17473 [02:48<00:12, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16280/17473 [02:48<00:12, 96.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16300/17473 [02:48<00:12, 96.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  93% 16320/17473 [02:48<00:11, 96.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16340/17473 [02:48<00:11, 97.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16360/17473 [02:48<00:11, 97.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16380/17473 [02:48<00:11, 97.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16400/17473 [02:48<00:11, 97.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16420/17473 [02:48<00:10, 97.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16440/17473 [02:48<00:10, 97.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16460/17473 [02:49<00:10, 97.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16480/17473 [02:49<00:10, 97.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  94% 16500/17473 [02:49<00:09, 97.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16520/17473 [02:49<00:09, 97.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16540/17473 [02:49<00:09, 97.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16560/17473 [02:49<00:09, 97.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16580/17473 [02:49<00:09, 97.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16600/17473 [02:49<00:08, 97.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16620/17473 [02:49<00:08, 97.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16640/17473 [02:50<00:08, 97.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16660/17473 [02:50<00:08, 97.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  95% 16680/17473 [02:50<00:08, 97.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16700/17473 [02:50<00:07, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16720/17473 [02:50<00:07, 98.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16740/17473 [02:50<00:07, 98.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16760/17473 [02:50<00:07, 98.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16780/17473 [02:50<00:07, 98.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16800/17473 [02:50<00:06, 98.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16820/17473 [02:50<00:06, 98.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16840/17473 [02:51<00:06, 98.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  96% 16860/17473 [02:51<00:06, 98.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16880/17473 [02:51<00:06, 98.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16900/17473 [02:51<00:05, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16920/17473 [02:51<00:05, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16940/17473 [02:51<00:05, 98.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16960/17473 [02:51<00:05, 98.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 16980/17473 [02:51<00:04, 98.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 17000/17473 [02:51<00:04, 98.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  97% 17020/17473 [02:51<00:04, 98.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17040/17473 [02:52<00:04, 99.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17060/17473 [02:52<00:04, 99.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17080/17473 [02:52<00:03, 99.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17100/17473 [02:52<00:03, 99.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17120/17473 [02:52<00:03, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17140/17473 [02:52<00:03, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17160/17473 [02:52<00:03, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17180/17473 [02:52<00:02, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  98% 17200/17473 [02:52<00:02, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17220/17473 [02:52<00:02, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17240/17473 [02:53<00:02, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17260/17473 [02:53<00:02, 99.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17280/17473 [02:53<00:01, 99.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17300/17473 [02:53<00:01, 99.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17320/17473 [02:53<00:01, 99.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17340/17473 [02:53<00:01, 99.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17360/17473 [02:53<00:01, 99.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33:  99% 17380/17473 [02:53<00:00, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33: 100% 17400/17473 [02:53<00:00, 100.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33: 100% 17420/17473 [02:53<00:00, 100.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33: 100% 17440/17473 [02:54<00:00, 100.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33: 100% 17460/17473 [02:54<00:00, 100.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 33: 100% 17473/17473 [02:54<00:00, 100.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.380\n",
            "Epoch 34:  80% 13960/17473 [02:31<00:38, 92.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  80% 13980/17473 [02:36<00:39, 89.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  80% 14000/17473 [02:36<00:38, 89.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  80% 14020/17473 [02:36<00:38, 89.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  80% 14040/17473 [02:36<00:38, 89.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  80% 14060/17473 [02:36<00:38, 89.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14080/17473 [02:36<00:37, 89.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14100/17473 [02:36<00:37, 89.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14120/17473 [02:36<00:37, 90.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14140/17473 [02:36<00:37, 90.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14160/17473 [02:37<00:36, 90.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14180/17473 [02:37<00:36, 90.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14200/17473 [02:37<00:36, 90.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14220/17473 [02:37<00:36, 90.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  81% 14240/17473 [02:37<00:35, 90.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14260/17473 [02:37<00:35, 90.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14280/17473 [02:37<00:35, 90.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14300/17473 [02:37<00:35, 90.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14320/17473 [02:37<00:34, 90.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14340/17473 [02:38<00:34, 90.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14360/17473 [02:38<00:34, 90.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14380/17473 [02:38<00:34, 90.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  82% 14400/17473 [02:38<00:33, 90.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14420/17473 [02:38<00:33, 91.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14440/17473 [02:38<00:33, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14460/17473 [02:38<00:33, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14480/17473 [02:38<00:32, 91.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14500/17473 [02:38<00:32, 91.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14520/17473 [02:38<00:32, 91.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14540/17473 [02:39<00:32, 91.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14560/17473 [02:39<00:31, 91.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  83% 14580/17473 [02:39<00:31, 91.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14600/17473 [02:39<00:31, 91.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14620/17473 [02:39<00:31, 91.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14640/17473 [02:39<00:30, 91.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14660/17473 [02:39<00:30, 91.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14680/17473 [02:39<00:30, 91.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14700/17473 [02:39<00:30, 91.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14720/17473 [02:40<00:29, 91.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14740/17473 [02:40<00:29, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  84% 14760/17473 [02:40<00:29, 92.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14780/17473 [02:40<00:29, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14800/17473 [02:40<00:28, 92.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14820/17473 [02:40<00:28, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14840/17473 [02:40<00:28, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14860/17473 [02:40<00:28, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14880/17473 [02:40<00:28, 92.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14900/17473 [02:40<00:27, 92.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  85% 14920/17473 [02:41<00:27, 92.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 14940/17473 [02:41<00:27, 92.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 14960/17473 [02:41<00:27, 92.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 14980/17473 [02:41<00:26, 92.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15000/17473 [02:41<00:26, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15020/17473 [02:41<00:26, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15040/17473 [02:41<00:26, 93.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15060/17473 [02:41<00:25, 93.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15080/17473 [02:41<00:25, 93.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  86% 15100/17473 [02:41<00:25, 93.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15120/17473 [02:42<00:25, 93.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15140/17473 [02:42<00:24, 93.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15160/17473 [02:42<00:24, 93.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15180/17473 [02:42<00:24, 93.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15200/17473 [02:42<00:24, 93.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15220/17473 [02:42<00:24, 93.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15240/17473 [02:42<00:23, 93.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15260/17473 [02:42<00:23, 93.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  87% 15280/17473 [02:42<00:23, 93.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15300/17473 [02:43<00:23, 93.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15320/17473 [02:43<00:22, 93.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15340/17473 [02:43<00:22, 93.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15360/17473 [02:43<00:22, 94.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15380/17473 [02:43<00:22, 94.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15400/17473 [02:43<00:22, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15420/17473 [02:43<00:21, 94.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15440/17473 [02:43<00:21, 94.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  88% 15460/17473 [02:43<00:21, 94.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15480/17473 [02:43<00:21, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15500/17473 [02:44<00:20, 94.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15520/17473 [02:44<00:20, 94.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15540/17473 [02:44<00:20, 94.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15560/17473 [02:44<00:20, 94.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15580/17473 [02:44<00:19, 94.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15600/17473 [02:44<00:19, 94.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  89% 15620/17473 [02:44<00:19, 94.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15640/17473 [02:44<00:19, 94.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15660/17473 [02:44<00:19, 95.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15680/17473 [02:44<00:18, 95.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15700/17473 [02:45<00:18, 95.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15720/17473 [02:45<00:18, 95.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15740/17473 [02:45<00:18, 95.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15760/17473 [02:45<00:17, 95.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15780/17473 [02:45<00:17, 95.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  90% 15800/17473 [02:45<00:17, 95.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15820/17473 [02:45<00:17, 95.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15840/17473 [02:45<00:17, 95.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15860/17473 [02:45<00:16, 95.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15880/17473 [02:45<00:16, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15900/17473 [02:46<00:16, 95.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15920/17473 [02:46<00:16, 95.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15940/17473 [02:46<00:15, 95.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15960/17473 [02:46<00:15, 95.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  91% 15980/17473 [02:46<00:15, 96.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16000/17473 [02:46<00:15, 96.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16020/17473 [02:46<00:15, 96.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16040/17473 [02:46<00:14, 96.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16060/17473 [02:46<00:14, 96.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16080/17473 [02:46<00:14, 96.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16100/17473 [02:47<00:14, 96.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16120/17473 [02:47<00:14, 96.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16140/17473 [02:47<00:13, 96.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  92% 16160/17473 [02:47<00:13, 96.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16180/17473 [02:47<00:13, 96.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16200/17473 [02:47<00:13, 96.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16220/17473 [02:47<00:12, 96.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16240/17473 [02:47<00:12, 96.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16260/17473 [02:47<00:12, 96.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16280/17473 [02:48<00:12, 96.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16300/17473 [02:48<00:12, 96.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  93% 16320/17473 [02:48<00:11, 97.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16340/17473 [02:48<00:11, 97.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16360/17473 [02:48<00:11, 97.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16380/17473 [02:48<00:11, 97.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16400/17473 [02:48<00:11, 97.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16420/17473 [02:48<00:10, 97.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16440/17473 [02:48<00:10, 97.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16460/17473 [02:48<00:10, 97.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16480/17473 [02:49<00:10, 97.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  94% 16500/17473 [02:49<00:09, 97.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16520/17473 [02:49<00:09, 97.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16540/17473 [02:49<00:09, 97.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16560/17473 [02:49<00:09, 97.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16580/17473 [02:49<00:09, 97.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16600/17473 [02:49<00:08, 97.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16620/17473 [02:49<00:08, 97.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16640/17473 [02:49<00:08, 97.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16660/17473 [02:49<00:08, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  95% 16680/17473 [02:50<00:08, 98.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16700/17473 [02:50<00:07, 98.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16720/17473 [02:50<00:07, 98.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16740/17473 [02:50<00:07, 98.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16760/17473 [02:50<00:07, 98.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16780/17473 [02:50<00:07, 98.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16800/17473 [02:50<00:06, 98.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16820/17473 [02:50<00:06, 98.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16840/17473 [02:50<00:06, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  96% 16860/17473 [02:50<00:06, 98.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16880/17473 [02:51<00:06, 98.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16900/17473 [02:51<00:05, 98.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16920/17473 [02:51<00:05, 98.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16940/17473 [02:51<00:05, 98.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16960/17473 [02:51<00:05, 98.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 16980/17473 [02:51<00:04, 98.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 17000/17473 [02:51<00:04, 99.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  97% 17020/17473 [02:51<00:04, 99.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17040/17473 [02:51<00:04, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17060/17473 [02:51<00:04, 99.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17080/17473 [02:52<00:03, 99.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17100/17473 [02:52<00:03, 99.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17120/17473 [02:52<00:03, 99.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17140/17473 [02:52<00:03, 99.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17160/17473 [02:52<00:03, 99.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17180/17473 [02:52<00:02, 99.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  98% 17200/17473 [02:52<00:02, 99.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17220/17473 [02:52<00:02, 99.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17240/17473 [02:52<00:02, 99.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17260/17473 [02:52<00:02, 99.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17280/17473 [02:53<00:01, 99.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17300/17473 [02:53<00:01, 99.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17320/17473 [02:53<00:01, 99.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17340/17473 [02:53<00:01, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17360/17473 [02:53<00:01, 100.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34:  99% 17380/17473 [02:53<00:00, 100.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34: 100% 17400/17473 [02:53<00:00, 100.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34: 100% 17420/17473 [02:53<00:00, 100.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34: 100% 17440/17473 [02:53<00:00, 100.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34: 100% 17460/17473 [02:53<00:00, 100.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 34: 100% 17473/17473 [02:54<00:00, 100.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  80% 13960/17473 [02:30<00:37, 92.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  80% 13980/17473 [02:35<00:38, 90.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  80% 14000/17473 [02:35<00:38, 90.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  80% 14020/17473 [02:35<00:38, 90.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  80% 14040/17473 [02:35<00:37, 90.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  80% 14060/17473 [02:35<00:37, 90.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14080/17473 [02:35<00:37, 90.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14100/17473 [02:35<00:37, 90.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14120/17473 [02:35<00:36, 90.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14140/17473 [02:35<00:36, 90.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14160/17473 [02:35<00:36, 90.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14180/17473 [02:36<00:36, 90.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14200/17473 [02:36<00:35, 90.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14220/17473 [02:36<00:35, 91.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  81% 14240/17473 [02:36<00:35, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14260/17473 [02:36<00:35, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14280/17473 [02:36<00:34, 91.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14300/17473 [02:36<00:34, 91.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14320/17473 [02:36<00:34, 91.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14340/17473 [02:36<00:34, 91.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14360/17473 [02:36<00:34, 91.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14380/17473 [02:37<00:33, 91.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  82% 14400/17473 [02:37<00:33, 91.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14420/17473 [02:37<00:33, 91.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14440/17473 [02:37<00:33, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14460/17473 [02:37<00:32, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14480/17473 [02:37<00:32, 91.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14500/17473 [02:37<00:32, 92.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14520/17473 [02:37<00:32, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14540/17473 [02:37<00:31, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14560/17473 [02:37<00:31, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  83% 14580/17473 [02:38<00:31, 92.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14600/17473 [02:38<00:31, 92.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14620/17473 [02:38<00:30, 92.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14640/17473 [02:38<00:30, 92.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14660/17473 [02:38<00:30, 92.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14680/17473 [02:38<00:30, 92.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14700/17473 [02:38<00:29, 92.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14720/17473 [02:38<00:29, 92.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14740/17473 [02:38<00:29, 92.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  84% 14760/17473 [02:38<00:29, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14780/17473 [02:39<00:28, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14800/17473 [02:39<00:28, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14820/17473 [02:39<00:28, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14840/17473 [02:39<00:28, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14860/17473 [02:39<00:28, 93.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14880/17473 [02:39<00:27, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14900/17473 [02:39<00:27, 93.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  85% 14920/17473 [02:39<00:27, 93.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 14940/17473 [02:39<00:27, 93.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 14960/17473 [02:39<00:26, 93.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 14980/17473 [02:40<00:26, 93.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15000/17473 [02:40<00:26, 93.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15020/17473 [02:40<00:26, 93.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15040/17473 [02:40<00:25, 93.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15060/17473 [02:40<00:25, 93.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15080/17473 [02:40<00:25, 93.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  86% 15100/17473 [02:40<00:25, 93.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15120/17473 [02:40<00:25, 94.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15140/17473 [02:40<00:24, 94.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15160/17473 [02:41<00:24, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15180/17473 [02:41<00:24, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15200/17473 [02:41<00:24, 94.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15220/17473 [02:41<00:23, 94.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15240/17473 [02:41<00:23, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15260/17473 [02:41<00:23, 94.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  87% 15280/17473 [02:41<00:23, 94.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15300/17473 [02:41<00:22, 94.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15320/17473 [02:41<00:22, 94.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15340/17473 [02:41<00:22, 94.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15360/17473 [02:42<00:22, 94.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15380/17473 [02:42<00:22, 94.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15400/17473 [02:42<00:21, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15420/17473 [02:42<00:21, 94.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15440/17473 [02:42<00:21, 95.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  88% 15460/17473 [02:42<00:21, 95.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15480/17473 [02:42<00:20, 95.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15500/17473 [02:42<00:20, 95.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15520/17473 [02:42<00:20, 95.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15540/17473 [02:42<00:20, 95.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15560/17473 [02:43<00:20, 95.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15580/17473 [02:43<00:19, 95.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15600/17473 [02:43<00:19, 95.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  89% 15620/17473 [02:43<00:19, 95.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15640/17473 [02:43<00:19, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15660/17473 [02:43<00:18, 95.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15680/17473 [02:43<00:18, 95.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15700/17473 [02:43<00:18, 95.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15720/17473 [02:43<00:18, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15740/17473 [02:43<00:18, 95.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15760/17473 [02:44<00:17, 96.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15780/17473 [02:44<00:17, 96.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  90% 15800/17473 [02:44<00:17, 96.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15820/17473 [02:44<00:17, 96.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15840/17473 [02:44<00:16, 96.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15860/17473 [02:44<00:16, 96.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15880/17473 [02:44<00:16, 96.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15900/17473 [02:44<00:16, 96.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15920/17473 [02:44<00:16, 96.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15940/17473 [02:45<00:15, 96.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15960/17473 [02:45<00:15, 96.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  91% 15980/17473 [02:45<00:15, 96.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16000/17473 [02:45<00:15, 96.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16020/17473 [02:45<00:15, 96.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16040/17473 [02:45<00:14, 96.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16060/17473 [02:45<00:14, 96.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16080/17473 [02:45<00:14, 97.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16100/17473 [02:45<00:14, 97.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16120/17473 [02:45<00:13, 97.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16140/17473 [02:46<00:13, 97.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  92% 16160/17473 [02:46<00:13, 97.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16180/17473 [02:46<00:13, 97.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16200/17473 [02:46<00:13, 97.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16220/17473 [02:46<00:12, 97.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16240/17473 [02:46<00:12, 97.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16260/17473 [02:46<00:12, 97.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16280/17473 [02:46<00:12, 97.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16300/17473 [02:46<00:12, 97.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  93% 16320/17473 [02:46<00:11, 97.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16340/17473 [02:47<00:11, 97.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16360/17473 [02:47<00:11, 97.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16380/17473 [02:47<00:11, 97.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16400/17473 [02:47<00:10, 97.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16420/17473 [02:47<00:10, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16440/17473 [02:47<00:10, 98.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16460/17473 [02:47<00:10, 98.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16480/17473 [02:47<00:10, 98.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  94% 16500/17473 [02:47<00:09, 98.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16520/17473 [02:47<00:09, 98.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16540/17473 [02:48<00:09, 98.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16560/17473 [02:48<00:09, 98.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16580/17473 [02:48<00:09, 98.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16600/17473 [02:48<00:08, 98.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16620/17473 [02:48<00:08, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16640/17473 [02:48<00:08, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16660/17473 [02:48<00:08, 98.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  95% 16680/17473 [02:48<00:08, 98.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16700/17473 [02:48<00:07, 98.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16720/17473 [02:49<00:07, 98.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16740/17473 [02:49<00:07, 98.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16760/17473 [02:49<00:07, 99.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16780/17473 [02:49<00:06, 99.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16800/17473 [02:49<00:06, 99.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16820/17473 [02:49<00:06, 99.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16840/17473 [02:49<00:06, 99.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  96% 16860/17473 [02:49<00:06, 99.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16880/17473 [02:49<00:05, 99.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16900/17473 [02:49<00:05, 99.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16920/17473 [02:49<00:05, 99.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16940/17473 [02:50<00:05, 99.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16960/17473 [02:50<00:05, 99.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 16980/17473 [02:50<00:04, 99.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 17000/17473 [02:50<00:04, 99.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  97% 17020/17473 [02:50<00:04, 99.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17040/17473 [02:50<00:04, 99.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17060/17473 [02:50<00:04, 99.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17080/17473 [02:50<00:03, 100.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17100/17473 [02:50<00:03, 100.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17120/17473 [02:50<00:03, 100.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17140/17473 [02:51<00:03, 100.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17160/17473 [02:51<00:03, 100.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17180/17473 [02:51<00:02, 100.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  98% 17200/17473 [02:51<00:02, 100.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17220/17473 [02:51<00:02, 100.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17240/17473 [02:51<00:02, 100.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17260/17473 [02:51<00:02, 100.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17280/17473 [02:51<00:01, 100.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17300/17473 [02:51<00:01, 100.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17320/17473 [02:51<00:01, 100.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17340/17473 [02:52<00:01, 100.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17360/17473 [02:52<00:01, 100.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35:  99% 17380/17473 [02:52<00:00, 100.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35: 100% 17400/17473 [02:52<00:00, 100.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35: 100% 17420/17473 [02:52<00:00, 101.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35: 100% 17440/17473 [02:52<00:00, 101.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35: 100% 17460/17473 [02:52<00:00, 101.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 35: 100% 17473/17473 [02:52<00:00, 101.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  80% 13960/17473 [02:31<00:38, 91.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  80% 13980/17473 [02:36<00:39, 89.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  80% 14000/17473 [02:36<00:38, 89.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  80% 14020/17473 [02:36<00:38, 89.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  80% 14040/17473 [02:36<00:38, 89.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  80% 14060/17473 [02:36<00:38, 89.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14080/17473 [02:37<00:37, 89.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14100/17473 [02:37<00:37, 89.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14120/17473 [02:37<00:37, 89.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14140/17473 [02:37<00:37, 89.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14160/17473 [02:37<00:36, 89.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14180/17473 [02:37<00:36, 90.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14200/17473 [02:37<00:36, 90.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14220/17473 [02:37<00:36, 90.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  81% 14240/17473 [02:37<00:35, 90.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14260/17473 [02:37<00:35, 90.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14280/17473 [02:38<00:35, 90.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14300/17473 [02:38<00:35, 90.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14320/17473 [02:38<00:34, 90.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14340/17473 [02:38<00:34, 90.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14360/17473 [02:38<00:34, 90.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14380/17473 [02:38<00:34, 90.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  82% 14400/17473 [02:38<00:33, 90.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14420/17473 [02:38<00:33, 90.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14440/17473 [02:38<00:33, 90.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14460/17473 [02:38<00:33, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14480/17473 [02:39<00:32, 91.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14500/17473 [02:39<00:32, 91.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14520/17473 [02:39<00:32, 91.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14540/17473 [02:39<00:32, 91.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14560/17473 [02:39<00:31, 91.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  83% 14580/17473 [02:39<00:31, 91.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14600/17473 [02:39<00:31, 91.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14620/17473 [02:39<00:31, 91.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14640/17473 [02:39<00:30, 91.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14660/17473 [02:39<00:30, 91.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14680/17473 [02:40<00:30, 91.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14700/17473 [02:40<00:30, 91.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14720/17473 [02:40<00:29, 91.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14740/17473 [02:40<00:29, 91.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  84% 14760/17473 [02:40<00:29, 92.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14780/17473 [02:40<00:29, 92.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14800/17473 [02:40<00:29, 92.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14820/17473 [02:40<00:28, 92.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14840/17473 [02:40<00:28, 92.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14860/17473 [02:40<00:28, 92.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14880/17473 [02:41<00:28, 92.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14900/17473 [02:41<00:27, 92.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  85% 14920/17473 [02:41<00:27, 92.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 14940/17473 [02:41<00:27, 92.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 14960/17473 [02:41<00:27, 92.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 14980/17473 [02:41<00:26, 92.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15000/17473 [02:41<00:26, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15020/17473 [02:41<00:26, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15040/17473 [02:41<00:26, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15060/17473 [02:42<00:25, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15080/17473 [02:42<00:25, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  86% 15100/17473 [02:42<00:25, 93.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15120/17473 [02:42<00:25, 93.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15140/17473 [02:42<00:25, 93.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15160/17473 [02:42<00:24, 93.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15180/17473 [02:42<00:24, 93.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15200/17473 [02:42<00:24, 93.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15220/17473 [02:42<00:24, 93.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15240/17473 [02:42<00:23, 93.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15260/17473 [02:43<00:23, 93.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  87% 15280/17473 [02:43<00:23, 93.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15300/17473 [02:43<00:23, 93.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15320/17473 [02:43<00:22, 93.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15340/17473 [02:43<00:22, 93.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15360/17473 [02:43<00:22, 93.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15380/17473 [02:43<00:22, 93.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15400/17473 [02:43<00:22, 94.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15420/17473 [02:43<00:21, 94.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15440/17473 [02:43<00:21, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  88% 15460/17473 [02:44<00:21, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15480/17473 [02:44<00:21, 94.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15500/17473 [02:44<00:20, 94.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15520/17473 [02:44<00:20, 94.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15540/17473 [02:44<00:20, 94.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15560/17473 [02:44<00:20, 94.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15580/17473 [02:44<00:20, 94.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15600/17473 [02:44<00:19, 94.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  89% 15620/17473 [02:44<00:19, 94.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15640/17473 [02:44<00:19, 94.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15660/17473 [02:45<00:19, 94.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15680/17473 [02:45<00:18, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15700/17473 [02:45<00:18, 94.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15720/17473 [02:45<00:18, 95.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15740/17473 [02:45<00:18, 95.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15760/17473 [02:45<00:17, 95.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15780/17473 [02:45<00:17, 95.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  90% 15800/17473 [02:45<00:17, 95.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15820/17473 [02:45<00:17, 95.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15840/17473 [02:45<00:17, 95.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15860/17473 [02:46<00:16, 95.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15880/17473 [02:46<00:16, 95.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15900/17473 [02:46<00:16, 95.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15920/17473 [02:46<00:16, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15940/17473 [02:46<00:16, 95.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15960/17473 [02:46<00:15, 95.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  91% 15980/17473 [02:46<00:15, 95.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16000/17473 [02:46<00:15, 95.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16020/17473 [02:46<00:15, 96.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16040/17473 [02:46<00:14, 96.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16060/17473 [02:47<00:14, 96.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16080/17473 [02:47<00:14, 96.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16100/17473 [02:47<00:14, 96.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16120/17473 [02:47<00:14, 96.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16140/17473 [02:47<00:13, 96.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  92% 16160/17473 [02:47<00:13, 96.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16180/17473 [02:47<00:13, 96.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16200/17473 [02:47<00:13, 96.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16220/17473 [02:47<00:12, 96.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16240/17473 [02:47<00:12, 96.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16260/17473 [02:48<00:12, 96.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16280/17473 [02:48<00:12, 96.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16300/17473 [02:48<00:12, 96.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  93% 16320/17473 [02:48<00:11, 96.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16340/17473 [02:48<00:11, 96.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16360/17473 [02:48<00:11, 97.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16380/17473 [02:48<00:11, 97.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16400/17473 [02:48<00:11, 97.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16420/17473 [02:48<00:10, 97.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16440/17473 [02:49<00:10, 97.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16460/17473 [02:49<00:10, 97.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16480/17473 [02:49<00:10, 97.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  94% 16500/17473 [02:49<00:09, 97.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16520/17473 [02:49<00:09, 97.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16540/17473 [02:49<00:09, 97.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16560/17473 [02:49<00:09, 97.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16580/17473 [02:49<00:09, 97.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16600/17473 [02:49<00:08, 97.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16620/17473 [02:49<00:08, 97.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16640/17473 [02:50<00:08, 97.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16660/17473 [02:50<00:08, 97.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  95% 16680/17473 [02:50<00:08, 97.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16700/17473 [02:50<00:07, 98.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16720/17473 [02:50<00:07, 98.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16740/17473 [02:50<00:07, 98.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16760/17473 [02:50<00:07, 98.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16780/17473 [02:50<00:07, 98.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16800/17473 [02:50<00:06, 98.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16820/17473 [02:50<00:06, 98.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16840/17473 [02:51<00:06, 98.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  96% 16860/17473 [02:51<00:06, 98.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16880/17473 [02:51<00:06, 98.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16900/17473 [02:51<00:05, 98.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16920/17473 [02:51<00:05, 98.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16940/17473 [02:51<00:05, 98.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16960/17473 [02:51<00:05, 98.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 16980/17473 [02:51<00:04, 98.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 17000/17473 [02:51<00:04, 98.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  97% 17020/17473 [02:51<00:04, 98.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17040/17473 [02:52<00:04, 99.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17060/17473 [02:52<00:04, 99.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17080/17473 [02:52<00:03, 99.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17100/17473 [02:52<00:03, 99.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17120/17473 [02:52<00:03, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17140/17473 [02:52<00:03, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17160/17473 [02:52<00:03, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17180/17473 [02:52<00:02, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  98% 17200/17473 [02:52<00:02, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17220/17473 [02:52<00:02, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17240/17473 [02:53<00:02, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17260/17473 [02:53<00:02, 99.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17280/17473 [02:53<00:01, 99.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17300/17473 [02:53<00:01, 99.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17320/17473 [02:53<00:01, 99.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17340/17473 [02:53<00:01, 99.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17360/17473 [02:53<00:01, 99.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36:  99% 17380/17473 [02:53<00:00, 100.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36: 100% 17400/17473 [02:53<00:00, 100.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36: 100% 17420/17473 [02:54<00:00, 100.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36: 100% 17440/17473 [02:54<00:00, 100.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36: 100% 17460/17473 [02:54<00:00, 100.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 36: 100% 17473/17473 [02:54<00:00, 100.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  80% 13960/17473 [02:31<00:38, 92.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  80% 13980/17473 [02:35<00:38, 89.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  80% 14000/17473 [02:36<00:38, 89.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  80% 14020/17473 [02:36<00:38, 89.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  80% 14040/17473 [02:36<00:38, 89.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  80% 14060/17473 [02:36<00:37, 89.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14080/17473 [02:36<00:37, 89.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14100/17473 [02:36<00:37, 90.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14120/17473 [02:36<00:37, 90.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14140/17473 [02:36<00:36, 90.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14160/17473 [02:36<00:36, 90.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14180/17473 [02:37<00:36, 90.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14200/17473 [02:37<00:36, 90.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14220/17473 [02:37<00:35, 90.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  81% 14240/17473 [02:37<00:35, 90.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14260/17473 [02:37<00:35, 90.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14280/17473 [02:37<00:35, 90.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14300/17473 [02:37<00:34, 90.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14320/17473 [02:37<00:34, 90.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14340/17473 [02:37<00:34, 90.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14360/17473 [02:37<00:34, 90.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14380/17473 [02:38<00:34, 90.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  82% 14400/17473 [02:38<00:33, 91.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14420/17473 [02:38<00:33, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14440/17473 [02:38<00:33, 91.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14460/17473 [02:38<00:33, 91.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14480/17473 [02:38<00:32, 91.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14500/17473 [02:38<00:32, 91.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14520/17473 [02:38<00:32, 91.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14540/17473 [02:38<00:32, 91.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14560/17473 [02:39<00:31, 91.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  83% 14580/17473 [02:39<00:31, 91.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14600/17473 [02:39<00:31, 91.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14620/17473 [02:39<00:31, 91.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14640/17473 [02:39<00:30, 91.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14660/17473 [02:39<00:30, 91.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14680/17473 [02:39<00:30, 91.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14700/17473 [02:39<00:30, 92.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14720/17473 [02:39<00:29, 92.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14740/17473 [02:39<00:29, 92.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  84% 14760/17473 [02:40<00:29, 92.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14780/17473 [02:40<00:29, 92.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14800/17473 [02:40<00:28, 92.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14820/17473 [02:40<00:28, 92.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14840/17473 [02:40<00:28, 92.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14860/17473 [02:40<00:28, 92.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14880/17473 [02:40<00:28, 92.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14900/17473 [02:40<00:27, 92.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  85% 14920/17473 [02:40<00:27, 92.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 14940/17473 [02:40<00:27, 92.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 14960/17473 [02:41<00:27, 92.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 14980/17473 [02:41<00:26, 92.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15000/17473 [02:41<00:26, 93.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15020/17473 [02:41<00:26, 93.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15040/17473 [02:41<00:26, 93.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15060/17473 [02:41<00:25, 93.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15080/17473 [02:41<00:25, 93.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  86% 15100/17473 [02:41<00:25, 93.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15120/17473 [02:41<00:25, 93.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15140/17473 [02:42<00:24, 93.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15160/17473 [02:42<00:24, 93.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15180/17473 [02:42<00:24, 93.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15200/17473 [02:42<00:24, 93.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15220/17473 [02:42<00:24, 93.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15240/17473 [02:42<00:23, 93.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15260/17473 [02:42<00:23, 93.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  87% 15280/17473 [02:42<00:23, 93.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15300/17473 [02:42<00:23, 93.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15320/17473 [02:42<00:22, 94.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15340/17473 [02:43<00:22, 94.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15360/17473 [02:43<00:22, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15380/17473 [02:43<00:22, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15400/17473 [02:43<00:21, 94.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15420/17473 [02:43<00:21, 94.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15440/17473 [02:43<00:21, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  88% 15460/17473 [02:43<00:21, 94.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15480/17473 [02:43<00:21, 94.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15500/17473 [02:43<00:20, 94.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15520/17473 [02:43<00:20, 94.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15540/17473 [02:44<00:20, 94.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15560/17473 [02:44<00:20, 94.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15580/17473 [02:44<00:19, 94.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15600/17473 [02:44<00:19, 94.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  89% 15620/17473 [02:44<00:19, 95.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15640/17473 [02:44<00:19, 95.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15660/17473 [02:44<00:19, 95.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15680/17473 [02:44<00:18, 95.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15700/17473 [02:44<00:18, 95.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15720/17473 [02:44<00:18, 95.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15740/17473 [02:44<00:18, 95.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15760/17473 [02:45<00:17, 95.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15780/17473 [02:45<00:17, 95.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  90% 15800/17473 [02:45<00:17, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15820/17473 [02:45<00:17, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15840/17473 [02:45<00:17, 95.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15860/17473 [02:45<00:16, 95.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15880/17473 [02:45<00:16, 95.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15900/17473 [02:45<00:16, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15920/17473 [02:45<00:16, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15940/17473 [02:45<00:15, 96.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15960/17473 [02:46<00:15, 96.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  91% 15980/17473 [02:46<00:15, 96.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16000/17473 [02:46<00:15, 96.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16020/17473 [02:46<00:15, 96.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16040/17473 [02:46<00:14, 96.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16060/17473 [02:46<00:14, 96.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16080/17473 [02:46<00:14, 96.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16100/17473 [02:46<00:14, 96.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16120/17473 [02:46<00:14, 96.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16140/17473 [02:46<00:13, 96.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  92% 16160/17473 [02:47<00:13, 96.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16180/17473 [02:47<00:13, 96.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16200/17473 [02:47<00:13, 96.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16220/17473 [02:47<00:12, 96.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16240/17473 [02:47<00:12, 96.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16260/17473 [02:47<00:12, 97.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16280/17473 [02:47<00:12, 97.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16300/17473 [02:47<00:12, 97.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  93% 16320/17473 [02:47<00:11, 97.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16340/17473 [02:47<00:11, 97.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16360/17473 [02:48<00:11, 97.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16380/17473 [02:48<00:11, 97.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16400/17473 [02:48<00:11, 97.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16420/17473 [02:48<00:10, 97.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16440/17473 [02:48<00:10, 97.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16460/17473 [02:48<00:10, 97.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16480/17473 [02:48<00:10, 97.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  94% 16500/17473 [02:48<00:09, 97.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16520/17473 [02:48<00:09, 97.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16540/17473 [02:48<00:09, 97.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16560/17473 [02:48<00:09, 97.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16580/17473 [02:49<00:09, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16600/17473 [02:49<00:08, 98.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16620/17473 [02:49<00:08, 98.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16640/17473 [02:49<00:08, 98.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16660/17473 [02:49<00:08, 98.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  95% 16680/17473 [02:49<00:08, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16700/17473 [02:49<00:07, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16720/17473 [02:49<00:07, 98.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16740/17473 [02:49<00:07, 98.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16760/17473 [02:49<00:07, 98.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16780/17473 [02:50<00:07, 98.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16800/17473 [02:50<00:06, 98.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16820/17473 [02:50<00:06, 98.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16840/17473 [02:50<00:06, 98.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  96% 16860/17473 [02:50<00:06, 98.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16880/17473 [02:50<00:05, 98.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16900/17473 [02:50<00:05, 99.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16920/17473 [02:50<00:05, 99.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16940/17473 [02:50<00:05, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16960/17473 [02:50<00:05, 99.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 16980/17473 [02:51<00:04, 99.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 17000/17473 [02:51<00:04, 99.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  97% 17020/17473 [02:51<00:04, 99.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17040/17473 [02:51<00:04, 99.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17060/17473 [02:51<00:04, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17080/17473 [02:51<00:03, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17100/17473 [02:51<00:03, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17120/17473 [02:51<00:03, 99.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17140/17473 [02:51<00:03, 99.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17160/17473 [02:51<00:03, 99.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17180/17473 [02:52<00:02, 99.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  98% 17200/17473 [02:52<00:02, 99.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17220/17473 [02:52<00:02, 99.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17240/17473 [02:52<00:02, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17260/17473 [02:52<00:02, 100.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17280/17473 [02:52<00:01, 100.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17300/17473 [02:52<00:01, 100.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17320/17473 [02:52<00:01, 100.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17340/17473 [02:52<00:01, 100.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17360/17473 [02:52<00:01, 100.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37:  99% 17380/17473 [02:53<00:00, 100.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37: 100% 17400/17473 [02:53<00:00, 100.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37: 100% 17420/17473 [02:53<00:00, 100.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37: 100% 17440/17473 [02:53<00:00, 100.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37: 100% 17460/17473 [02:53<00:00, 100.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 37: 100% 17473/17473 [02:53<00:00, 100.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  80% 13960/17473 [02:32<00:38, 91.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  80% 13980/17473 [02:37<00:39, 88.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  80% 14000/17473 [02:37<00:39, 89.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  80% 14020/17473 [02:37<00:38, 89.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  80% 14040/17473 [02:37<00:38, 89.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  80% 14060/17473 [02:37<00:38, 89.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14080/17473 [02:37<00:37, 89.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14100/17473 [02:37<00:37, 89.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14120/17473 [02:37<00:37, 89.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14140/17473 [02:37<00:37, 89.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14160/17473 [02:38<00:36, 89.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14180/17473 [02:38<00:36, 89.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14200/17473 [02:38<00:36, 89.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14220/17473 [02:38<00:36, 89.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  81% 14240/17473 [02:38<00:35, 89.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14260/17473 [02:38<00:35, 89.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14280/17473 [02:38<00:35, 90.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14300/17473 [02:38<00:35, 90.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14320/17473 [02:38<00:34, 90.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14340/17473 [02:38<00:34, 90.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14360/17473 [02:39<00:34, 90.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14380/17473 [02:39<00:34, 90.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  82% 14400/17473 [02:39<00:33, 90.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14420/17473 [02:39<00:33, 90.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14440/17473 [02:39<00:33, 90.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14460/17473 [02:39<00:33, 90.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14480/17473 [02:39<00:33, 90.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14500/17473 [02:39<00:32, 90.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14520/17473 [02:39<00:32, 90.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14540/17473 [02:39<00:32, 90.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14560/17473 [02:40<00:32, 90.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  83% 14580/17473 [02:40<00:31, 91.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14600/17473 [02:40<00:31, 91.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14620/17473 [02:40<00:31, 91.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14640/17473 [02:40<00:31, 91.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14660/17473 [02:40<00:30, 91.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14680/17473 [02:40<00:30, 91.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14700/17473 [02:40<00:30, 91.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14720/17473 [02:40<00:30, 91.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14740/17473 [02:41<00:29, 91.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  84% 14760/17473 [02:41<00:29, 91.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14780/17473 [02:41<00:29, 91.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14800/17473 [02:41<00:29, 91.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14820/17473 [02:41<00:28, 91.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14840/17473 [02:41<00:28, 91.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14860/17473 [02:41<00:28, 91.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14880/17473 [02:41<00:28, 92.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14900/17473 [02:41<00:27, 92.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  85% 14920/17473 [02:41<00:27, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 14940/17473 [02:42<00:27, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 14960/17473 [02:42<00:27, 92.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 14980/17473 [02:42<00:27, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15000/17473 [02:42<00:26, 92.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15020/17473 [02:42<00:26, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15040/17473 [02:42<00:26, 92.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15060/17473 [02:42<00:26, 92.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15080/17473 [02:42<00:25, 92.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  86% 15100/17473 [02:42<00:25, 92.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15120/17473 [02:43<00:25, 92.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15140/17473 [02:43<00:25, 92.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15160/17473 [02:43<00:24, 92.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15180/17473 [02:43<00:24, 92.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15200/17473 [02:43<00:24, 93.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15220/17473 [02:43<00:24, 93.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15240/17473 [02:43<00:23, 93.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15260/17473 [02:43<00:23, 93.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  87% 15280/17473 [02:43<00:23, 93.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15300/17473 [02:43<00:23, 93.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15320/17473 [02:44<00:23, 93.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15340/17473 [02:44<00:22, 93.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15360/17473 [02:44<00:22, 93.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15380/17473 [02:44<00:22, 93.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15400/17473 [02:44<00:22, 93.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15420/17473 [02:44<00:21, 93.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15440/17473 [02:44<00:21, 93.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  88% 15460/17473 [02:44<00:21, 93.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15480/17473 [02:44<00:21, 93.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15500/17473 [02:44<00:20, 93.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15520/17473 [02:45<00:20, 94.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15540/17473 [02:45<00:20, 94.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15560/17473 [02:45<00:20, 94.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15580/17473 [02:45<00:20, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15600/17473 [02:45<00:19, 94.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  89% 15620/17473 [02:45<00:19, 94.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15640/17473 [02:45<00:19, 94.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15660/17473 [02:45<00:19, 94.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15680/17473 [02:45<00:18, 94.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15700/17473 [02:45<00:18, 94.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15720/17473 [02:46<00:18, 94.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15740/17473 [02:46<00:18, 94.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15760/17473 [02:46<00:18, 94.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15780/17473 [02:46<00:17, 94.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  90% 15800/17473 [02:46<00:17, 94.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15820/17473 [02:46<00:17, 94.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15840/17473 [02:46<00:17, 95.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15860/17473 [02:46<00:16, 95.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15880/17473 [02:46<00:16, 95.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15900/17473 [02:47<00:16, 95.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15920/17473 [02:47<00:16, 95.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15940/17473 [02:47<00:16, 95.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15960/17473 [02:47<00:15, 95.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  91% 15980/17473 [02:47<00:15, 95.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16000/17473 [02:47<00:15, 95.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16020/17473 [02:47<00:15, 95.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16040/17473 [02:47<00:14, 95.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16060/17473 [02:47<00:14, 95.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16080/17473 [02:47<00:14, 95.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16100/17473 [02:48<00:14, 95.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16120/17473 [02:48<00:14, 95.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16140/17473 [02:48<00:13, 95.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  92% 16160/17473 [02:48<00:13, 96.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16180/17473 [02:48<00:13, 96.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16200/17473 [02:48<00:13, 96.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16220/17473 [02:48<00:13, 96.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16240/17473 [02:48<00:12, 96.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16260/17473 [02:48<00:12, 96.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16280/17473 [02:48<00:12, 96.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16300/17473 [02:49<00:12, 96.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  93% 16320/17473 [02:49<00:11, 96.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16340/17473 [02:49<00:11, 96.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16360/17473 [02:49<00:11, 96.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16380/17473 [02:49<00:11, 96.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16400/17473 [02:49<00:11, 96.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16420/17473 [02:49<00:10, 96.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16440/17473 [02:49<00:10, 96.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16460/17473 [02:49<00:10, 96.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16480/17473 [02:49<00:10, 97.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  94% 16500/17473 [02:49<00:10, 97.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16520/17473 [02:50<00:09, 97.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16540/17473 [02:50<00:09, 97.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16560/17473 [02:50<00:09, 97.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16580/17473 [02:50<00:09, 97.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16600/17473 [02:50<00:08, 97.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16620/17473 [02:50<00:08, 97.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16640/17473 [02:50<00:08, 97.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16660/17473 [02:50<00:08, 97.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  95% 16680/17473 [02:50<00:08, 97.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16700/17473 [02:50<00:07, 97.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16720/17473 [02:51<00:07, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16740/17473 [02:51<00:07, 97.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16760/17473 [02:51<00:07, 97.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16780/17473 [02:51<00:07, 97.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16800/17473 [02:51<00:06, 97.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16820/17473 [02:51<00:06, 98.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16840/17473 [02:51<00:06, 98.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  96% 16860/17473 [02:51<00:06, 98.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16880/17473 [02:51<00:06, 98.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16900/17473 [02:52<00:05, 98.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16920/17473 [02:52<00:05, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16940/17473 [02:52<00:05, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16960/17473 [02:52<00:05, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 16980/17473 [02:52<00:05, 98.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 17000/17473 [02:52<00:04, 98.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  97% 17020/17473 [02:52<00:04, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17040/17473 [02:52<00:04, 98.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17060/17473 [02:52<00:04, 98.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17080/17473 [02:52<00:03, 98.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17100/17473 [02:53<00:03, 98.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17120/17473 [02:53<00:03, 98.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17140/17473 [02:53<00:03, 98.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17160/17473 [02:53<00:03, 98.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17180/17473 [02:53<00:02, 99.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  98% 17200/17473 [02:53<00:02, 99.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17220/17473 [02:53<00:02, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17240/17473 [02:53<00:02, 99.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17260/17473 [02:53<00:02, 99.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17280/17473 [02:53<00:01, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17300/17473 [02:54<00:01, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17320/17473 [02:54<00:01, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17340/17473 [02:54<00:01, 99.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17360/17473 [02:54<00:01, 99.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38:  99% 17380/17473 [02:54<00:00, 99.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38: 100% 17400/17473 [02:54<00:00, 99.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38: 100% 17420/17473 [02:54<00:00, 99.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38: 100% 17440/17473 [02:54<00:00, 99.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38: 100% 17460/17473 [02:54<00:00, 99.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 38: 100% 17473/17473 [02:54<00:00, 99.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  80% 13960/17473 [02:31<00:38, 91.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  80% 13980/17473 [02:36<00:39, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  80% 14000/17473 [02:36<00:38, 89.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  80% 14020/17473 [02:36<00:38, 89.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  80% 14040/17473 [02:36<00:38, 89.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  80% 14060/17473 [02:36<00:38, 89.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14080/17473 [02:36<00:37, 89.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14100/17473 [02:37<00:37, 89.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14120/17473 [02:37<00:37, 89.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14140/17473 [02:37<00:37, 89.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14160/17473 [02:37<00:36, 89.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14180/17473 [02:37<00:36, 90.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14200/17473 [02:37<00:36, 90.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14220/17473 [02:37<00:36, 90.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  81% 14240/17473 [02:37<00:35, 90.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14260/17473 [02:37<00:35, 90.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14280/17473 [02:37<00:35, 90.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14300/17473 [02:38<00:35, 90.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14320/17473 [02:38<00:34, 90.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14340/17473 [02:38<00:34, 90.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14360/17473 [02:38<00:34, 90.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14380/17473 [02:38<00:34, 90.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  82% 14400/17473 [02:38<00:33, 90.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14420/17473 [02:38<00:33, 90.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14440/17473 [02:38<00:33, 90.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14460/17473 [02:38<00:33, 91.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14480/17473 [02:39<00:32, 91.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14500/17473 [02:39<00:32, 91.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14520/17473 [02:39<00:32, 91.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14540/17473 [02:39<00:32, 91.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14560/17473 [02:39<00:31, 91.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  83% 14580/17473 [02:39<00:31, 91.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14600/17473 [02:39<00:31, 91.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14620/17473 [02:39<00:31, 91.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14640/17473 [02:39<00:30, 91.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14660/17473 [02:39<00:30, 91.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14680/17473 [02:40<00:30, 91.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14700/17473 [02:40<00:30, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14720/17473 [02:40<00:29, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14740/17473 [02:40<00:29, 91.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  84% 14760/17473 [02:40<00:29, 91.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14780/17473 [02:40<00:29, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14800/17473 [02:40<00:29, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14820/17473 [02:40<00:28, 92.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14840/17473 [02:40<00:28, 92.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14860/17473 [02:40<00:28, 92.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14880/17473 [02:41<00:28, 92.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14900/17473 [02:41<00:27, 92.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  85% 14920/17473 [02:41<00:27, 92.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 14940/17473 [02:41<00:27, 92.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 14960/17473 [02:41<00:27, 92.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 14980/17473 [02:41<00:26, 92.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15000/17473 [02:41<00:26, 92.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15020/17473 [02:41<00:26, 92.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15040/17473 [02:41<00:26, 92.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15060/17473 [02:42<00:25, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15080/17473 [02:42<00:25, 93.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  86% 15100/17473 [02:42<00:25, 93.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15120/17473 [02:42<00:25, 93.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15140/17473 [02:42<00:25, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15160/17473 [02:42<00:24, 93.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15180/17473 [02:42<00:24, 93.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15200/17473 [02:42<00:24, 93.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15220/17473 [02:42<00:24, 93.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15240/17473 [02:42<00:23, 93.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15260/17473 [02:42<00:23, 93.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  87% 15280/17473 [02:43<00:23, 93.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15300/17473 [02:43<00:23, 93.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15320/17473 [02:43<00:22, 93.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15340/17473 [02:43<00:22, 93.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15360/17473 [02:43<00:22, 93.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15380/17473 [02:43<00:22, 94.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15400/17473 [02:43<00:22, 94.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15420/17473 [02:43<00:21, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15440/17473 [02:43<00:21, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  88% 15460/17473 [02:43<00:21, 94.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15480/17473 [02:44<00:21, 94.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15500/17473 [02:44<00:20, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15520/17473 [02:44<00:20, 94.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15540/17473 [02:44<00:20, 94.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15560/17473 [02:44<00:20, 94.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15580/17473 [02:44<00:19, 94.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15600/17473 [02:44<00:19, 94.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  89% 15620/17473 [02:44<00:19, 94.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15640/17473 [02:44<00:19, 94.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15660/17473 [02:44<00:19, 94.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15680/17473 [02:45<00:18, 95.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15700/17473 [02:45<00:18, 95.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15720/17473 [02:45<00:18, 95.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15740/17473 [02:45<00:18, 95.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15760/17473 [02:45<00:17, 95.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15780/17473 [02:45<00:17, 95.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  90% 15800/17473 [02:45<00:17, 95.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15820/17473 [02:45<00:17, 95.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15840/17473 [02:45<00:17, 95.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15860/17473 [02:45<00:16, 95.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15880/17473 [02:46<00:16, 95.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15900/17473 [02:46<00:16, 95.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15920/17473 [02:46<00:16, 95.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15940/17473 [02:46<00:15, 95.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15960/17473 [02:46<00:15, 95.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  91% 15980/17473 [02:46<00:15, 95.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16000/17473 [02:46<00:15, 96.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16020/17473 [02:46<00:15, 96.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16040/17473 [02:46<00:14, 96.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16060/17473 [02:46<00:14, 96.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16080/17473 [02:47<00:14, 96.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16100/17473 [02:47<00:14, 96.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16120/17473 [02:47<00:14, 96.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16140/17473 [02:47<00:13, 96.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  92% 16160/17473 [02:47<00:13, 96.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16180/17473 [02:47<00:13, 96.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16200/17473 [02:47<00:13, 96.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16220/17473 [02:47<00:12, 96.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16240/17473 [02:47<00:12, 96.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16260/17473 [02:48<00:12, 96.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16280/17473 [02:48<00:12, 96.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16300/17473 [02:48<00:12, 96.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  93% 16320/17473 [02:48<00:11, 96.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16340/17473 [02:48<00:11, 97.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16360/17473 [02:48<00:11, 97.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16380/17473 [02:48<00:11, 97.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16400/17473 [02:48<00:11, 97.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16420/17473 [02:48<00:10, 97.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16440/17473 [02:48<00:10, 97.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16460/17473 [02:49<00:10, 97.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16480/17473 [02:49<00:10, 97.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  94% 16500/17473 [02:49<00:09, 97.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16520/17473 [02:49<00:09, 97.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16540/17473 [02:49<00:09, 97.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16560/17473 [02:49<00:09, 97.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16580/17473 [02:49<00:09, 97.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16600/17473 [02:49<00:08, 97.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16620/17473 [02:49<00:08, 97.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16640/17473 [02:49<00:08, 97.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16660/17473 [02:50<00:08, 97.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  95% 16680/17473 [02:50<00:08, 98.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16700/17473 [02:50<00:07, 98.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16720/17473 [02:50<00:07, 98.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16740/17473 [02:50<00:07, 98.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16760/17473 [02:50<00:07, 98.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16780/17473 [02:50<00:07, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16800/17473 [02:50<00:06, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16820/17473 [02:50<00:06, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16840/17473 [02:51<00:06, 98.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  96% 16860/17473 [02:51<00:06, 98.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16880/17473 [02:51<00:06, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16900/17473 [02:51<00:05, 98.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16920/17473 [02:51<00:05, 98.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16940/17473 [02:51<00:05, 98.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16960/17473 [02:51<00:05, 98.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 16980/17473 [02:51<00:04, 98.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 17000/17473 [02:51<00:04, 98.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  97% 17020/17473 [02:51<00:04, 98.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17040/17473 [02:52<00:04, 99.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17060/17473 [02:52<00:04, 99.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17080/17473 [02:52<00:03, 99.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17100/17473 [02:52<00:03, 99.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17120/17473 [02:52<00:03, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17140/17473 [02:52<00:03, 99.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17160/17473 [02:52<00:03, 99.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17180/17473 [02:52<00:02, 99.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  98% 17200/17473 [02:52<00:02, 99.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17220/17473 [02:53<00:02, 99.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17240/17473 [02:53<00:02, 99.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17260/17473 [02:53<00:02, 99.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17280/17473 [02:53<00:01, 99.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17300/17473 [02:53<00:01, 99.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17320/17473 [02:53<00:01, 99.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17340/17473 [02:53<00:01, 99.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17360/17473 [02:53<00:01, 99.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39:  99% 17380/17473 [02:53<00:00, 99.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39: 100% 17400/17473 [02:53<00:00, 100.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39: 100% 17420/17473 [02:54<00:00, 100.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39: 100% 17440/17473 [02:54<00:00, 100.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39: 100% 17460/17473 [02:54<00:00, 100.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 39: 100% 17473/17473 [02:54<00:00, 100.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  80% 13960/17473 [02:31<00:38, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  80% 13980/17473 [02:36<00:39, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  80% 14000/17473 [02:36<00:38, 89.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  80% 14020/17473 [02:36<00:38, 89.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  80% 14040/17473 [02:36<00:38, 89.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  80% 14060/17473 [02:36<00:38, 89.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14080/17473 [02:37<00:37, 89.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14100/17473 [02:37<00:37, 89.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14120/17473 [02:37<00:37, 89.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14140/17473 [02:37<00:37, 89.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14160/17473 [02:37<00:36, 89.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14180/17473 [02:37<00:36, 89.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14200/17473 [02:37<00:36, 90.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14220/17473 [02:37<00:36, 90.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  81% 14240/17473 [02:37<00:35, 90.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14260/17473 [02:37<00:35, 90.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14280/17473 [02:38<00:35, 90.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14300/17473 [02:38<00:35, 90.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14320/17473 [02:38<00:34, 90.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14340/17473 [02:38<00:34, 90.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14360/17473 [02:38<00:34, 90.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14380/17473 [02:38<00:34, 90.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  82% 14400/17473 [02:38<00:33, 90.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14420/17473 [02:38<00:33, 90.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14440/17473 [02:38<00:33, 90.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14460/17473 [02:39<00:33, 90.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14480/17473 [02:39<00:32, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14500/17473 [02:39<00:32, 91.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14520/17473 [02:39<00:32, 91.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14540/17473 [02:39<00:32, 91.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14560/17473 [02:39<00:31, 91.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  83% 14580/17473 [02:39<00:31, 91.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14600/17473 [02:39<00:31, 91.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14620/17473 [02:39<00:31, 91.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14640/17473 [02:39<00:30, 91.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14660/17473 [02:40<00:30, 91.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14680/17473 [02:40<00:30, 91.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14700/17473 [02:40<00:30, 91.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14720/17473 [02:40<00:29, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14740/17473 [02:40<00:29, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  84% 14760/17473 [02:40<00:29, 91.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14780/17473 [02:40<00:29, 91.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14800/17473 [02:40<00:29, 92.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14820/17473 [02:40<00:28, 92.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14840/17473 [02:40<00:28, 92.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14860/17473 [02:41<00:28, 92.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14880/17473 [02:41<00:28, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14900/17473 [02:41<00:27, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  85% 14920/17473 [02:41<00:27, 92.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 14940/17473 [02:41<00:27, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 14960/17473 [02:41<00:27, 92.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 14980/17473 [02:41<00:26, 92.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15000/17473 [02:41<00:26, 92.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15020/17473 [02:41<00:26, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15040/17473 [02:42<00:26, 92.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15060/17473 [02:42<00:25, 92.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15080/17473 [02:42<00:25, 92.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  86% 15100/17473 [02:42<00:25, 93.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15120/17473 [02:42<00:25, 93.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15140/17473 [02:42<00:25, 93.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15160/17473 [02:42<00:24, 93.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15180/17473 [02:42<00:24, 93.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15200/17473 [02:42<00:24, 93.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15220/17473 [02:42<00:24, 93.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15240/17473 [02:43<00:23, 93.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15260/17473 [02:43<00:23, 93.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  87% 15280/17473 [02:43<00:23, 93.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15300/17473 [02:43<00:23, 93.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15320/17473 [02:43<00:22, 93.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15340/17473 [02:43<00:22, 93.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15360/17473 [02:43<00:22, 93.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15380/17473 [02:43<00:22, 93.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15400/17473 [02:43<00:22, 93.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15420/17473 [02:43<00:21, 94.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15440/17473 [02:44<00:21, 94.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  88% 15460/17473 [02:44<00:21, 94.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15480/17473 [02:44<00:21, 94.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15500/17473 [02:44<00:20, 94.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15520/17473 [02:44<00:20, 94.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15540/17473 [02:44<00:20, 94.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15560/17473 [02:44<00:20, 94.50it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15580/17473 [02:44<00:20, 94.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15600/17473 [02:44<00:19, 94.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  89% 15620/17473 [02:44<00:19, 94.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15640/17473 [02:45<00:19, 94.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15660/17473 [02:45<00:19, 94.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15680/17473 [02:45<00:18, 94.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15700/17473 [02:45<00:18, 94.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15720/17473 [02:45<00:18, 95.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15740/17473 [02:45<00:18, 95.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15760/17473 [02:45<00:18, 95.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15780/17473 [02:45<00:17, 95.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  90% 15800/17473 [02:45<00:17, 95.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15820/17473 [02:46<00:17, 95.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15840/17473 [02:46<00:17, 95.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15860/17473 [02:46<00:16, 95.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15880/17473 [02:46<00:16, 95.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15900/17473 [02:46<00:16, 95.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15920/17473 [02:46<00:16, 95.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15940/17473 [02:46<00:16, 95.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15960/17473 [02:46<00:15, 95.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  91% 15980/17473 [02:46<00:15, 95.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16000/17473 [02:46<00:15, 95.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16020/17473 [02:47<00:15, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16040/17473 [02:47<00:14, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16060/17473 [02:47<00:14, 96.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16080/17473 [02:47<00:14, 96.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16100/17473 [02:47<00:14, 96.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16120/17473 [02:47<00:14, 96.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16140/17473 [02:47<00:13, 96.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  92% 16160/17473 [02:47<00:13, 96.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16180/17473 [02:47<00:13, 96.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16200/17473 [02:47<00:13, 96.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16220/17473 [02:48<00:12, 96.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16240/17473 [02:48<00:12, 96.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16260/17473 [02:48<00:12, 96.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16280/17473 [02:48<00:12, 96.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16300/17473 [02:48<00:12, 96.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  93% 16320/17473 [02:48<00:11, 96.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16340/17473 [02:48<00:11, 96.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16360/17473 [02:48<00:11, 96.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16380/17473 [02:48<00:11, 97.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16400/17473 [02:48<00:11, 97.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16420/17473 [02:49<00:10, 97.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16440/17473 [02:49<00:10, 97.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16460/17473 [02:49<00:10, 97.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16480/17473 [02:49<00:10, 97.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  94% 16500/17473 [02:49<00:09, 97.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16520/17473 [02:49<00:09, 97.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16540/17473 [02:49<00:09, 97.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16560/17473 [02:49<00:09, 97.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16580/17473 [02:49<00:09, 97.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16600/17473 [02:49<00:08, 97.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16620/17473 [02:50<00:08, 97.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16640/17473 [02:50<00:08, 97.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16660/17473 [02:50<00:08, 97.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  95% 16680/17473 [02:50<00:08, 97.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16700/17473 [02:50<00:07, 98.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16720/17473 [02:50<00:07, 98.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16740/17473 [02:50<00:07, 98.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16760/17473 [02:50<00:07, 98.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16780/17473 [02:50<00:07, 98.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16800/17473 [02:50<00:06, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16820/17473 [02:51<00:06, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16840/17473 [02:51<00:06, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  96% 16860/17473 [02:51<00:06, 98.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16880/17473 [02:51<00:06, 98.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16900/17473 [02:51<00:05, 98.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16920/17473 [02:51<00:05, 98.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16940/17473 [02:51<00:05, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16960/17473 [02:51<00:05, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 16980/17473 [02:51<00:04, 98.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 17000/17473 [02:51<00:04, 98.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  97% 17020/17473 [02:51<00:04, 98.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17040/17473 [02:52<00:04, 99.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17060/17473 [02:52<00:04, 99.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17080/17473 [02:52<00:03, 99.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17100/17473 [02:52<00:03, 99.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17120/17473 [02:52<00:03, 99.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17140/17473 [02:52<00:03, 99.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17160/17473 [02:52<00:03, 99.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17180/17473 [02:52<00:02, 99.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  98% 17200/17473 [02:52<00:02, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17220/17473 [02:52<00:02, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17240/17473 [02:53<00:02, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17260/17473 [02:53<00:02, 99.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17280/17473 [02:53<00:01, 99.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17300/17473 [02:53<00:01, 99.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17320/17473 [02:53<00:01, 99.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17340/17473 [02:53<00:01, 99.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17360/17473 [02:53<00:01, 99.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40:  99% 17380/17473 [02:53<00:00, 99.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40: 100% 17400/17473 [02:53<00:00, 100.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40: 100% 17420/17473 [02:54<00:00, 100.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40: 100% 17440/17473 [02:54<00:00, 100.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40: 100% 17460/17473 [02:54<00:00, 100.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 40: 100% 17473/17473 [02:54<00:00, 100.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  80% 13960/17473 [02:32<00:38, 91.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  80% 13980/17473 [02:36<00:39, 89.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  80% 14000/17473 [02:37<00:38, 89.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  80% 14020/17473 [02:37<00:38, 89.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  80% 14040/17473 [02:37<00:38, 89.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  80% 14060/17473 [02:37<00:38, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14080/17473 [02:37<00:37, 89.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14100/17473 [02:37<00:37, 89.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14120/17473 [02:37<00:37, 89.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14140/17473 [02:37<00:37, 89.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14160/17473 [02:37<00:36, 89.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14180/17473 [02:37<00:36, 89.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14200/17473 [02:38<00:36, 89.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14220/17473 [02:38<00:36, 89.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  81% 14240/17473 [02:38<00:35, 89.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14260/17473 [02:38<00:35, 90.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14280/17473 [02:38<00:35, 90.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14300/17473 [02:38<00:35, 90.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14320/17473 [02:38<00:34, 90.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14340/17473 [02:38<00:34, 90.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14360/17473 [02:38<00:34, 90.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14380/17473 [02:38<00:34, 90.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  82% 14400/17473 [02:39<00:33, 90.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14420/17473 [02:39<00:33, 90.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14440/17473 [02:39<00:33, 90.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14460/17473 [02:39<00:33, 90.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14480/17473 [02:39<00:32, 90.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14500/17473 [02:39<00:32, 90.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14520/17473 [02:39<00:32, 90.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14540/17473 [02:39<00:32, 91.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14560/17473 [02:39<00:31, 91.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  83% 14580/17473 [02:39<00:31, 91.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14600/17473 [02:40<00:31, 91.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14620/17473 [02:40<00:31, 91.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14640/17473 [02:40<00:31, 91.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14660/17473 [02:40<00:30, 91.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14680/17473 [02:40<00:30, 91.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14700/17473 [02:40<00:30, 91.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14720/17473 [02:40<00:30, 91.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14740/17473 [02:40<00:29, 91.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  84% 14760/17473 [02:40<00:29, 91.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14780/17473 [02:41<00:29, 91.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14800/17473 [02:41<00:29, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14820/17473 [02:41<00:28, 91.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14840/17473 [02:41<00:28, 92.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14860/17473 [02:41<00:28, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14880/17473 [02:41<00:28, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14900/17473 [02:41<00:27, 92.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  85% 14920/17473 [02:41<00:27, 92.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 14940/17473 [02:41<00:27, 92.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 14960/17473 [02:41<00:27, 92.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 14980/17473 [02:42<00:26, 92.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15000/17473 [02:42<00:26, 92.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15020/17473 [02:42<00:26, 92.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15040/17473 [02:42<00:26, 92.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15060/17473 [02:42<00:26, 92.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15080/17473 [02:42<00:25, 92.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  86% 15100/17473 [02:42<00:25, 92.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15120/17473 [02:42<00:25, 92.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15140/17473 [02:42<00:25, 93.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15160/17473 [02:42<00:24, 93.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15180/17473 [02:43<00:24, 93.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15200/17473 [02:43<00:24, 93.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15220/17473 [02:43<00:24, 93.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15240/17473 [02:43<00:23, 93.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15260/17473 [02:43<00:23, 93.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  87% 15280/17473 [02:43<00:23, 93.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15300/17473 [02:43<00:23, 93.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15320/17473 [02:43<00:23, 93.58it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15340/17473 [02:43<00:22, 93.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15360/17473 [02:43<00:22, 93.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15380/17473 [02:44<00:22, 93.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15400/17473 [02:44<00:22, 93.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15420/17473 [02:44<00:21, 93.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15440/17473 [02:44<00:21, 93.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  88% 15460/17473 [02:44<00:21, 94.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15480/17473 [02:44<00:21, 94.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15500/17473 [02:44<00:20, 94.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15520/17473 [02:44<00:20, 94.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15540/17473 [02:44<00:20, 94.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15560/17473 [02:44<00:20, 94.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15580/17473 [02:45<00:20, 94.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15600/17473 [02:45<00:19, 94.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  89% 15620/17473 [02:45<00:19, 94.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15640/17473 [02:45<00:19, 94.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15660/17473 [02:45<00:19, 94.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15680/17473 [02:45<00:18, 94.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15700/17473 [02:45<00:18, 94.75it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15720/17473 [02:45<00:18, 94.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15740/17473 [02:45<00:18, 94.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15760/17473 [02:46<00:18, 94.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15780/17473 [02:46<00:17, 95.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  90% 15800/17473 [02:46<00:17, 95.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15820/17473 [02:46<00:17, 95.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15840/17473 [02:46<00:17, 95.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15860/17473 [02:46<00:16, 95.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15880/17473 [02:46<00:16, 95.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15900/17473 [02:46<00:16, 95.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15920/17473 [02:46<00:16, 95.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15940/17473 [02:46<00:16, 95.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15960/17473 [02:47<00:15, 95.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  91% 15980/17473 [02:47<00:15, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16000/17473 [02:47<00:15, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16020/17473 [02:47<00:15, 95.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16040/17473 [02:47<00:14, 95.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16060/17473 [02:47<00:14, 95.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16080/17473 [02:47<00:14, 95.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16100/17473 [02:47<00:14, 95.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16120/17473 [02:47<00:14, 96.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16140/17473 [02:47<00:13, 96.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  92% 16160/17473 [02:48<00:13, 96.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16180/17473 [02:48<00:13, 96.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16200/17473 [02:48<00:13, 96.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16220/17473 [02:48<00:13, 96.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16240/17473 [02:48<00:12, 96.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16260/17473 [02:48<00:12, 96.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16280/17473 [02:48<00:12, 96.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16300/17473 [02:48<00:12, 96.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  93% 16320/17473 [02:48<00:11, 96.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16340/17473 [02:49<00:11, 96.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16360/17473 [02:49<00:11, 96.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16380/17473 [02:49<00:11, 96.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16400/17473 [02:49<00:11, 96.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16420/17473 [02:49<00:10, 96.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16440/17473 [02:49<00:10, 96.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16460/17473 [02:49<00:10, 97.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16480/17473 [02:49<00:10, 97.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  94% 16500/17473 [02:49<00:10, 97.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16520/17473 [02:49<00:09, 97.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16540/17473 [02:50<00:09, 97.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16560/17473 [02:50<00:09, 97.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16580/17473 [02:50<00:09, 97.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16600/17473 [02:50<00:08, 97.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16620/17473 [02:50<00:08, 97.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16640/17473 [02:50<00:08, 97.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16660/17473 [02:50<00:08, 97.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  95% 16680/17473 [02:50<00:08, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16700/17473 [02:50<00:07, 97.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16720/17473 [02:50<00:07, 97.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16740/17473 [02:51<00:07, 97.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16760/17473 [02:51<00:07, 97.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16780/17473 [02:51<00:07, 98.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16800/17473 [02:51<00:06, 98.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16820/17473 [02:51<00:06, 98.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16840/17473 [02:51<00:06, 98.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  96% 16860/17473 [02:51<00:06, 98.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16880/17473 [02:51<00:06, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16900/17473 [02:51<00:05, 98.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16920/17473 [02:51<00:05, 98.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16940/17473 [02:52<00:05, 98.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16960/17473 [02:52<00:05, 98.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 16980/17473 [02:52<00:05, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 17000/17473 [02:52<00:04, 98.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  97% 17020/17473 [02:52<00:04, 98.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17040/17473 [02:52<00:04, 98.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17060/17473 [02:52<00:04, 98.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17080/17473 [02:52<00:03, 98.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17100/17473 [02:52<00:03, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17120/17473 [02:52<00:03, 99.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17140/17473 [02:53<00:03, 99.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17160/17473 [02:53<00:03, 99.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17180/17473 [02:53<00:02, 99.18it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  98% 17200/17473 [02:53<00:02, 99.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17220/17473 [02:53<00:02, 99.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17240/17473 [02:53<00:02, 99.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17260/17473 [02:53<00:02, 99.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17280/17473 [02:53<00:01, 99.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17300/17473 [02:53<00:01, 99.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17320/17473 [02:53<00:01, 99.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17340/17473 [02:54<00:01, 99.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17360/17473 [02:54<00:01, 99.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41:  99% 17380/17473 [02:54<00:00, 99.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41: 100% 17400/17473 [02:54<00:00, 99.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41: 100% 17420/17473 [02:54<00:00, 99.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41: 100% 17440/17473 [02:54<00:00, 99.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41: 100% 17460/17473 [02:54<00:00, 99.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 41: 100% 17473/17473 [02:54<00:00, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  80% 13960/17473 [02:32<00:38, 91.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  80% 13980/17473 [02:36<00:39, 89.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  80% 14000/17473 [02:37<00:38, 89.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  80% 14020/17473 [02:37<00:38, 89.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  80% 14040/17473 [02:37<00:38, 89.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  80% 14060/17473 [02:37<00:38, 89.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14080/17473 [02:37<00:37, 89.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14100/17473 [02:37<00:37, 89.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14120/17473 [02:37<00:37, 89.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14140/17473 [02:37<00:37, 89.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14160/17473 [02:37<00:36, 89.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14180/17473 [02:38<00:36, 89.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14200/17473 [02:38<00:36, 89.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14220/17473 [02:38<00:36, 89.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  81% 14240/17473 [02:38<00:35, 89.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14260/17473 [02:38<00:35, 90.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14280/17473 [02:38<00:35, 90.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14300/17473 [02:38<00:35, 90.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14320/17473 [02:38<00:34, 90.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14340/17473 [02:38<00:34, 90.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14360/17473 [02:38<00:34, 90.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14380/17473 [02:39<00:34, 90.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  82% 14400/17473 [02:39<00:33, 90.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14420/17473 [02:39<00:33, 90.56it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14440/17473 [02:39<00:33, 90.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14460/17473 [02:39<00:33, 90.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14480/17473 [02:39<00:32, 90.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14500/17473 [02:39<00:32, 90.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14520/17473 [02:39<00:32, 90.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14540/17473 [02:39<00:32, 90.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14560/17473 [02:39<00:31, 91.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  83% 14580/17473 [02:40<00:31, 91.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14600/17473 [02:40<00:31, 91.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14620/17473 [02:40<00:31, 91.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14640/17473 [02:40<00:31, 91.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14660/17473 [02:40<00:30, 91.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14680/17473 [02:40<00:30, 91.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14700/17473 [02:40<00:30, 91.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14720/17473 [02:40<00:30, 91.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14740/17473 [02:40<00:29, 91.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  84% 14760/17473 [02:40<00:29, 91.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14780/17473 [02:41<00:29, 91.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14800/17473 [02:41<00:29, 91.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14820/17473 [02:41<00:28, 91.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14840/17473 [02:41<00:28, 91.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14860/17473 [02:41<00:28, 92.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14880/17473 [02:41<00:28, 92.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14900/17473 [02:41<00:27, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  85% 14920/17473 [02:41<00:27, 92.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 14940/17473 [02:41<00:27, 92.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 14960/17473 [02:41<00:27, 92.39it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 14980/17473 [02:42<00:26, 92.45it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15000/17473 [02:42<00:26, 92.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15020/17473 [02:42<00:26, 92.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15040/17473 [02:42<00:26, 92.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15060/17473 [02:42<00:26, 92.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15080/17473 [02:42<00:25, 92.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  86% 15100/17473 [02:42<00:25, 92.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15120/17473 [02:42<00:25, 92.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15140/17473 [02:42<00:25, 92.99it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15160/17473 [02:42<00:24, 93.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15180/17473 [02:43<00:24, 93.12it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15200/17473 [02:43<00:24, 93.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15220/17473 [02:43<00:24, 93.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15240/17473 [02:43<00:23, 93.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15260/17473 [02:43<00:23, 93.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  87% 15280/17473 [02:43<00:23, 93.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15300/17473 [02:43<00:23, 93.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15320/17473 [02:43<00:23, 93.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15340/17473 [02:43<00:22, 93.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15360/17473 [02:43<00:22, 93.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15380/17473 [02:43<00:22, 93.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15400/17473 [02:44<00:22, 93.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15420/17473 [02:44<00:21, 93.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15440/17473 [02:44<00:21, 93.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  88% 15460/17473 [02:44<00:21, 94.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15480/17473 [02:44<00:21, 94.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15500/17473 [02:44<00:20, 94.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15520/17473 [02:44<00:20, 94.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15540/17473 [02:44<00:20, 94.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15560/17473 [02:44<00:20, 94.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15580/17473 [02:44<00:20, 94.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15600/17473 [02:45<00:19, 94.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  89% 15620/17473 [02:45<00:19, 94.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15640/17473 [02:45<00:19, 94.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15660/17473 [02:45<00:19, 94.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15680/17473 [02:45<00:18, 94.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15700/17473 [02:45<00:18, 94.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15720/17473 [02:45<00:18, 94.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15740/17473 [02:45<00:18, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15760/17473 [02:45<00:18, 94.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15780/17473 [02:46<00:17, 95.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  90% 15800/17473 [02:46<00:17, 95.10it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15820/17473 [02:46<00:17, 95.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15840/17473 [02:46<00:17, 95.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15860/17473 [02:46<00:16, 95.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15880/17473 [02:46<00:16, 95.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15900/17473 [02:46<00:16, 95.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15920/17473 [02:46<00:16, 95.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15940/17473 [02:46<00:16, 95.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15960/17473 [02:46<00:15, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  91% 15980/17473 [02:47<00:15, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16000/17473 [02:47<00:15, 95.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16020/17473 [02:47<00:15, 95.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16040/17473 [02:47<00:14, 95.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16060/17473 [02:47<00:14, 95.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16080/17473 [02:47<00:14, 95.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16100/17473 [02:47<00:14, 96.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16120/17473 [02:47<00:14, 96.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16140/17473 [02:47<00:13, 96.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  92% 16160/17473 [02:47<00:13, 96.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16180/17473 [02:48<00:13, 96.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16200/17473 [02:48<00:13, 96.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16220/17473 [02:48<00:12, 96.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16240/17473 [02:48<00:12, 96.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16260/17473 [02:48<00:12, 96.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16280/17473 [02:48<00:12, 96.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16300/17473 [02:48<00:12, 96.62it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  93% 16320/17473 [02:48<00:11, 96.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16340/17473 [02:48<00:11, 96.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16360/17473 [02:49<00:11, 96.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16380/17473 [02:49<00:11, 96.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16400/17473 [02:49<00:11, 96.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16420/17473 [02:49<00:10, 96.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16440/17473 [02:49<00:10, 97.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16460/17473 [02:49<00:10, 97.08it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16480/17473 [02:49<00:10, 97.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  94% 16500/17473 [02:49<00:10, 97.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16520/17473 [02:49<00:09, 97.26it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16540/17473 [02:49<00:09, 97.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16560/17473 [02:50<00:09, 97.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16580/17473 [02:50<00:09, 97.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16600/17473 [02:50<00:08, 97.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16620/17473 [02:50<00:08, 97.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16640/17473 [02:50<00:08, 97.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16660/17473 [02:50<00:08, 97.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  95% 16680/17473 [02:50<00:08, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16700/17473 [02:50<00:07, 97.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16720/17473 [02:50<00:07, 97.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16740/17473 [02:51<00:07, 97.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16760/17473 [02:51<00:07, 97.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16780/17473 [02:51<00:07, 98.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16800/17473 [02:51<00:06, 98.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16820/17473 [02:51<00:06, 98.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16840/17473 [02:51<00:06, 98.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  96% 16860/17473 [02:51<00:06, 98.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16880/17473 [02:51<00:06, 98.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16900/17473 [02:51<00:05, 98.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16920/17473 [02:51<00:05, 98.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16940/17473 [02:52<00:05, 98.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16960/17473 [02:52<00:05, 98.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 16980/17473 [02:52<00:05, 98.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 17000/17473 [02:52<00:04, 98.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  97% 17020/17473 [02:52<00:04, 98.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17040/17473 [02:52<00:04, 98.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17060/17473 [02:52<00:04, 98.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17080/17473 [02:52<00:03, 98.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17100/17473 [02:52<00:03, 98.94it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17120/17473 [02:52<00:03, 99.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17140/17473 [02:53<00:03, 99.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17160/17473 [02:53<00:03, 99.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17180/17473 [02:53<00:02, 99.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  98% 17200/17473 [02:53<00:02, 99.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17220/17473 [02:53<00:02, 99.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17240/17473 [02:53<00:02, 99.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17260/17473 [02:53<00:02, 99.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17280/17473 [02:53<00:01, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17300/17473 [02:53<00:01, 99.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17320/17473 [02:54<00:01, 99.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17340/17473 [02:54<00:01, 99.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17360/17473 [02:54<00:01, 99.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42:  99% 17380/17473 [02:54<00:00, 99.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42: 100% 17400/17473 [02:54<00:00, 99.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42: 100% 17420/17473 [02:54<00:00, 99.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42: 100% 17440/17473 [02:54<00:00, 99.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42: 100% 17460/17473 [02:54<00:00, 99.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 42: 100% 17473/17473 [02:54<00:00, 99.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  80% 13960/17473 [02:31<00:38, 92.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/3495 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  80% 13980/17473 [02:36<00:38, 89.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  80% 14000/17473 [02:36<00:38, 89.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  80% 14020/17473 [02:36<00:38, 89.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  80% 14040/17473 [02:36<00:38, 89.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  80% 14060/17473 [02:36<00:37, 89.85it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14080/17473 [02:36<00:37, 89.91it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14100/17473 [02:36<00:37, 89.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14120/17473 [02:36<00:37, 90.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14140/17473 [02:36<00:36, 90.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14160/17473 [02:37<00:36, 90.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14180/17473 [02:37<00:36, 90.24it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14200/17473 [02:37<00:36, 90.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14220/17473 [02:37<00:35, 90.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  81% 14240/17473 [02:37<00:35, 90.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14260/17473 [02:37<00:35, 90.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14280/17473 [02:37<00:35, 90.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14300/17473 [02:37<00:35, 90.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14320/17473 [02:37<00:34, 90.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14340/17473 [02:37<00:34, 90.76it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14360/17473 [02:38<00:34, 90.82it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14380/17473 [02:38<00:34, 90.88it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  82% 14400/17473 [02:38<00:33, 90.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14420/17473 [02:38<00:33, 91.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14440/17473 [02:38<00:33, 91.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14460/17473 [02:38<00:33, 91.14it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14480/17473 [02:38<00:32, 91.20it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14500/17473 [02:38<00:32, 91.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14520/17473 [02:38<00:32, 91.33it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14540/17473 [02:39<00:32, 91.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14560/17473 [02:39<00:31, 91.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  83% 14580/17473 [02:39<00:31, 91.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14600/17473 [02:39<00:31, 91.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14620/17473 [02:39<00:31, 91.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14640/17473 [02:39<00:30, 91.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14660/17473 [02:39<00:30, 91.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14680/17473 [02:39<00:30, 91.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14700/17473 [02:39<00:30, 91.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14720/17473 [02:40<00:29, 92.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14740/17473 [02:40<00:29, 92.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  84% 14760/17473 [02:40<00:29, 92.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14780/17473 [02:40<00:29, 92.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14800/17473 [02:40<00:28, 92.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14820/17473 [02:40<00:28, 92.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14840/17473 [02:40<00:28, 92.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14860/17473 [02:40<00:28, 92.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14880/17473 [02:40<00:28, 92.51it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14900/17473 [02:40<00:27, 92.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  85% 14920/17473 [02:41<00:27, 92.64it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 14940/17473 [02:41<00:27, 92.70it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 14960/17473 [02:41<00:27, 92.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 14980/17473 [02:41<00:26, 92.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15000/17473 [02:41<00:26, 92.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15020/17473 [02:41<00:26, 92.96it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15040/17473 [02:41<00:26, 93.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15060/17473 [02:41<00:25, 93.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15080/17473 [02:41<00:25, 93.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  86% 15100/17473 [02:41<00:25, 93.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15120/17473 [02:42<00:25, 93.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15140/17473 [02:42<00:24, 93.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15160/17473 [02:42<00:24, 93.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15180/17473 [02:42<00:24, 93.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15200/17473 [02:42<00:24, 93.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15220/17473 [02:42<00:24, 93.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15240/17473 [02:42<00:23, 93.68it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15260/17473 [02:42<00:23, 93.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  87% 15280/17473 [02:42<00:23, 93.81it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15300/17473 [02:42<00:23, 93.87it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15320/17473 [02:43<00:22, 93.93it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15340/17473 [02:43<00:22, 93.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15360/17473 [02:43<00:22, 94.04it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15380/17473 [02:43<00:22, 94.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15400/17473 [02:43<00:22, 94.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15420/17473 [02:43<00:21, 94.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15440/17473 [02:43<00:21, 94.28it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  88% 15460/17473 [02:43<00:21, 94.34it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15480/17473 [02:43<00:21, 94.40it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15500/17473 [02:44<00:20, 94.46it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15520/17473 [02:44<00:20, 94.52it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15540/17473 [02:44<00:20, 94.57it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15560/17473 [02:44<00:20, 94.63it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15580/17473 [02:44<00:19, 94.69it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15600/17473 [02:44<00:19, 94.74it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  89% 15620/17473 [02:44<00:19, 94.80it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15640/17473 [02:44<00:19, 94.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15660/17473 [02:44<00:19, 94.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15680/17473 [02:45<00:18, 94.97it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15700/17473 [02:45<00:18, 95.03it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15720/17473 [02:45<00:18, 95.09it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15740/17473 [02:45<00:18, 95.15it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15760/17473 [02:45<00:17, 95.21it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15780/17473 [02:45<00:17, 95.27it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  90% 15800/17473 [02:45<00:17, 95.32it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15820/17473 [02:45<00:17, 95.38it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15840/17473 [02:45<00:17, 95.44it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15860/17473 [02:46<00:16, 95.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15880/17473 [02:46<00:16, 95.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15900/17473 [02:46<00:16, 95.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15920/17473 [02:46<00:16, 95.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15940/17473 [02:46<00:16, 95.73it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15960/17473 [02:46<00:15, 95.79it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  91% 15980/17473 [02:46<00:15, 95.86it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16000/17473 [02:46<00:15, 95.92it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16020/17473 [02:46<00:15, 95.98it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16040/17473 [02:47<00:14, 96.05it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16060/17473 [02:47<00:14, 96.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16080/17473 [02:47<00:14, 96.17it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16100/17473 [02:47<00:14, 96.23it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16120/17473 [02:47<00:14, 96.29it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16140/17473 [02:47<00:13, 96.35it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  92% 16160/17473 [02:47<00:13, 96.41it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16180/17473 [02:47<00:13, 96.47it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16200/17473 [02:47<00:13, 96.53it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16220/17473 [02:47<00:12, 96.59it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16240/17473 [02:48<00:12, 96.65it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16260/17473 [02:48<00:12, 96.71it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16280/17473 [02:48<00:12, 96.77it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16300/17473 [02:48<00:12, 96.83it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  93% 16320/17473 [02:48<00:11, 96.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16340/17473 [02:48<00:11, 96.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16360/17473 [02:48<00:11, 97.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16380/17473 [02:48<00:11, 97.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16400/17473 [02:48<00:11, 97.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16420/17473 [02:48<00:10, 97.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16440/17473 [02:49<00:10, 97.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16460/17473 [02:49<00:10, 97.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16480/17473 [02:49<00:10, 97.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  94% 16500/17473 [02:49<00:09, 97.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16520/17473 [02:49<00:09, 97.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16540/17473 [02:49<00:09, 97.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16560/17473 [02:49<00:09, 97.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16580/17473 [02:49<00:09, 97.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16600/17473 [02:49<00:08, 97.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16620/17473 [02:49<00:08, 97.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16640/17473 [02:50<00:08, 97.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16660/17473 [02:50<00:08, 97.90it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  95% 16680/17473 [02:50<00:08, 97.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16700/17473 [02:50<00:07, 98.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16720/17473 [02:50<00:07, 98.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16740/17473 [02:50<00:07, 98.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16760/17473 [02:50<00:07, 98.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16780/17473 [02:50<00:07, 98.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16800/17473 [02:50<00:06, 98.30it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16820/17473 [02:51<00:06, 98.36it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16840/17473 [02:51<00:06, 98.42it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  96% 16860/17473 [02:51<00:06, 98.48it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16880/17473 [02:51<00:06, 98.54it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16900/17473 [02:51<00:05, 98.60it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16920/17473 [02:51<00:05, 98.66it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16940/17473 [02:51<00:05, 98.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16960/17473 [02:51<00:05, 98.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 16980/17473 [02:51<00:04, 98.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 17000/17473 [02:51<00:04, 98.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  97% 17020/17473 [02:51<00:04, 98.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17040/17473 [02:52<00:04, 99.01it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17060/17473 [02:52<00:04, 99.07it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17080/17473 [02:52<00:03, 99.13it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17100/17473 [02:52<00:03, 99.19it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17120/17473 [02:52<00:03, 99.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17140/17473 [02:52<00:03, 99.31it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17160/17473 [02:52<00:03, 99.37it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17180/17473 [02:52<00:02, 99.43it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  98% 17200/17473 [02:52<00:02, 99.49it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17220/17473 [02:52<00:02, 99.55it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17240/17473 [02:53<00:02, 99.61it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17260/17473 [02:53<00:02, 99.67it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17280/17473 [02:53<00:01, 99.72it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17300/17473 [02:53<00:01, 99.78it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17320/17473 [02:53<00:01, 99.84it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17340/17473 [02:53<00:01, 99.89it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17360/17473 [02:53<00:01, 99.95it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43:  99% 17380/17473 [02:53<00:00, 100.00it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43: 100% 17400/17473 [02:53<00:00, 100.06it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43: 100% 17420/17473 [02:54<00:00, 100.11it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43: 100% 17440/17473 [02:54<00:00, 100.16it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43: 100% 17460/17473 [02:54<00:00, 100.22it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "Epoch 43: 100% 17473/17473 [02:54<00:00, 100.25it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "                                                                 \u001b[AMonitored metric avg_val_loss did not improve in the last 10 records. Best score: 3.380. Signaling Trainer to stop.\n",
            "Epoch 43: 100% 17473/17473 [02:54<00:00, 100.02it/s, loss=3.36, v_num=24, val_loss=3.380, avg_val_loss=3.380, train_loss=3.360]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 411\n",
            "Shape of the feature representation generated by the base encoder: (41093, 64)\n",
            "end time: 1728345780.5267558\n",
            "Execution time: 2.15 hours\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_Simulation_09_final_final.h5ad\" \\\n",
        "                                              --batch_size 100 \\\n",
        "                                              --size_factor 0.35 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_simulation_beta09_final_final\" \\\n",
        "                                              --train_test 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import calinski_harabasz_score\n",
        "import anndata as ad\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WfV_iWj7OSH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adata = ad.read_h5ad('/content/drive/MyDrive/Colab Notebooks/data/Larry_Simulation_09_final_final.h5ad')\n",
        "lineage_label = adata.obs[\"clone_id\"].to_numpy()\n",
        "x = np.load(\"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_simulation_beta09_final_final/scBaseEncoderFeat_Z_bs100_tau0.5.npy\")\n",
        "\n",
        "score = calinski_harabasz_score(x, lineage_label)\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySVHic5nOUtW",
        "outputId": "c41975f7-170f-4023-9ed7-b8d8e92f2d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25254.684053753543"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
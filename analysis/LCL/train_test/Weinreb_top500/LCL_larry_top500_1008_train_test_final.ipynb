{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "e89495cd-5e2c-4238-bd6d-1cdc7b210991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFzoXTPCVub",
        "outputId": "fa910166-79fb-40e6-c856-b363d2188174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "e12c1ca7-1662-4873-ec16-e7198200f817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.7 pytorch-lightning-2.4.0 torchmetrics-1.4.2\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
            "Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: array-api-compat, anndata\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.26.4)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.4.2)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.7)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.19.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.17.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (2.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.4.0\n",
            "    Uninstalling pytorch-lightning-2.4.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.4.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.10.9)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.5)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.9)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=f08dfc561515ffe22bc6da3d2f017243593fe3d072dd500f072e858c9e2b18c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, session-info, pynndescent, umap-learn, scanpy\n",
            "Successfully installed legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525cd51b-9da0-466b-a6da-2432b42f1dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1728458186.308112\n",
            "usage: scContrastiveLearning_Main_709_ckpt_epoch.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                                                    [--batch_size BATCH_SIZE]\n",
            "                                                    [--size_factor SIZE_FACTOR]\n",
            "                                                    [--temperature TEMPERATURE]\n",
            "                                                    [--patience PATIENCE] [--min_delta MIN_DELTA]\n",
            "                                                    [--max_epoch MAX_EPOCH] --output_dir\n",
            "                                                    OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                                                    [--hidden_dims HIDDEN_DIMS]\n",
            "                                                    [--embedding_size EMBEDDING_SIZE]\n",
            "                                                    [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3V30WCiJDs9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24738f33-a09e-470d-cd80-26a2cff3cf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1728422966.8766015\n",
            "-------------------------------INFO-------------------------------\n",
            "Anndata Info:  /content/drive/MyDrive/Colab Notebooks/data/Larry_500_train.h5ad\n",
            "batch_size:  200\n",
            "size_factor:  0.7\n",
            "temperature:  0.5\n",
            "number of epochs:  220\n",
            "train_test_ratio:  0.8\n",
            "input_dim:  2000\n",
            "hidden_dims:  [1024, 256, 64]\n",
            "embedding_size:  32\n",
            "The range of number of cells in a lineage: (18, 159), average of number of cells in a lineage 34.11\n",
            "number of batches:  9047\n",
            "total number of pairs:  1809400\n",
            "num_workers(number of available CPU cores):  12\n",
            "Training the data with validation set\n",
            "-------------------------------Dataloading-------------------------------\n",
            "number of total batch: 9047\n",
            "number of training batch: 7237\n",
            "number of validation batch: 1810\n",
            "\n",
            "lineage_info shape: (1809400, 1)\n",
            "lineage_info shape of training data: (1447400, 1)\n",
            "lineage_info shape of validation data: (362000, 1)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1008_train_test_top500_final/saved_models/ exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/content/drive/MyDrive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:105: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=self.config.epochs)\n",
            "\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | AddProjectionMLP | 2.3 M \n",
            "1 | loss  | ContrastiveLoss  | 0     \n",
            "-------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.348     Total estimated model params size (MB)\n",
            "2024-10-08 21:30:38.141792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-08 21:30:38.653250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 21:30:38.882638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 21:30:38.942128: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 21:30:39.301221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-08 21:30:41.215143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.37it/s]/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 0:  80% 7220/9047 [01:30<00:22, 79.53it/s, loss=5.96, v_num=40]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 7240/9047 [01:35<00:23, 76.01it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  80% 7260/9047 [01:35<00:23, 76.12it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  80% 7280/9047 [01:35<00:23, 76.23it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  81% 7300/9047 [01:35<00:22, 76.34it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  81% 7320/9047 [01:35<00:22, 76.44it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  81% 7340/9047 [01:35<00:22, 76.55it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  81% 7360/9047 [01:36<00:22, 76.65it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  82% 7380/9047 [01:36<00:21, 76.76it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  82% 7400/9047 [01:36<00:21, 76.86it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  82% 7420/9047 [01:36<00:21, 76.96it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  82% 7440/9047 [01:36<00:20, 77.07it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  82% 7460/9047 [01:36<00:20, 77.17it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  83% 7480/9047 [01:36<00:20, 77.27it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  83% 7500/9047 [01:36<00:19, 77.37it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  83% 7520/9047 [01:37<00:19, 77.48it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  83% 7540/9047 [01:37<00:19, 77.58it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  84% 7560/9047 [01:37<00:19, 77.68it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  84% 7580/9047 [01:37<00:18, 77.79it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  84% 7600/9047 [01:37<00:18, 77.89it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  84% 7620/9047 [01:37<00:18, 77.99it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  84% 7640/9047 [01:37<00:18, 78.09it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  85% 7660/9047 [01:37<00:17, 78.19it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  85% 7680/9047 [01:38<00:17, 78.29it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  85% 7700/9047 [01:38<00:17, 78.38it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  85% 7720/9047 [01:38<00:16, 78.48it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  86% 7740/9047 [01:38<00:16, 78.58it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  86% 7760/9047 [01:38<00:16, 78.68it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  86% 7780/9047 [01:38<00:16, 78.78it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  86% 7800/9047 [01:38<00:15, 78.88it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  86% 7820/9047 [01:39<00:15, 78.98it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  87% 7840/9047 [01:39<00:15, 79.08it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  87% 7860/9047 [01:39<00:14, 79.18it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  87% 7880/9047 [01:39<00:14, 79.28it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  87% 7900/9047 [01:39<00:14, 79.38it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  88% 7920/9047 [01:39<00:14, 79.48it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  88% 7940/9047 [01:39<00:13, 79.58it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  88% 7960/9047 [01:39<00:13, 79.68it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  88% 7980/9047 [01:40<00:13, 79.78it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  88% 8000/9047 [01:40<00:13, 79.88it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  89% 8020/9047 [01:40<00:12, 79.98it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  89% 8040/9047 [01:40<00:12, 80.08it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  89% 8060/9047 [01:40<00:12, 80.18it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  89% 8080/9047 [01:40<00:12, 80.28it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  90% 8100/9047 [01:40<00:11, 80.37it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  90% 8120/9047 [01:40<00:11, 80.47it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  90% 8140/9047 [01:41<00:11, 80.57it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  90% 8160/9047 [01:41<00:10, 80.67it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  90% 8180/9047 [01:41<00:10, 80.77it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  91% 8200/9047 [01:41<00:10, 80.86it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  91% 8220/9047 [01:41<00:10, 80.96it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  91% 8240/9047 [01:41<00:09, 81.05it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  91% 8260/9047 [01:41<00:09, 81.15it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  92% 8280/9047 [01:41<00:09, 81.24it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  92% 8300/9047 [01:42<00:09, 81.34it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  92% 8320/9047 [01:42<00:08, 81.43it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  92% 8340/9047 [01:42<00:08, 81.53it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  92% 8360/9047 [01:42<00:08, 81.62it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  93% 8380/9047 [01:42<00:08, 81.72it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  93% 8400/9047 [01:42<00:07, 81.81it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  93% 8420/9047 [01:42<00:07, 81.91it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  93% 8440/9047 [01:42<00:07, 81.99it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  94% 8460/9047 [01:43<00:07, 82.08it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  94% 8480/9047 [01:43<00:06, 82.16it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  94% 8500/9047 [01:43<00:06, 82.24it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  94% 8520/9047 [01:43<00:06, 82.33it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  94% 8540/9047 [01:43<00:06, 82.42it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  95% 8560/9047 [01:43<00:05, 82.50it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  95% 8580/9047 [01:43<00:05, 82.58it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  95% 8600/9047 [01:44<00:05, 82.67it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  95% 8620/9047 [01:44<00:05, 82.76it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  96% 8640/9047 [01:44<00:04, 82.85it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  96% 8660/9047 [01:44<00:04, 82.94it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  96% 8680/9047 [01:44<00:04, 83.03it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  96% 8700/9047 [01:44<00:04, 83.12it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  96% 8720/9047 [01:44<00:03, 83.20it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  97% 8740/9047 [01:44<00:03, 83.29it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  97% 8760/9047 [01:45<00:03, 83.38it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  97% 8780/9047 [01:45<00:03, 83.47it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  97% 8800/9047 [01:45<00:02, 83.56it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  97% 8820/9047 [01:45<00:02, 83.65it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  98% 8840/9047 [01:45<00:02, 83.73it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  98% 8860/9047 [01:45<00:02, 83.82it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  98% 8880/9047 [01:45<00:01, 83.91it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  98% 8900/9047 [01:45<00:01, 84.00it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  99% 8920/9047 [01:46<00:01, 84.08it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  99% 8940/9047 [01:46<00:01, 84.16it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  99% 8960/9047 [01:46<00:01, 84.25it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  99% 8980/9047 [01:46<00:00, 84.33it/s, loss=5.96, v_num=40]\n",
            "Epoch 0:  99% 9000/9047 [01:46<00:00, 84.42it/s, loss=5.96, v_num=40]\n",
            "Epoch 0: 100% 9020/9047 [01:46<00:00, 84.51it/s, loss=5.96, v_num=40]\n",
            "Epoch 0: 100% 9040/9047 [01:46<00:00, 84.60it/s, loss=5.96, v_num=40]\n",
            "Epoch 0: 100% 9047/9047 [01:46<00:00, 84.62it/s, loss=5.96, v_num=40, val_loss=5.960, avg_val_loss=5.960]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved. New best score: 5.955\n",
            "Epoch 1:  80% 7220/9047 [01:36<00:24, 74.96it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 7240/9047 [01:40<00:25, 71.72it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  80% 7260/9047 [01:41<00:24, 71.82it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  80% 7280/9047 [01:41<00:24, 71.92it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  81% 7300/9047 [01:41<00:24, 72.02it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  81% 7320/9047 [01:41<00:23, 72.13it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  81% 7340/9047 [01:41<00:23, 72.23it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  81% 7360/9047 [01:41<00:23, 72.33it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  82% 7380/9047 [01:41<00:23, 72.44it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  82% 7400/9047 [01:42<00:22, 72.54it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  82% 7420/9047 [01:42<00:22, 72.65it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  82% 7440/9047 [01:42<00:22, 72.75it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  82% 7460/9047 [01:42<00:21, 72.86it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  83% 7480/9047 [01:42<00:21, 72.97it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  83% 7500/9047 [01:42<00:21, 73.07it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  83% 7520/9047 [01:42<00:20, 73.17it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  83% 7540/9047 [01:42<00:20, 73.27it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  84% 7560/9047 [01:43<00:20, 73.37it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  84% 7580/9047 [01:43<00:19, 73.48it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  84% 7600/9047 [01:43<00:19, 73.57it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  84% 7620/9047 [01:43<00:19, 73.68it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  84% 7640/9047 [01:43<00:19, 73.77it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  85% 7660/9047 [01:43<00:18, 73.86it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  85% 7680/9047 [01:43<00:18, 73.97it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  85% 7700/9047 [01:43<00:18, 74.07it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  85% 7720/9047 [01:44<00:17, 74.16it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  86% 7740/9047 [01:44<00:17, 74.26it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  86% 7760/9047 [01:44<00:17, 74.37it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  86% 7780/9047 [01:44<00:17, 74.47it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  86% 7800/9047 [01:44<00:16, 74.57it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  86% 7820/9047 [01:44<00:16, 74.67it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  87% 7840/9047 [01:44<00:16, 74.76it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  87% 7860/9047 [01:44<00:15, 74.86it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  87% 7880/9047 [01:45<00:15, 74.96it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  87% 7900/9047 [01:45<00:15, 75.06it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  88% 7920/9047 [01:45<00:14, 75.16it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  88% 7940/9047 [01:45<00:14, 75.26it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  88% 7960/9047 [01:45<00:14, 75.36it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  88% 7980/9047 [01:45<00:14, 75.46it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  88% 8000/9047 [01:45<00:13, 75.56it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  89% 8020/9047 [01:45<00:13, 75.66it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  89% 8040/9047 [01:46<00:13, 75.75it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  89% 8060/9047 [01:46<00:13, 75.85it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  89% 8080/9047 [01:46<00:12, 75.93it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  90% 8100/9047 [01:46<00:12, 76.03it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  90% 8120/9047 [01:46<00:12, 76.13it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  90% 8140/9047 [01:46<00:11, 76.22it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  90% 8160/9047 [01:46<00:11, 76.32it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  90% 8180/9047 [01:47<00:11, 76.41it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  91% 8200/9047 [01:47<00:11, 76.42it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  91% 8220/9047 [01:47<00:10, 76.51it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  91% 8240/9047 [01:47<00:10, 76.60it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  91% 8260/9047 [01:47<00:10, 76.69it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  92% 8280/9047 [01:47<00:09, 76.79it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  92% 8300/9047 [01:47<00:09, 76.88it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  92% 8320/9047 [01:48<00:09, 76.98it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  92% 8340/9047 [01:48<00:09, 77.08it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  92% 8360/9047 [01:48<00:08, 77.17it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  93% 8380/9047 [01:48<00:08, 77.27it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  93% 8400/9047 [01:48<00:08, 77.36it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  93% 8420/9047 [01:48<00:08, 77.46it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  93% 8440/9047 [01:48<00:07, 77.55it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  94% 8460/9047 [01:48<00:07, 77.65it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  94% 8480/9047 [01:49<00:07, 77.74it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  94% 8500/9047 [01:49<00:07, 77.84it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  94% 8520/9047 [01:49<00:06, 77.93it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  94% 8540/9047 [01:49<00:06, 78.03it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  95% 8560/9047 [01:49<00:06, 78.12it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  95% 8580/9047 [01:49<00:05, 78.22it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  95% 8600/9047 [01:49<00:05, 78.31it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  95% 8620/9047 [01:49<00:05, 78.40it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  96% 8640/9047 [01:50<00:05, 78.49it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  96% 8660/9047 [01:50<00:04, 78.58it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  96% 8680/9047 [01:50<00:04, 78.67it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  96% 8700/9047 [01:50<00:04, 78.77it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  96% 8720/9047 [01:50<00:04, 78.86it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  97% 8740/9047 [01:50<00:03, 78.95it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  97% 8760/9047 [01:50<00:03, 79.04it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  97% 8780/9047 [01:50<00:03, 79.13it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  97% 8800/9047 [01:51<00:03, 79.22it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  97% 8820/9047 [01:51<00:02, 79.31it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  98% 8840/9047 [01:51<00:02, 79.40it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  98% 8860/9047 [01:51<00:02, 79.49it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  98% 8880/9047 [01:51<00:02, 79.57it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  98% 8900/9047 [01:51<00:01, 79.66it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  99% 8920/9047 [01:51<00:01, 79.75it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  99% 8940/9047 [01:51<00:01, 79.84it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  99% 8960/9047 [01:52<00:01, 79.93it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  99% 8980/9047 [01:52<00:00, 80.02it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1:  99% 9000/9047 [01:52<00:00, 80.11it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1: 100% 9020/9047 [01:52<00:00, 80.19it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1: 100% 9040/9047 [01:52<00:00, 80.28it/s, loss=4.49, v_num=40, val_loss=5.960, avg_val_loss=5.960, train_loss=5.950]\n",
            "Epoch 1: 100% 9047/9047 [01:52<00:00, 80.30it/s, loss=4.5, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=5.950] \n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 1.470 >= min_delta = 0.001. New best score: 4.485\n",
            "Epoch 2:  80% 7220/9047 [01:35<00:24, 75.45it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 7240/9047 [01:40<00:25, 71.92it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  80% 7260/9047 [01:40<00:24, 72.02it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  80% 7280/9047 [01:40<00:24, 72.13it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  81% 7300/9047 [01:41<00:24, 72.23it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  81% 7320/9047 [01:41<00:23, 72.34it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  81% 7340/9047 [01:41<00:23, 72.44it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  81% 7360/9047 [01:41<00:23, 72.55it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  82% 7380/9047 [01:41<00:22, 72.66it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  82% 7400/9047 [01:41<00:22, 72.76it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  82% 7420/9047 [01:41<00:22, 72.87it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  82% 7440/9047 [01:41<00:22, 72.97it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  82% 7460/9047 [01:42<00:21, 73.08it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  83% 7480/9047 [01:42<00:21, 73.19it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  83% 7500/9047 [01:42<00:21, 73.29it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  83% 7520/9047 [01:42<00:20, 73.40it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  83% 7540/9047 [01:42<00:20, 73.50it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  84% 7560/9047 [01:42<00:20, 73.61it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  84% 7580/9047 [01:42<00:19, 73.71it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  84% 7600/9047 [01:42<00:19, 73.82it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  84% 7620/9047 [01:43<00:19, 73.92it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  84% 7640/9047 [01:43<00:19, 74.02it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  85% 7660/9047 [01:43<00:18, 74.12it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  85% 7680/9047 [01:43<00:18, 74.22it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  85% 7700/9047 [01:43<00:18, 74.32it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  85% 7720/9047 [01:43<00:17, 74.43it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  86% 7740/9047 [01:43<00:17, 74.53it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  86% 7760/9047 [01:43<00:17, 74.63it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  86% 7780/9047 [01:44<00:16, 74.74it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  86% 7800/9047 [01:44<00:16, 74.84it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  86% 7820/9047 [01:44<00:16, 74.94it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  87% 7840/9047 [01:44<00:16, 75.04it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  87% 7860/9047 [01:44<00:15, 75.14it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  87% 7880/9047 [01:44<00:15, 75.24it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  87% 7900/9047 [01:44<00:15, 75.35it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  88% 7920/9047 [01:44<00:14, 75.45it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  88% 7940/9047 [01:45<00:14, 75.55it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  88% 7960/9047 [01:45<00:14, 75.66it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  88% 7980/9047 [01:45<00:14, 75.76it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  88% 8000/9047 [01:45<00:13, 75.86it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  89% 8020/9047 [01:45<00:13, 75.96it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  89% 8040/9047 [01:45<00:13, 76.06it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  89% 8060/9047 [01:45<00:12, 76.16it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  89% 8080/9047 [01:45<00:12, 76.27it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  90% 8100/9047 [01:46<00:12, 76.37it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  90% 8120/9047 [01:46<00:12, 76.47it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  90% 8140/9047 [01:46<00:11, 76.57it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  90% 8160/9047 [01:46<00:11, 76.67it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  90% 8180/9047 [01:46<00:11, 76.77it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  91% 8200/9047 [01:46<00:11, 76.87it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  91% 8220/9047 [01:46<00:10, 76.97it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  91% 8240/9047 [01:46<00:10, 77.07it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  91% 8260/9047 [01:47<00:10, 77.17it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  92% 8280/9047 [01:47<00:09, 77.27it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  92% 8300/9047 [01:47<00:09, 77.37it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  92% 8320/9047 [01:47<00:09, 77.46it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  92% 8340/9047 [01:47<00:09, 77.56it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  92% 8360/9047 [01:47<00:08, 77.66it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  93% 8380/9047 [01:47<00:08, 77.76it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  93% 8400/9047 [01:47<00:08, 77.85it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  93% 8420/9047 [01:48<00:08, 77.95it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  93% 8440/9047 [01:48<00:07, 78.05it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  94% 8460/9047 [01:48<00:07, 78.15it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  94% 8480/9047 [01:48<00:07, 78.24it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  94% 8500/9047 [01:48<00:06, 78.34it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  94% 8520/9047 [01:48<00:06, 78.42it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  94% 8540/9047 [01:48<00:06, 78.52it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  95% 8560/9047 [01:48<00:06, 78.61it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  95% 8580/9047 [01:49<00:05, 78.70it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  95% 8600/9047 [01:49<00:05, 78.80it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  95% 8620/9047 [01:49<00:05, 78.89it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  96% 8640/9047 [01:49<00:05, 78.98it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  96% 8660/9047 [01:49<00:04, 79.07it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  96% 8680/9047 [01:49<00:04, 79.15it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  96% 8700/9047 [01:49<00:04, 79.23it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  96% 8720/9047 [01:49<00:04, 79.32it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  97% 8740/9047 [01:50<00:03, 79.42it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  97% 8760/9047 [01:50<00:03, 79.51it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  97% 8780/9047 [01:50<00:03, 79.60it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  97% 8800/9047 [01:50<00:03, 79.69it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  97% 8820/9047 [01:50<00:02, 79.77it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  98% 8840/9047 [01:50<00:02, 79.86it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  98% 8860/9047 [01:50<00:02, 79.94it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  98% 8880/9047 [01:50<00:02, 80.03it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  98% 8900/9047 [01:51<00:01, 80.11it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  99% 8920/9047 [01:51<00:01, 80.20it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  99% 8940/9047 [01:51<00:01, 80.28it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  99% 8960/9047 [01:51<00:01, 80.36it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  99% 8980/9047 [01:51<00:00, 80.43it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2:  99% 9000/9047 [01:51<00:00, 80.52it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2: 100% 9020/9047 [01:51<00:00, 80.60it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2: 100% 9040/9047 [01:52<00:00, 80.69it/s, loss=4.24, v_num=40, val_loss=4.490, avg_val_loss=4.490, train_loss=4.800]\n",
            "Epoch 2: 100% 9047/9047 [01:52<00:00, 80.71it/s, loss=4.24, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.800]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.247 >= min_delta = 0.001. New best score: 4.238\n",
            "Epoch 3:  80% 7220/9047 [01:37<00:24, 74.37it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 7240/9047 [01:41<00:25, 71.10it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  80% 7260/9047 [01:41<00:25, 71.20it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  80% 7280/9047 [01:42<00:24, 71.31it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  81% 7300/9047 [01:42<00:24, 71.41it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  81% 7320/9047 [01:42<00:24, 71.51it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  81% 7340/9047 [01:42<00:23, 71.61it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  81% 7360/9047 [01:42<00:23, 71.71it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  82% 7380/9047 [01:42<00:23, 71.82it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  82% 7400/9047 [01:42<00:22, 71.92it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  82% 7420/9047 [01:43<00:22, 72.03it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  82% 7440/9047 [01:43<00:22, 72.13it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  82% 7460/9047 [01:43<00:21, 72.24it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  83% 7480/9047 [01:43<00:21, 72.34it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  83% 7500/9047 [01:43<00:21, 72.45it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  83% 7520/9047 [01:43<00:21, 72.55it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  83% 7540/9047 [01:43<00:20, 72.66it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  84% 7560/9047 [01:43<00:20, 72.76it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  84% 7580/9047 [01:44<00:20, 72.87it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  84% 7600/9047 [01:44<00:19, 72.98it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  84% 7620/9047 [01:44<00:19, 73.08it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  84% 7640/9047 [01:44<00:19, 73.19it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  85% 7660/9047 [01:44<00:18, 73.29it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  85% 7680/9047 [01:44<00:18, 73.40it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  85% 7700/9047 [01:44<00:18, 73.50it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  85% 7720/9047 [01:44<00:18, 73.60it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  86% 7740/9047 [01:45<00:17, 73.69it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  86% 7760/9047 [01:45<00:17, 73.79it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  86% 7780/9047 [01:45<00:17, 73.90it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  86% 7800/9047 [01:45<00:16, 73.99it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  86% 7820/9047 [01:45<00:16, 74.09it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  87% 7840/9047 [01:45<00:16, 74.18it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  87% 7860/9047 [01:45<00:15, 74.28it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  87% 7880/9047 [01:45<00:15, 74.37it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  87% 7900/9047 [01:46<00:15, 74.47it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  88% 7920/9047 [01:46<00:15, 74.57it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  88% 7940/9047 [01:46<00:14, 74.67it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  88% 7960/9047 [01:46<00:14, 74.77it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  88% 7980/9047 [01:46<00:14, 74.87it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  88% 8000/9047 [01:46<00:13, 74.97it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  89% 8020/9047 [01:46<00:13, 75.06it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  89% 8040/9047 [01:46<00:13, 75.15it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  89% 8060/9047 [01:47<00:13, 75.24it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  89% 8080/9047 [01:47<00:12, 75.33it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  90% 8100/9047 [01:47<00:12, 75.42it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  90% 8120/9047 [01:47<00:12, 75.51it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  90% 8140/9047 [01:47<00:11, 75.61it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  90% 8160/9047 [01:47<00:11, 75.70it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  90% 8180/9047 [01:47<00:11, 75.79it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  91% 8200/9047 [01:48<00:11, 75.88it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  91% 8220/9047 [01:48<00:10, 75.97it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  91% 8240/9047 [01:48<00:10, 76.06it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  91% 8260/9047 [01:48<00:10, 76.16it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  92% 8280/9047 [01:48<00:10, 76.25it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  92% 8300/9047 [01:48<00:09, 76.34it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  92% 8320/9047 [01:48<00:09, 76.43it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  92% 8340/9047 [01:48<00:09, 76.53it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  92% 8360/9047 [01:49<00:08, 76.62it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  93% 8380/9047 [01:49<00:08, 76.72it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  93% 8400/9047 [01:49<00:08, 76.81it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  93% 8420/9047 [01:49<00:08, 76.89it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  93% 8440/9047 [01:49<00:07, 76.98it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  94% 8460/9047 [01:49<00:07, 77.07it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  94% 8480/9047 [01:49<00:07, 77.15it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  94% 8500/9047 [01:50<00:07, 77.23it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  94% 8520/9047 [01:50<00:06, 77.32it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  94% 8540/9047 [01:50<00:06, 77.41it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  95% 8560/9047 [01:50<00:06, 77.50it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  95% 8580/9047 [01:50<00:06, 77.59it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  95% 8600/9047 [01:50<00:05, 77.67it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  95% 8620/9047 [01:50<00:05, 77.76it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  96% 8640/9047 [01:50<00:05, 77.85it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  96% 8660/9047 [01:51<00:04, 77.93it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  96% 8680/9047 [01:51<00:04, 78.03it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  96% 8700/9047 [01:51<00:04, 78.12it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  96% 8720/9047 [01:51<00:04, 78.21it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  97% 8740/9047 [01:51<00:03, 78.30it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  97% 8760/9047 [01:51<00:03, 78.39it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  97% 8780/9047 [01:51<00:03, 78.48it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  97% 8800/9047 [01:52<00:03, 78.57it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  97% 8820/9047 [01:52<00:02, 78.65it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  98% 8840/9047 [01:52<00:02, 78.74it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  98% 8860/9047 [01:52<00:02, 78.82it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  98% 8880/9047 [01:52<00:02, 78.91it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  98% 8900/9047 [01:52<00:01, 79.00it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  99% 8920/9047 [01:52<00:01, 79.08it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  99% 8940/9047 [01:52<00:01, 79.16it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  99% 8960/9047 [01:53<00:01, 79.24it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  99% 8980/9047 [01:53<00:00, 79.33it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3:  99% 9000/9047 [01:53<00:00, 79.41it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3: 100% 9020/9047 [01:53<00:00, 79.50it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3: 100% 9040/9047 [01:53<00:00, 79.59it/s, loss=4.18, v_num=40, val_loss=4.240, avg_val_loss=4.240, train_loss=4.330]\n",
            "Epoch 3: 100% 9047/9047 [01:53<00:00, 79.62it/s, loss=4.18, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.330]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.062 >= min_delta = 0.001. New best score: 4.176\n",
            "Epoch 4:  80% 7220/9047 [01:37<00:24, 74.30it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 7240/9047 [01:41<00:25, 71.00it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  80% 7260/9047 [01:42<00:25, 71.09it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  80% 7280/9047 [01:42<00:24, 71.19it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  81% 7300/9047 [01:42<00:24, 71.30it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  81% 7320/9047 [01:42<00:24, 71.41it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  81% 7340/9047 [01:42<00:23, 71.51it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  81% 7360/9047 [01:42<00:23, 71.61it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  82% 7380/9047 [01:42<00:23, 71.71it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  82% 7400/9047 [01:43<00:22, 71.81it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  82% 7420/9047 [01:43<00:22, 71.91it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  82% 7440/9047 [01:43<00:22, 72.01it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  82% 7460/9047 [01:43<00:22, 72.12it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  83% 7480/9047 [01:43<00:21, 72.22it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  83% 7500/9047 [01:43<00:21, 72.32it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  83% 7520/9047 [01:43<00:21, 72.41it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  83% 7540/9047 [01:43<00:20, 72.51it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  84% 7560/9047 [01:44<00:20, 72.61it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  84% 7580/9047 [01:44<00:20, 72.70it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  84% 7600/9047 [01:44<00:19, 72.80it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  84% 7620/9047 [01:44<00:19, 72.90it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  84% 7640/9047 [01:44<00:19, 73.00it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  85% 7660/9047 [01:44<00:18, 73.11it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  85% 7680/9047 [01:44<00:18, 73.20it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  85% 7700/9047 [01:45<00:18, 73.30it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  85% 7720/9047 [01:45<00:18, 73.40it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  86% 7740/9047 [01:45<00:17, 73.49it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  86% 7760/9047 [01:45<00:17, 73.59it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  86% 7780/9047 [01:45<00:17, 73.69it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  86% 7800/9047 [01:45<00:16, 73.79it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  86% 7820/9047 [01:45<00:16, 73.89it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  87% 7840/9047 [01:45<00:16, 73.99it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  87% 7860/9047 [01:46<00:16, 74.08it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  87% 7880/9047 [01:46<00:15, 74.18it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  87% 7900/9047 [01:46<00:15, 74.28it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  88% 7920/9047 [01:46<00:15, 74.38it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  88% 7940/9047 [01:46<00:14, 74.48it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  88% 7960/9047 [01:46<00:14, 74.58it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  88% 7980/9047 [01:46<00:14, 74.68it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  88% 8000/9047 [01:46<00:14, 74.78it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  89% 8020/9047 [01:47<00:13, 74.88it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  89% 8040/9047 [01:47<00:13, 74.98it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  89% 8060/9047 [01:47<00:13, 75.08it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  89% 8080/9047 [01:47<00:12, 75.17it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  90% 8100/9047 [01:47<00:12, 75.27it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  90% 8120/9047 [01:47<00:12, 75.36it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  90% 8140/9047 [01:47<00:12, 75.44it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  90% 8160/9047 [01:48<00:11, 75.54it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  90% 8180/9047 [01:48<00:11, 75.64it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  91% 8200/9047 [01:48<00:11, 75.74it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  91% 8220/9047 [01:48<00:10, 75.83it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  91% 8240/9047 [01:48<00:10, 75.93it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  91% 8260/9047 [01:48<00:10, 76.02it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  92% 8280/9047 [01:48<00:10, 76.12it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  92% 8300/9047 [01:48<00:09, 76.22it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  92% 8320/9047 [01:49<00:09, 76.31it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  92% 8340/9047 [01:49<00:09, 76.41it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  92% 8360/9047 [01:49<00:08, 76.50it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  93% 8380/9047 [01:49<00:08, 76.60it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  93% 8400/9047 [01:49<00:08, 76.70it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  93% 8420/9047 [01:49<00:08, 76.79it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  93% 8440/9047 [01:49<00:07, 76.89it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  94% 8460/9047 [01:49<00:07, 76.98it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  94% 8480/9047 [01:50<00:07, 77.08it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  94% 8500/9047 [01:50<00:07, 77.18it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  94% 8520/9047 [01:50<00:06, 77.27it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  94% 8540/9047 [01:50<00:06, 77.36it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  95% 8560/9047 [01:50<00:06, 77.45it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  95% 8580/9047 [01:50<00:06, 77.55it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  95% 8600/9047 [01:50<00:05, 77.64it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  95% 8620/9047 [01:50<00:05, 77.73it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  96% 8640/9047 [01:51<00:05, 77.82it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  96% 8660/9047 [01:51<00:04, 77.91it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  96% 8680/9047 [01:51<00:04, 78.01it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  96% 8700/9047 [01:51<00:04, 78.10it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  96% 8720/9047 [01:51<00:04, 78.19it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  97% 8740/9047 [01:51<00:03, 78.28it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  97% 8760/9047 [01:51<00:03, 78.38it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  97% 8780/9047 [01:51<00:03, 78.47it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  97% 8800/9047 [01:52<00:03, 78.56it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  97% 8820/9047 [01:52<00:02, 78.65it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  98% 8840/9047 [01:52<00:02, 78.74it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  98% 8860/9047 [01:52<00:02, 78.83it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  98% 8880/9047 [01:52<00:02, 78.93it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  98% 8900/9047 [01:52<00:01, 79.02it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  99% 8920/9047 [01:52<00:01, 79.10it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  99% 8940/9047 [01:52<00:01, 79.19it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  99% 8960/9047 [01:53<00:01, 79.28it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  99% 8980/9047 [01:53<00:00, 79.37it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4:  99% 9000/9047 [01:53<00:00, 79.45it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4: 100% 9020/9047 [01:53<00:00, 79.54it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4: 100% 9040/9047 [01:53<00:00, 79.63it/s, loss=4.13, v_num=40, val_loss=4.180, avg_val_loss=4.180, train_loss=4.200]\n",
            "Epoch 4: 100% 9047/9047 [01:53<00:00, 79.66it/s, loss=4.13, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.200]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.047 >= min_delta = 0.001. New best score: 4.130\n",
            "Epoch 5:  80% 7220/9047 [01:36<00:24, 74.81it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 7240/9047 [01:41<00:25, 71.37it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  80% 7260/9047 [01:41<00:25, 71.47it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  80% 7280/9047 [01:41<00:24, 71.58it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  81% 7300/9047 [01:41<00:24, 71.69it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  81% 7320/9047 [01:41<00:24, 71.80it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  81% 7340/9047 [01:42<00:23, 71.91it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  81% 7360/9047 [01:42<00:23, 72.01it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  82% 7380/9047 [01:42<00:23, 72.11it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  82% 7400/9047 [01:42<00:22, 72.22it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  82% 7420/9047 [01:42<00:22, 72.32it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  82% 7440/9047 [01:42<00:22, 72.42it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  82% 7460/9047 [01:42<00:21, 72.52it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  83% 7480/9047 [01:42<00:21, 72.62it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  83% 7500/9047 [01:43<00:21, 72.73it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  83% 7520/9047 [01:43<00:20, 72.83it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  83% 7540/9047 [01:43<00:20, 72.93it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  84% 7560/9047 [01:43<00:20, 73.03it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  84% 7580/9047 [01:43<00:20, 73.13it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  84% 7600/9047 [01:43<00:19, 73.23it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  84% 7620/9047 [01:43<00:19, 73.33it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  84% 7640/9047 [01:44<00:19, 73.43it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  85% 7660/9047 [01:44<00:18, 73.53it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  85% 7680/9047 [01:44<00:18, 73.63it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  85% 7700/9047 [01:44<00:18, 73.73it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  85% 7720/9047 [01:44<00:17, 73.83it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  86% 7740/9047 [01:44<00:17, 73.93it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  86% 7760/9047 [01:44<00:17, 74.03it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  86% 7780/9047 [01:44<00:17, 74.13it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  86% 7800/9047 [01:45<00:16, 74.23it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  86% 7820/9047 [01:45<00:16, 74.33it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  87% 7840/9047 [01:45<00:16, 74.43it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  87% 7860/9047 [01:45<00:15, 74.53it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  87% 7880/9047 [01:45<00:15, 74.63it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  87% 7900/9047 [01:45<00:15, 74.73it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  88% 7920/9047 [01:45<00:15, 74.83it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  88% 7940/9047 [01:45<00:14, 74.93it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  88% 7960/9047 [01:46<00:14, 75.03it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  88% 7980/9047 [01:46<00:14, 75.13it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  88% 8000/9047 [01:46<00:13, 75.23it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  89% 8020/9047 [01:46<00:13, 75.33it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  89% 8040/9047 [01:46<00:13, 75.43it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  89% 8060/9047 [01:46<00:13, 75.53it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  89% 8080/9047 [01:46<00:12, 75.63it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  90% 8100/9047 [01:46<00:12, 75.72it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  90% 8120/9047 [01:47<00:12, 75.82it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  90% 8140/9047 [01:47<00:11, 75.92it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  90% 8160/9047 [01:47<00:11, 76.02it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  90% 8180/9047 [01:47<00:11, 76.12it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  91% 8200/9047 [01:47<00:11, 76.21it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  91% 8220/9047 [01:47<00:10, 76.30it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  91% 8240/9047 [01:47<00:10, 76.39it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  91% 8260/9047 [01:47<00:10, 76.48it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  92% 8280/9047 [01:48<00:10, 76.58it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  92% 8300/9047 [01:48<00:09, 76.67it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  92% 8320/9047 [01:48<00:09, 76.76it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  92% 8340/9047 [01:48<00:09, 76.85it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  92% 8360/9047 [01:48<00:08, 76.94it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  93% 8380/9047 [01:48<00:08, 77.03it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  93% 8400/9047 [01:48<00:08, 77.12it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  93% 8420/9047 [01:49<00:08, 77.21it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  93% 8440/9047 [01:49<00:07, 77.30it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  94% 8460/9047 [01:49<00:07, 77.38it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  94% 8480/9047 [01:49<00:07, 77.47it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  94% 8500/9047 [01:49<00:07, 77.56it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  94% 8520/9047 [01:49<00:06, 77.65it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  94% 8540/9047 [01:49<00:06, 77.74it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  95% 8560/9047 [01:49<00:06, 77.84it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  95% 8580/9047 [01:50<00:05, 77.93it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  95% 8600/9047 [01:50<00:05, 78.02it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  95% 8620/9047 [01:50<00:05, 78.12it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  96% 8640/9047 [01:50<00:05, 78.21it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  96% 8660/9047 [01:50<00:04, 78.30it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  96% 8680/9047 [01:50<00:04, 78.39it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  96% 8700/9047 [01:50<00:04, 78.48it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  96% 8720/9047 [01:50<00:04, 78.57it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  97% 8740/9047 [01:51<00:03, 78.66it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  97% 8760/9047 [01:51<00:03, 78.75it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  97% 8780/9047 [01:51<00:03, 78.85it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  97% 8800/9047 [01:51<00:03, 78.94it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  97% 8820/9047 [01:51<00:02, 79.03it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  98% 8840/9047 [01:51<00:02, 79.10it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  98% 8860/9047 [01:51<00:02, 79.19it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  98% 8880/9047 [01:52<00:02, 79.28it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  98% 8900/9047 [01:52<00:01, 79.37it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  99% 8920/9047 [01:52<00:01, 79.46it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  99% 8940/9047 [01:52<00:01, 79.54it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  99% 8960/9047 [01:52<00:01, 79.63it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  99% 8980/9047 [01:52<00:00, 79.71it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5:  99% 9000/9047 [01:52<00:00, 79.79it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5: 100% 9020/9047 [01:52<00:00, 79.88it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5: 100% 9040/9047 [01:53<00:00, 79.97it/s, loss=4.1, v_num=40, val_loss=4.130, avg_val_loss=4.130, train_loss=4.150]\n",
            "Epoch 5: 100% 9047/9047 [01:53<00:00, 79.99it/s, loss=4.1, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.150]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.022 >= min_delta = 0.001. New best score: 4.107\n",
            "Epoch 6:  80% 7220/9047 [01:37<00:24, 74.31it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 7240/9047 [01:41<00:25, 70.99it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  80% 7260/9047 [01:42<00:25, 71.09it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  80% 7280/9047 [01:42<00:24, 71.19it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  81% 7300/9047 [01:42<00:24, 71.29it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  81% 7320/9047 [01:42<00:24, 71.39it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  81% 7340/9047 [01:42<00:23, 71.49it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  81% 7360/9047 [01:42<00:23, 71.59it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  82% 7380/9047 [01:42<00:23, 71.70it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  82% 7400/9047 [01:43<00:22, 71.80it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  82% 7420/9047 [01:43<00:22, 71.90it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  82% 7440/9047 [01:43<00:22, 72.00it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  82% 7460/9047 [01:43<00:22, 72.10it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  83% 7480/9047 [01:43<00:21, 72.21it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  83% 7500/9047 [01:43<00:21, 72.32it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  83% 7520/9047 [01:43<00:21, 72.42it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  83% 7540/9047 [01:43<00:20, 72.52it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  84% 7560/9047 [01:44<00:20, 72.62it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  84% 7580/9047 [01:44<00:20, 72.72it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  84% 7600/9047 [01:44<00:19, 72.82it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  84% 7620/9047 [01:44<00:19, 72.92it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  84% 7640/9047 [01:44<00:19, 73.02it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  85% 7660/9047 [01:44<00:18, 73.12it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  85% 7680/9047 [01:44<00:18, 73.22it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  85% 7700/9047 [01:45<00:18, 73.32it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  85% 7720/9047 [01:45<00:18, 73.41it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  86% 7740/9047 [01:45<00:17, 73.50it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  86% 7760/9047 [01:45<00:17, 73.59it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  86% 7780/9047 [01:45<00:17, 73.70it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  86% 7800/9047 [01:45<00:16, 73.80it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  86% 7820/9047 [01:45<00:16, 73.90it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  87% 7840/9047 [01:45<00:16, 74.00it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  87% 7860/9047 [01:46<00:16, 74.09it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  87% 7880/9047 [01:46<00:15, 74.19it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  87% 7900/9047 [01:46<00:15, 74.28it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  88% 7920/9047 [01:46<00:15, 74.37it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  88% 7940/9047 [01:46<00:14, 74.47it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  88% 7960/9047 [01:46<00:14, 74.57it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  88% 7980/9047 [01:46<00:14, 74.66it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  88% 8000/9047 [01:47<00:14, 74.75it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  89% 8020/9047 [01:47<00:13, 74.84it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  89% 8040/9047 [01:47<00:13, 74.94it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  89% 8060/9047 [01:47<00:13, 75.04it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  89% 8080/9047 [01:47<00:12, 75.14it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  90% 8100/9047 [01:47<00:12, 75.24it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  90% 8120/9047 [01:47<00:12, 75.32it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  90% 8140/9047 [01:47<00:12, 75.41it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  90% 8160/9047 [01:48<00:11, 75.50it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  90% 8180/9047 [01:48<00:11, 75.59it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  91% 8200/9047 [01:48<00:11, 75.67it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  91% 8220/9047 [01:48<00:10, 75.75it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  91% 8240/9047 [01:48<00:10, 75.83it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  91% 8260/9047 [01:48<00:10, 75.92it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  92% 8280/9047 [01:48<00:10, 76.00it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  92% 8300/9047 [01:49<00:09, 76.07it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  92% 8320/9047 [01:49<00:09, 76.15it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  92% 8340/9047 [01:49<00:09, 76.24it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  92% 8360/9047 [01:49<00:09, 76.33it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  93% 8380/9047 [01:49<00:08, 76.41it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  93% 8400/9047 [01:49<00:08, 76.48it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  93% 8420/9047 [01:49<00:08, 76.57it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  93% 8440/9047 [01:50<00:07, 76.66it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  94% 8460/9047 [01:50<00:07, 76.75it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  94% 8480/9047 [01:50<00:07, 76.83it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  94% 8500/9047 [01:50<00:07, 76.92it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  94% 8520/9047 [01:50<00:06, 77.02it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  94% 8540/9047 [01:50<00:06, 77.11it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  95% 8560/9047 [01:50<00:06, 77.20it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  95% 8580/9047 [01:51<00:06, 77.27it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  95% 8600/9047 [01:51<00:05, 77.37it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  95% 8620/9047 [01:51<00:05, 77.45it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  96% 8640/9047 [01:51<00:05, 77.54it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  96% 8660/9047 [01:51<00:04, 77.63it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  96% 8680/9047 [01:51<00:04, 77.72it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  96% 8700/9047 [01:51<00:04, 77.82it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  96% 8720/9047 [01:51<00:04, 77.90it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  97% 8740/9047 [01:52<00:03, 77.99it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  97% 8760/9047 [01:52<00:03, 78.08it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  97% 8780/9047 [01:52<00:03, 78.17it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  97% 8800/9047 [01:52<00:03, 78.25it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  97% 8820/9047 [01:52<00:02, 78.34it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  98% 8840/9047 [01:52<00:02, 78.42it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  98% 8860/9047 [01:52<00:02, 78.50it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  98% 8880/9047 [01:53<00:02, 78.58it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  98% 8900/9047 [01:53<00:01, 78.66it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  99% 8920/9047 [01:53<00:01, 78.74it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  99% 8940/9047 [01:53<00:01, 78.83it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  99% 8960/9047 [01:53<00:01, 78.92it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  99% 8980/9047 [01:53<00:00, 79.01it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6:  99% 9000/9047 [01:53<00:00, 79.10it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6: 100% 9020/9047 [01:53<00:00, 79.19it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6: 100% 9040/9047 [01:54<00:00, 79.29it/s, loss=4.09, v_num=40, val_loss=4.110, avg_val_loss=4.110, train_loss=4.120]\n",
            "Epoch 6: 100% 9047/9047 [01:54<00:00, 79.31it/s, loss=4.09, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.120]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.016 >= min_delta = 0.001. New best score: 4.092\n",
            "Epoch 7:  80% 7220/9047 [01:37<00:24, 74.21it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 7240/9047 [01:42<00:25, 70.75it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  80% 7260/9047 [01:42<00:25, 70.86it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  80% 7280/9047 [01:42<00:24, 70.97it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  81% 7300/9047 [01:42<00:24, 71.07it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  81% 7320/9047 [01:42<00:24, 71.17it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  81% 7340/9047 [01:42<00:23, 71.27it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  81% 7360/9047 [01:43<00:23, 71.38it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  82% 7380/9047 [01:43<00:23, 71.48it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  82% 7400/9047 [01:43<00:23, 71.58it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  82% 7420/9047 [01:43<00:22, 71.68it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  82% 7440/9047 [01:43<00:22, 71.78it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  82% 7460/9047 [01:43<00:22, 71.88it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  83% 7480/9047 [01:43<00:21, 71.99it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  83% 7500/9047 [01:44<00:21, 72.09it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  83% 7520/9047 [01:44<00:21, 72.19it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  83% 7540/9047 [01:44<00:20, 72.29it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  84% 7560/9047 [01:44<00:20, 72.38it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  84% 7580/9047 [01:44<00:20, 72.47it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  84% 7600/9047 [01:44<00:19, 72.56it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  84% 7620/9047 [01:44<00:19, 72.65it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  84% 7640/9047 [01:45<00:19, 72.75it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  85% 7660/9047 [01:45<00:19, 72.84it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  85% 7680/9047 [01:45<00:18, 72.94it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  85% 7700/9047 [01:45<00:18, 73.03it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  85% 7720/9047 [01:45<00:18, 73.12it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  86% 7740/9047 [01:45<00:17, 73.21it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  86% 7760/9047 [01:45<00:17, 73.31it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  86% 7780/9047 [01:45<00:17, 73.41it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  86% 7800/9047 [01:46<00:16, 73.51it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  86% 7820/9047 [01:46<00:16, 73.61it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  87% 7840/9047 [01:46<00:16, 73.70it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  87% 7860/9047 [01:46<00:16, 73.81it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  87% 7880/9047 [01:46<00:15, 73.91it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  87% 7900/9047 [01:46<00:15, 73.99it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  88% 7920/9047 [01:46<00:15, 74.09it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  88% 7940/9047 [01:47<00:14, 74.18it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  88% 7960/9047 [01:47<00:14, 74.27it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  88% 7980/9047 [01:47<00:14, 74.37it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  88% 8000/9047 [01:47<00:14, 74.46it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  89% 8020/9047 [01:47<00:13, 74.56it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  89% 8040/9047 [01:47<00:13, 74.65it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  89% 8060/9047 [01:47<00:13, 74.74it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  89% 8080/9047 [01:47<00:12, 74.84it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  90% 8100/9047 [01:48<00:12, 74.94it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  90% 8120/9047 [01:48<00:12, 75.04it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  90% 8140/9047 [01:48<00:12, 75.13it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  90% 8160/9047 [01:48<00:11, 75.23it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  90% 8180/9047 [01:48<00:11, 75.32it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  91% 8200/9047 [01:48<00:11, 75.41it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  91% 8220/9047 [01:48<00:10, 75.51it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  91% 8240/9047 [01:48<00:10, 75.60it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  91% 8260/9047 [01:49<00:10, 75.69it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  92% 8280/9047 [01:49<00:10, 75.78it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  92% 8300/9047 [01:49<00:09, 75.87it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  92% 8320/9047 [01:49<00:09, 75.95it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  92% 8340/9047 [01:49<00:09, 76.03it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  92% 8360/9047 [01:49<00:09, 76.12it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  93% 8380/9047 [01:49<00:08, 76.22it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  93% 8400/9047 [01:50<00:08, 76.31it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  93% 8420/9047 [01:50<00:08, 76.40it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  93% 8440/9047 [01:50<00:07, 76.49it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  94% 8460/9047 [01:50<00:07, 76.59it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  94% 8480/9047 [01:50<00:07, 76.68it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  94% 8500/9047 [01:50<00:07, 76.77it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  94% 8520/9047 [01:50<00:06, 76.86it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  94% 8540/9047 [01:50<00:06, 76.95it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  95% 8560/9047 [01:51<00:06, 77.04it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  95% 8580/9047 [01:51<00:06, 77.13it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  95% 8600/9047 [01:51<00:05, 77.23it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  95% 8620/9047 [01:51<00:05, 77.32it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  96% 8640/9047 [01:51<00:05, 77.41it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  96% 8660/9047 [01:51<00:04, 77.51it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  96% 8680/9047 [01:51<00:04, 77.59it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  96% 8700/9047 [01:51<00:04, 77.68it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  96% 8720/9047 [01:52<00:04, 77.77it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  97% 8740/9047 [01:52<00:03, 77.86it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  97% 8760/9047 [01:52<00:03, 77.95it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  97% 8780/9047 [01:52<00:03, 78.04it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  97% 8800/9047 [01:52<00:03, 78.13it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  97% 8820/9047 [01:52<00:02, 78.22it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  98% 8840/9047 [01:52<00:02, 78.31it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  98% 8860/9047 [01:53<00:02, 78.40it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  98% 8880/9047 [01:53<00:02, 78.49it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  98% 8900/9047 [01:53<00:01, 78.58it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  99% 8920/9047 [01:53<00:01, 78.67it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  99% 8940/9047 [01:53<00:01, 78.76it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  99% 8960/9047 [01:53<00:01, 78.85it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  99% 8980/9047 [01:53<00:00, 78.94it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7:  99% 9000/9047 [01:53<00:00, 79.03it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7: 100% 9020/9047 [01:54<00:00, 79.11it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7: 100% 9040/9047 [01:54<00:00, 79.20it/s, loss=4.08, v_num=40, val_loss=4.090, avg_val_loss=4.090, train_loss=4.100]\n",
            "Epoch 7: 100% 9047/9047 [01:54<00:00, 79.23it/s, loss=4.08, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.100]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.010 >= min_delta = 0.001. New best score: 4.082\n",
            "Epoch 8:  80% 7220/9047 [01:37<00:24, 74.36it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 7240/9047 [01:42<00:25, 70.89it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  80% 7260/9047 [01:42<00:25, 70.99it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  80% 7280/9047 [01:42<00:24, 71.09it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  81% 7300/9047 [01:42<00:24, 71.19it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  81% 7320/9047 [01:42<00:24, 71.30it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  81% 7340/9047 [01:42<00:23, 71.40it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  81% 7360/9047 [01:42<00:23, 71.50it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  82% 7380/9047 [01:43<00:23, 71.60it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  82% 7400/9047 [01:43<00:22, 71.70it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  82% 7420/9047 [01:43<00:22, 71.80it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  82% 7440/9047 [01:43<00:22, 71.91it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  82% 7460/9047 [01:43<00:22, 72.01it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  83% 7480/9047 [01:43<00:21, 72.11it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  83% 7500/9047 [01:43<00:21, 72.21it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  83% 7520/9047 [01:43<00:21, 72.31it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  83% 7540/9047 [01:44<00:20, 72.41it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  84% 7560/9047 [01:44<00:20, 72.51it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  84% 7580/9047 [01:44<00:20, 72.61it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  84% 7600/9047 [01:44<00:19, 72.72it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  84% 7620/9047 [01:44<00:19, 72.82it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  84% 7640/9047 [01:44<00:19, 72.91it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  85% 7660/9047 [01:44<00:18, 73.01it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  85% 7680/9047 [01:45<00:18, 73.11it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  85% 7700/9047 [01:45<00:18, 73.20it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  85% 7720/9047 [01:45<00:18, 73.30it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  86% 7740/9047 [01:45<00:17, 73.40it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  86% 7760/9047 [01:45<00:17, 73.50it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  86% 7780/9047 [01:45<00:17, 73.60it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  86% 7800/9047 [01:45<00:16, 73.69it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  86% 7820/9047 [01:45<00:16, 73.79it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  87% 7840/9047 [01:46<00:16, 73.88it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  87% 7860/9047 [01:46<00:16, 73.98it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  87% 7880/9047 [01:46<00:15, 74.08it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  87% 7900/9047 [01:46<00:15, 74.18it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  88% 7920/9047 [01:46<00:15, 74.28it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  88% 7940/9047 [01:46<00:14, 74.38it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  88% 7960/9047 [01:46<00:14, 74.47it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  88% 7980/9047 [01:47<00:14, 74.57it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  88% 8000/9047 [01:47<00:14, 74.66it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  89% 8020/9047 [01:47<00:13, 74.76it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  89% 8040/9047 [01:47<00:13, 74.85it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  89% 8060/9047 [01:47<00:13, 74.93it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  89% 8080/9047 [01:47<00:12, 75.03it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  90% 8100/9047 [01:47<00:12, 75.13it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  90% 8120/9047 [01:47<00:12, 75.23it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  90% 8140/9047 [01:48<00:12, 75.32it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  90% 8160/9047 [01:48<00:11, 75.42it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  90% 8180/9047 [01:48<00:11, 75.50it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  91% 8200/9047 [01:48<00:11, 75.60it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  91% 8220/9047 [01:48<00:10, 75.69it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  91% 8240/9047 [01:48<00:10, 75.79it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  91% 8260/9047 [01:48<00:10, 75.88it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  92% 8280/9047 [01:48<00:10, 75.98it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  92% 8300/9047 [01:49<00:09, 76.08it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  92% 8320/9047 [01:49<00:09, 76.16it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  92% 8340/9047 [01:49<00:09, 76.24it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  92% 8360/9047 [01:49<00:08, 76.34it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  93% 8380/9047 [01:49<00:08, 76.42it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  93% 8400/9047 [01:49<00:08, 76.52it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  93% 8420/9047 [01:49<00:08, 76.61it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  93% 8440/9047 [01:50<00:07, 76.71it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  94% 8460/9047 [01:50<00:07, 76.80it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  94% 8480/9047 [01:50<00:07, 76.89it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  94% 8500/9047 [01:50<00:07, 76.99it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  94% 8520/9047 [01:50<00:06, 77.08it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  94% 8540/9047 [01:50<00:06, 77.17it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  95% 8560/9047 [01:50<00:06, 77.25it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  95% 8580/9047 [01:50<00:06, 77.35it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  95% 8600/9047 [01:51<00:05, 77.44it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  95% 8620/9047 [01:51<00:05, 77.53it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  96% 8640/9047 [01:51<00:05, 77.63it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  96% 8660/9047 [01:51<00:04, 77.72it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  96% 8680/9047 [01:51<00:04, 77.81it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  96% 8700/9047 [01:51<00:04, 77.90it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  96% 8720/9047 [01:51<00:04, 77.99it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  97% 8740/9047 [01:51<00:03, 78.08it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  97% 8760/9047 [01:52<00:03, 78.16it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  97% 8780/9047 [01:52<00:03, 78.25it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  97% 8800/9047 [01:52<00:03, 78.34it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  97% 8820/9047 [01:52<00:02, 78.43it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  98% 8840/9047 [01:52<00:02, 78.50it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  98% 8860/9047 [01:52<00:02, 78.58it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  98% 8880/9047 [01:52<00:02, 78.66it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  98% 8900/9047 [01:53<00:01, 78.75it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  99% 8920/9047 [01:53<00:01, 78.84it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  99% 8940/9047 [01:53<00:01, 78.91it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  99% 8960/9047 [01:53<00:01, 79.00it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  99% 8980/9047 [01:53<00:00, 79.09it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8:  99% 9000/9047 [01:53<00:00, 79.17it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8: 100% 9020/9047 [01:53<00:00, 79.26it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8: 100% 9040/9047 [01:53<00:00, 79.35it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 8: 100% 9047/9047 [01:53<00:00, 79.38it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.005 >= min_delta = 0.001. New best score: 4.077\n",
            "Epoch 9:  80% 7220/9047 [01:37<00:24, 74.24it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 7240/9047 [01:42<00:25, 70.83it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  80% 7260/9047 [01:42<00:25, 70.93it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  80% 7280/9047 [01:42<00:24, 71.03it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  81% 7300/9047 [01:42<00:24, 71.13it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  81% 7320/9047 [01:42<00:24, 71.24it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  81% 7340/9047 [01:42<00:23, 71.34it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  81% 7360/9047 [01:43<00:23, 71.44it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  82% 7380/9047 [01:43<00:23, 71.53it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  82% 7400/9047 [01:43<00:22, 71.63it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  82% 7420/9047 [01:43<00:22, 71.74it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  82% 7440/9047 [01:43<00:22, 71.84it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  82% 7460/9047 [01:43<00:22, 71.94it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  83% 7480/9047 [01:43<00:21, 72.04it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  83% 7500/9047 [01:43<00:21, 72.13it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  83% 7520/9047 [01:44<00:21, 72.24it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  83% 7540/9047 [01:44<00:20, 72.34it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  84% 7560/9047 [01:44<00:20, 72.44it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  84% 7580/9047 [01:44<00:20, 72.55it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  84% 7600/9047 [01:44<00:19, 72.64it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  84% 7620/9047 [01:44<00:19, 72.74it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  84% 7640/9047 [01:44<00:19, 72.84it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  85% 7660/9047 [01:45<00:19, 72.94it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  85% 7680/9047 [01:45<00:18, 73.05it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  85% 7700/9047 [01:45<00:18, 73.14it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  85% 7720/9047 [01:45<00:18, 73.23it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  86% 7740/9047 [01:45<00:17, 73.32it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  86% 7760/9047 [01:45<00:17, 73.41it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  86% 7780/9047 [01:45<00:17, 73.50it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  86% 7800/9047 [01:45<00:16, 73.60it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  86% 7820/9047 [01:46<00:16, 73.70it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  87% 7840/9047 [01:46<00:16, 73.79it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  87% 7860/9047 [01:46<00:16, 73.88it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  87% 7880/9047 [01:46<00:15, 73.98it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  87% 7900/9047 [01:46<00:15, 74.07it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  88% 7920/9047 [01:46<00:15, 74.17it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  88% 7940/9047 [01:46<00:14, 74.27it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  88% 7960/9047 [01:47<00:14, 74.36it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  88% 7980/9047 [01:47<00:14, 74.45it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  88% 8000/9047 [01:47<00:14, 74.55it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  89% 8020/9047 [01:47<00:13, 74.65it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  89% 8040/9047 [01:47<00:13, 74.74it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  89% 8060/9047 [01:47<00:13, 74.84it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  89% 8080/9047 [01:47<00:12, 74.94it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  90% 8100/9047 [01:47<00:12, 75.03it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  90% 8120/9047 [01:48<00:12, 75.11it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  90% 8140/9047 [01:48<00:12, 75.20it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  90% 8160/9047 [01:48<00:11, 75.29it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  90% 8180/9047 [01:48<00:11, 75.38it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  91% 8200/9047 [01:48<00:11, 75.48it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  91% 8220/9047 [01:48<00:10, 75.56it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  91% 8240/9047 [01:48<00:10, 75.64it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  91% 8260/9047 [01:49<00:10, 75.73it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  92% 8280/9047 [01:49<00:10, 75.73it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  92% 8300/9047 [01:49<00:09, 75.82it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  92% 8320/9047 [01:49<00:09, 75.92it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  92% 8340/9047 [01:49<00:09, 76.01it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  92% 8360/9047 [01:49<00:09, 76.11it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  93% 8380/9047 [01:49<00:08, 76.20it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  93% 8400/9047 [01:50<00:08, 76.29it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  93% 8420/9047 [01:50<00:08, 76.38it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  93% 8440/9047 [01:50<00:07, 76.46it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  94% 8460/9047 [01:50<00:07, 76.55it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  94% 8480/9047 [01:50<00:07, 76.64it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  94% 8500/9047 [01:50<00:07, 76.74it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  94% 8520/9047 [01:50<00:06, 76.83it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  94% 8540/9047 [01:51<00:06, 76.92it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  95% 8560/9047 [01:51<00:06, 77.01it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  95% 8580/9047 [01:51<00:06, 77.10it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  95% 8600/9047 [01:51<00:05, 77.19it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  95% 8620/9047 [01:51<00:05, 77.28it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  96% 8640/9047 [01:51<00:05, 77.37it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  96% 8660/9047 [01:51<00:04, 77.47it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  96% 8680/9047 [01:51<00:04, 77.56it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  96% 8700/9047 [01:52<00:04, 77.66it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  96% 8720/9047 [01:52<00:04, 77.75it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  97% 8740/9047 [01:52<00:03, 77.84it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  97% 8760/9047 [01:52<00:03, 77.94it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  97% 8780/9047 [01:52<00:03, 78.03it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  97% 8800/9047 [01:52<00:03, 78.13it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  97% 8820/9047 [01:52<00:02, 78.22it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  98% 8840/9047 [01:52<00:02, 78.31it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  98% 8860/9047 [01:53<00:02, 78.41it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  98% 8880/9047 [01:53<00:02, 78.49it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  98% 8900/9047 [01:53<00:01, 78.58it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  99% 8920/9047 [01:53<00:01, 78.67it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  99% 8940/9047 [01:53<00:01, 78.76it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  99% 8960/9047 [01:53<00:01, 78.86it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  99% 8980/9047 [01:53<00:00, 78.95it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9:  99% 9000/9047 [01:53<00:00, 79.04it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9: 100% 9020/9047 [01:53<00:00, 79.12it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9: 100% 9040/9047 [01:54<00:00, 79.21it/s, loss=4.07, v_num=40, val_loss=4.080, avg_val_loss=4.080, train_loss=4.080]\n",
            "Epoch 9: 100% 9047/9047 [01:54<00:00, 79.24it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.080]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.074\n",
            "Epoch 10:  80% 7220/9047 [01:37<00:24, 74.06it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 7240/9047 [01:42<00:25, 70.60it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  80% 7260/9047 [01:42<00:25, 70.71it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  80% 7280/9047 [01:42<00:24, 70.81it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  81% 7300/9047 [01:42<00:24, 70.92it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  81% 7320/9047 [01:43<00:24, 71.02it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  81% 7340/9047 [01:43<00:23, 71.13it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  81% 7360/9047 [01:43<00:23, 71.23it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  82% 7380/9047 [01:43<00:23, 71.33it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  82% 7400/9047 [01:43<00:23, 71.43it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  82% 7420/9047 [01:43<00:22, 71.54it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  82% 7440/9047 [01:43<00:22, 71.64it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  82% 7460/9047 [01:43<00:22, 71.74it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  83% 7480/9047 [01:44<00:21, 71.85it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  83% 7500/9047 [01:44<00:21, 71.95it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  83% 7520/9047 [01:44<00:21, 72.05it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  83% 7540/9047 [01:44<00:20, 72.16it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  84% 7560/9047 [01:44<00:20, 72.26it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  84% 7580/9047 [01:44<00:20, 72.36it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  84% 7600/9047 [01:44<00:19, 72.46it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  84% 7620/9047 [01:45<00:19, 72.56it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  84% 7640/9047 [01:45<00:19, 72.66it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  85% 7660/9047 [01:45<00:19, 72.76it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  85% 7680/9047 [01:45<00:18, 72.86it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  85% 7700/9047 [01:45<00:18, 72.96it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  85% 7720/9047 [01:45<00:18, 73.07it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  86% 7740/9047 [01:45<00:17, 73.17it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  86% 7760/9047 [01:45<00:17, 73.27it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  86% 7780/9047 [01:46<00:17, 73.37it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  86% 7800/9047 [01:46<00:16, 73.47it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  86% 7820/9047 [01:46<00:16, 73.57it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  87% 7840/9047 [01:46<00:16, 73.67it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  87% 7860/9047 [01:46<00:16, 73.77it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  87% 7880/9047 [01:46<00:15, 73.87it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  87% 7900/9047 [01:46<00:15, 73.97it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  88% 7920/9047 [01:46<00:15, 74.07it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  88% 7940/9047 [01:47<00:14, 74.17it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  88% 7960/9047 [01:47<00:14, 74.27it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  88% 7980/9047 [01:47<00:14, 74.36it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  88% 8000/9047 [01:47<00:14, 74.45it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  89% 8020/9047 [01:47<00:13, 74.54it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  89% 8040/9047 [01:47<00:13, 74.64it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  89% 8060/9047 [01:47<00:13, 74.73it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  89% 8080/9047 [01:47<00:12, 74.83it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  90% 8100/9047 [01:48<00:12, 74.92it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  90% 8120/9047 [01:48<00:12, 75.01it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  90% 8140/9047 [01:48<00:12, 75.11it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  90% 8160/9047 [01:48<00:11, 75.21it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  90% 8180/9047 [01:48<00:11, 75.31it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  91% 8200/9047 [01:48<00:11, 75.40it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  91% 8220/9047 [01:48<00:10, 75.50it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  91% 8240/9047 [01:49<00:10, 75.59it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  91% 8260/9047 [01:49<00:10, 75.69it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  92% 8280/9047 [01:49<00:10, 75.79it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  92% 8300/9047 [01:49<00:09, 75.89it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  92% 8320/9047 [01:49<00:09, 75.99it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  92% 8340/9047 [01:49<00:09, 76.08it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  92% 8360/9047 [01:49<00:09, 76.18it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  93% 8380/9047 [01:49<00:08, 76.28it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  93% 8400/9047 [01:49<00:08, 76.38it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  93% 8420/9047 [01:50<00:08, 76.47it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  93% 8440/9047 [01:50<00:07, 76.57it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  94% 8460/9047 [01:50<00:07, 76.66it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  94% 8480/9047 [01:50<00:07, 76.76it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  94% 8500/9047 [01:50<00:07, 76.86it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  94% 8520/9047 [01:50<00:06, 76.95it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  94% 8540/9047 [01:50<00:06, 77.04it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  95% 8560/9047 [01:50<00:06, 77.14it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  95% 8580/9047 [01:51<00:06, 77.22it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  95% 8600/9047 [01:51<00:05, 77.30it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  95% 8620/9047 [01:51<00:05, 77.39it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  96% 8640/9047 [01:51<00:05, 77.48it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  96% 8660/9047 [01:51<00:04, 77.57it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  96% 8680/9047 [01:51<00:04, 77.67it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  96% 8700/9047 [01:51<00:04, 77.76it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  96% 8720/9047 [01:52<00:04, 77.85it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  97% 8740/9047 [01:52<00:03, 77.94it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  97% 8760/9047 [01:52<00:03, 78.03it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  97% 8780/9047 [01:52<00:03, 78.12it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  97% 8800/9047 [01:52<00:03, 78.22it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  97% 8820/9047 [01:52<00:02, 78.31it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  98% 8840/9047 [01:52<00:02, 78.39it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  98% 8860/9047 [01:52<00:02, 78.47it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  98% 8880/9047 [01:53<00:02, 78.56it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  98% 8900/9047 [01:53<00:01, 78.64it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  99% 8920/9047 [01:53<00:01, 78.73it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  99% 8940/9047 [01:53<00:01, 78.81it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  99% 8960/9047 [01:53<00:01, 78.89it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  99% 8980/9047 [01:53<00:00, 78.97it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10:  99% 9000/9047 [01:53<00:00, 79.05it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10: 100% 9020/9047 [01:53<00:00, 79.14it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10: 100% 9040/9047 [01:54<00:00, 79.23it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 10: 100% 9047/9047 [01:54<00:00, 79.25it/s, loss=4.07, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.071\n",
            "Epoch 11:  80% 7220/9047 [01:38<00:24, 73.43it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 7240/9047 [01:43<00:25, 70.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  80% 7260/9047 [01:43<00:25, 70.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  80% 7280/9047 [01:43<00:25, 70.33it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  81% 7300/9047 [01:43<00:24, 70.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  81% 7320/9047 [01:43<00:24, 70.54it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  81% 7340/9047 [01:43<00:24, 70.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  81% 7360/9047 [01:44<00:23, 70.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  82% 7380/9047 [01:44<00:23, 70.87it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  82% 7400/9047 [01:44<00:23, 70.97it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  82% 7420/9047 [01:44<00:22, 71.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  82% 7440/9047 [01:44<00:22, 71.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  82% 7460/9047 [01:44<00:22, 71.29it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  83% 7480/9047 [01:44<00:21, 71.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  83% 7500/9047 [01:44<00:21, 71.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  83% 7520/9047 [01:45<00:21, 71.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  83% 7540/9047 [01:45<00:21, 71.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  84% 7560/9047 [01:45<00:20, 71.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  84% 7580/9047 [01:45<00:20, 71.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  84% 7600/9047 [01:45<00:20, 71.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  84% 7620/9047 [01:45<00:19, 72.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  84% 7640/9047 [01:45<00:19, 72.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  85% 7660/9047 [01:45<00:19, 72.27it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  85% 7680/9047 [01:46<00:18, 72.37it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  85% 7700/9047 [01:46<00:18, 72.46it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  85% 7720/9047 [01:46<00:18, 72.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  86% 7740/9047 [01:46<00:17, 72.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  86% 7760/9047 [01:46<00:17, 72.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  86% 7780/9047 [01:46<00:17, 72.86it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  86% 7800/9047 [01:46<00:17, 72.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  86% 7820/9047 [01:47<00:16, 73.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  87% 7840/9047 [01:47<00:16, 73.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  87% 7860/9047 [01:47<00:16, 73.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  87% 7880/9047 [01:47<00:15, 73.34it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  87% 7900/9047 [01:47<00:15, 73.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  88% 7920/9047 [01:47<00:15, 73.53it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  88% 7940/9047 [01:47<00:15, 73.63it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  88% 7960/9047 [01:47<00:14, 73.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  88% 7980/9047 [01:48<00:14, 73.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  88% 8000/9047 [01:48<00:14, 73.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  89% 8020/9047 [01:48<00:13, 74.04it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  89% 8040/9047 [01:48<00:13, 74.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  89% 8060/9047 [01:48<00:13, 74.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  89% 8080/9047 [01:48<00:13, 74.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  90% 8100/9047 [01:48<00:12, 74.42it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  90% 8120/9047 [01:48<00:12, 74.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  90% 8140/9047 [01:49<00:12, 74.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  90% 8160/9047 [01:49<00:11, 74.68it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  90% 8180/9047 [01:49<00:11, 74.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  91% 8200/9047 [01:49<00:11, 74.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  91% 8220/9047 [01:49<00:11, 74.95it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  91% 8240/9047 [01:49<00:10, 75.04it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  91% 8260/9047 [01:49<00:10, 75.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  92% 8280/9047 [01:50<00:10, 75.20it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  92% 8300/9047 [01:50<00:09, 75.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  92% 8320/9047 [01:50<00:09, 75.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  92% 8340/9047 [01:50<00:09, 75.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  92% 8360/9047 [01:50<00:09, 75.57it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  93% 8380/9047 [01:50<00:08, 75.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  93% 8400/9047 [01:50<00:08, 75.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  93% 8420/9047 [01:51<00:08, 75.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  93% 8440/9047 [01:51<00:07, 75.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  94% 8460/9047 [01:51<00:07, 76.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  94% 8480/9047 [01:51<00:07, 76.11it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  94% 8500/9047 [01:51<00:07, 76.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  94% 8520/9047 [01:51<00:06, 76.28it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  94% 8540/9047 [01:51<00:06, 76.37it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  95% 8560/9047 [01:51<00:06, 76.46it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  95% 8580/9047 [01:52<00:06, 76.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  95% 8600/9047 [01:52<00:05, 76.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  95% 8620/9047 [01:52<00:05, 76.73it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  96% 8640/9047 [01:52<00:05, 76.82it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  96% 8660/9047 [01:52<00:05, 76.91it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  96% 8680/9047 [01:52<00:04, 77.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  96% 8700/9047 [01:52<00:04, 77.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  96% 8720/9047 [01:53<00:04, 77.16it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  97% 8740/9047 [01:53<00:03, 77.24it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  97% 8760/9047 [01:53<00:03, 77.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  97% 8780/9047 [01:53<00:03, 77.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  97% 8800/9047 [01:53<00:03, 77.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  97% 8820/9047 [01:53<00:02, 77.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  98% 8840/9047 [01:53<00:02, 77.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  98% 8860/9047 [01:53<00:02, 77.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  98% 8880/9047 [01:54<00:02, 77.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  98% 8900/9047 [01:54<00:01, 77.92it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  99% 8920/9047 [01:54<00:01, 78.01it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  99% 8940/9047 [01:54<00:01, 78.10it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  99% 8960/9047 [01:54<00:01, 78.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  99% 8980/9047 [01:54<00:00, 78.27it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11:  99% 9000/9047 [01:54<00:00, 78.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11: 100% 9020/9047 [01:54<00:00, 78.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11: 100% 9040/9047 [01:55<00:00, 78.53it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 11: 100% 9047/9047 [01:55<00:00, 78.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.069\n",
            "Epoch 12:  80% 7220/9047 [01:37<00:24, 74.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 7240/9047 [01:42<00:25, 70.78it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  80% 7260/9047 [01:42<00:25, 70.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  80% 7280/9047 [01:42<00:24, 70.99it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  81% 7300/9047 [01:42<00:24, 71.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  81% 7320/9047 [01:42<00:24, 71.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  81% 7340/9047 [01:42<00:23, 71.29it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  81% 7360/9047 [01:43<00:23, 71.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  82% 7380/9047 [01:43<00:23, 71.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  82% 7400/9047 [01:43<00:23, 71.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  82% 7420/9047 [01:43<00:22, 71.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  82% 7440/9047 [01:43<00:22, 71.81it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  82% 7460/9047 [01:43<00:22, 71.91it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  83% 7480/9047 [01:43<00:21, 72.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  83% 7500/9047 [01:43<00:21, 72.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  83% 7520/9047 [01:44<00:21, 72.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  83% 7540/9047 [01:44<00:20, 72.33it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  84% 7560/9047 [01:44<00:20, 72.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  84% 7580/9047 [01:44<00:20, 72.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  84% 7600/9047 [01:44<00:19, 72.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  84% 7620/9047 [01:44<00:19, 72.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  84% 7640/9047 [01:44<00:19, 72.86it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  85% 7660/9047 [01:44<00:19, 72.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  85% 7680/9047 [01:45<00:18, 73.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  85% 7700/9047 [01:45<00:18, 73.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  85% 7720/9047 [01:45<00:18, 73.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  86% 7740/9047 [01:45<00:17, 73.36it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  86% 7760/9047 [01:45<00:17, 73.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  86% 7780/9047 [01:45<00:17, 73.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  86% 7800/9047 [01:45<00:16, 73.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  86% 7820/9047 [01:46<00:16, 73.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  87% 7840/9047 [01:46<00:16, 73.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  87% 7860/9047 [01:46<00:16, 73.93it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  87% 7880/9047 [01:46<00:15, 74.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  87% 7900/9047 [01:46<00:15, 74.13it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  88% 7920/9047 [01:46<00:15, 74.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  88% 7940/9047 [01:46<00:14, 74.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  88% 7960/9047 [01:46<00:14, 74.42it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  88% 7980/9047 [01:47<00:14, 74.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  88% 8000/9047 [01:47<00:14, 74.59it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  89% 8020/9047 [01:47<00:13, 74.68it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  89% 8040/9047 [01:47<00:13, 74.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  89% 8060/9047 [01:47<00:13, 74.86it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  89% 8080/9047 [01:47<00:12, 74.95it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  90% 8100/9047 [01:47<00:12, 75.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  90% 8120/9047 [01:48<00:12, 75.13it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  90% 8140/9047 [01:48<00:12, 75.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  90% 8160/9047 [01:48<00:11, 75.33it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  90% 8180/9047 [01:48<00:11, 75.42it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  91% 8200/9047 [01:48<00:11, 75.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  91% 8220/9047 [01:48<00:10, 75.61it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  91% 8240/9047 [01:48<00:10, 75.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  91% 8260/9047 [01:48<00:10, 75.80it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  92% 8280/9047 [01:49<00:10, 75.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  92% 8300/9047 [01:49<00:09, 75.97it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  92% 8320/9047 [01:49<00:09, 76.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  92% 8340/9047 [01:49<00:09, 76.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  92% 8360/9047 [01:49<00:09, 76.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  93% 8380/9047 [01:49<00:08, 76.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  93% 8400/9047 [01:49<00:08, 76.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  93% 8420/9047 [01:50<00:08, 76.54it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  93% 8440/9047 [01:50<00:07, 76.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  94% 8460/9047 [01:50<00:07, 76.73it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  94% 8480/9047 [01:50<00:07, 76.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  94% 8500/9047 [01:50<00:07, 76.92it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  94% 8520/9047 [01:50<00:06, 77.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  94% 8540/9047 [01:50<00:06, 77.11it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  95% 8560/9047 [01:50<00:06, 77.21it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  95% 8580/9047 [01:50<00:06, 77.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  95% 8600/9047 [01:51<00:05, 77.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  95% 8620/9047 [01:51<00:05, 77.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  96% 8640/9047 [01:51<00:05, 77.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  96% 8660/9047 [01:51<00:04, 77.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  96% 8680/9047 [01:51<00:04, 77.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  96% 8700/9047 [01:51<00:04, 77.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  96% 8720/9047 [01:51<00:04, 77.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  97% 8740/9047 [01:52<00:03, 78.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  97% 8760/9047 [01:52<00:03, 78.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  97% 8780/9047 [01:52<00:03, 78.21it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  97% 8800/9047 [01:52<00:03, 78.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  97% 8820/9047 [01:52<00:02, 78.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  98% 8840/9047 [01:52<00:02, 78.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  98% 8860/9047 [01:52<00:02, 78.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  98% 8880/9047 [01:52<00:02, 78.68it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  98% 8900/9047 [01:52<00:01, 78.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  99% 8920/9047 [01:53<00:01, 78.86it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  99% 8940/9047 [01:53<00:01, 78.95it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  99% 8960/9047 [01:53<00:01, 79.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  99% 8980/9047 [01:53<00:00, 79.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12:  99% 9000/9047 [01:53<00:00, 79.21it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12: 100% 9020/9047 [01:53<00:00, 79.29it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12: 100% 9040/9047 [01:53<00:00, 79.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "Epoch 12: 100% 9047/9047 [01:53<00:00, 79.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.070]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.068\n",
            "Epoch 13:  80% 7220/9047 [01:36<00:24, 74.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  80% 7240/9047 [01:41<00:25, 71.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  80% 7260/9047 [01:41<00:25, 71.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  80% 7280/9047 [01:42<00:24, 71.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  81% 7300/9047 [01:42<00:24, 71.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  81% 7320/9047 [01:42<00:24, 71.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  81% 7340/9047 [01:42<00:23, 71.61it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  81% 7360/9047 [01:42<00:23, 71.72it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  82% 7380/9047 [01:42<00:23, 71.82it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  82% 7400/9047 [01:42<00:22, 71.93it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  82% 7420/9047 [01:43<00:22, 72.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  82% 7440/9047 [01:43<00:22, 72.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  82% 7460/9047 [01:43<00:21, 72.24it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  83% 7480/9047 [01:43<00:21, 72.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  83% 7500/9047 [01:43<00:21, 72.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  83% 7520/9047 [01:43<00:21, 72.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  83% 7540/9047 [01:43<00:20, 72.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  84% 7560/9047 [01:43<00:20, 72.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  84% 7580/9047 [01:44<00:20, 72.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  84% 7600/9047 [01:44<00:19, 72.95it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  84% 7620/9047 [01:44<00:19, 73.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  84% 7640/9047 [01:44<00:19, 73.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  85% 7660/9047 [01:44<00:18, 73.24it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  85% 7680/9047 [01:44<00:18, 73.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  85% 7700/9047 [01:44<00:18, 73.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  85% 7720/9047 [01:44<00:18, 73.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  86% 7740/9047 [01:45<00:17, 73.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  86% 7760/9047 [01:45<00:17, 73.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  86% 7780/9047 [01:45<00:17, 73.86it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  86% 7800/9047 [01:45<00:16, 73.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  86% 7820/9047 [01:45<00:16, 74.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  87% 7840/9047 [01:45<00:16, 74.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  87% 7860/9047 [01:45<00:15, 74.27it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  87% 7880/9047 [01:45<00:15, 74.37it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  87% 7900/9047 [01:46<00:15, 74.47it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  88% 7920/9047 [01:46<00:15, 74.57it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  88% 7940/9047 [01:46<00:14, 74.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  88% 7960/9047 [01:46<00:14, 74.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  88% 7980/9047 [01:46<00:14, 74.87it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  88% 8000/9047 [01:46<00:13, 74.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  89% 8020/9047 [01:46<00:13, 75.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  89% 8040/9047 [01:46<00:13, 75.16it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  89% 8060/9047 [01:47<00:13, 75.26it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  89% 8080/9047 [01:47<00:12, 75.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  90% 8100/9047 [01:47<00:12, 75.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  90% 8120/9047 [01:47<00:12, 75.54it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  90% 8140/9047 [01:47<00:11, 75.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  90% 8160/9047 [01:47<00:11, 75.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  90% 8180/9047 [01:47<00:11, 75.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  91% 8200/9047 [01:47<00:11, 75.93it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  91% 8220/9047 [01:48<00:10, 76.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  91% 8240/9047 [01:48<00:10, 76.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  91% 8260/9047 [01:48<00:10, 76.21it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  92% 8280/9047 [01:48<00:10, 76.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  92% 8300/9047 [01:48<00:09, 76.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  92% 8320/9047 [01:48<00:09, 76.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  92% 8340/9047 [01:48<00:09, 76.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  92% 8360/9047 [01:48<00:08, 76.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  93% 8380/9047 [01:49<00:08, 76.80it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  93% 8400/9047 [01:49<00:08, 76.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  93% 8420/9047 [01:49<00:08, 76.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  93% 8440/9047 [01:49<00:07, 77.07it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  94% 8460/9047 [01:49<00:07, 77.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  94% 8480/9047 [01:49<00:07, 77.27it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  94% 8500/9047 [01:49<00:07, 77.36it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  94% 8520/9047 [01:50<00:06, 77.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  94% 8540/9047 [01:50<00:06, 77.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  95% 8560/9047 [01:50<00:06, 77.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  95% 8580/9047 [01:50<00:06, 77.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  95% 8600/9047 [01:50<00:05, 77.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  95% 8620/9047 [01:50<00:05, 77.92it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  96% 8640/9047 [01:50<00:05, 78.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  96% 8660/9047 [01:50<00:04, 78.11it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  96% 8680/9047 [01:50<00:04, 78.20it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  96% 8700/9047 [01:51<00:04, 78.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  96% 8720/9047 [01:51<00:04, 78.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  97% 8740/9047 [01:51<00:03, 78.47it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  97% 8760/9047 [01:51<00:03, 78.56it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  97% 8780/9047 [01:51<00:03, 78.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  97% 8800/9047 [01:51<00:03, 78.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  97% 8820/9047 [01:51<00:02, 78.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  98% 8840/9047 [01:52<00:02, 78.92it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  98% 8860/9047 [01:52<00:02, 79.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  98% 8880/9047 [01:52<00:02, 79.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  98% 8900/9047 [01:52<00:01, 79.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  99% 8920/9047 [01:52<00:01, 79.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  99% 8940/9047 [01:52<00:01, 79.33it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  99% 8960/9047 [01:52<00:01, 79.42it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  99% 8980/9047 [01:52<00:00, 79.50it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13:  99% 9000/9047 [01:53<00:00, 79.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13: 100% 9020/9047 [01:53<00:00, 79.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13: 100% 9040/9047 [01:53<00:00, 79.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 13: 100% 9047/9047 [01:53<00:00, 79.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.067\n",
            "Epoch 14:  80% 7220/9047 [01:37<00:24, 74.24it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  80% 7240/9047 [01:42<00:25, 70.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  80% 7260/9047 [01:42<00:25, 70.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  80% 7280/9047 [01:42<00:24, 71.04it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  81% 7300/9047 [01:42<00:24, 71.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  81% 7320/9047 [01:42<00:24, 71.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  81% 7340/9047 [01:42<00:23, 71.36it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  81% 7360/9047 [01:42<00:23, 71.46it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  82% 7380/9047 [01:43<00:23, 71.56it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  82% 7400/9047 [01:43<00:22, 71.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  82% 7420/9047 [01:43<00:22, 71.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  82% 7440/9047 [01:43<00:22, 71.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  82% 7460/9047 [01:43<00:22, 71.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  83% 7480/9047 [01:43<00:21, 72.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  83% 7500/9047 [01:43<00:21, 72.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  83% 7520/9047 [01:44<00:21, 72.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  83% 7540/9047 [01:44<00:20, 72.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  84% 7560/9047 [01:44<00:20, 72.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  84% 7580/9047 [01:44<00:20, 72.59it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  84% 7600/9047 [01:44<00:19, 72.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  84% 7620/9047 [01:44<00:19, 72.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  84% 7640/9047 [01:44<00:19, 72.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  85% 7660/9047 [01:44<00:19, 72.99it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  85% 7680/9047 [01:45<00:18, 73.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  85% 7700/9047 [01:45<00:18, 73.20it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  85% 7720/9047 [01:45<00:18, 73.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  86% 7740/9047 [01:45<00:17, 73.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  86% 7760/9047 [01:45<00:17, 73.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  86% 7780/9047 [01:45<00:17, 73.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  86% 7800/9047 [01:45<00:16, 73.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  86% 7820/9047 [01:46<00:16, 73.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  87% 7840/9047 [01:46<00:16, 73.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  87% 7860/9047 [01:46<00:16, 73.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  87% 7880/9047 [01:46<00:15, 74.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  87% 7900/9047 [01:46<00:15, 74.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  88% 7920/9047 [01:46<00:15, 74.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  88% 7940/9047 [01:46<00:14, 74.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  88% 7960/9047 [01:46<00:14, 74.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  88% 7980/9047 [01:47<00:14, 74.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  88% 8000/9047 [01:47<00:14, 74.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  89% 8020/9047 [01:47<00:13, 74.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  89% 8040/9047 [01:47<00:13, 74.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  89% 8060/9047 [01:47<00:13, 74.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  89% 8080/9047 [01:47<00:12, 74.99it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  90% 8100/9047 [01:47<00:12, 75.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  90% 8120/9047 [01:47<00:12, 75.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  90% 8140/9047 [01:48<00:12, 75.29it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  90% 8160/9047 [01:48<00:11, 75.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  90% 8180/9047 [01:48<00:11, 75.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  91% 8200/9047 [01:48<00:11, 75.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  91% 8220/9047 [01:48<00:10, 75.68it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  91% 8240/9047 [01:48<00:10, 75.78it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  91% 8260/9047 [01:48<00:10, 75.87it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  92% 8280/9047 [01:49<00:10, 75.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  92% 8300/9047 [01:49<00:09, 76.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  92% 8320/9047 [01:49<00:09, 76.13it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  92% 8340/9047 [01:49<00:09, 76.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  92% 8360/9047 [01:49<00:09, 76.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  93% 8380/9047 [01:49<00:08, 76.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  93% 8400/9047 [01:49<00:08, 76.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  93% 8420/9047 [01:49<00:08, 76.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  93% 8440/9047 [01:50<00:07, 76.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  94% 8460/9047 [01:50<00:07, 76.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  94% 8480/9047 [01:50<00:07, 76.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  94% 8500/9047 [01:50<00:07, 76.93it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  94% 8520/9047 [01:50<00:06, 77.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  94% 8540/9047 [01:50<00:06, 77.11it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  95% 8560/9047 [01:50<00:06, 77.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  95% 8580/9047 [01:51<00:06, 77.28it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  95% 8600/9047 [01:51<00:05, 77.38it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  95% 8620/9047 [01:51<00:05, 77.47it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  96% 8640/9047 [01:51<00:05, 77.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  96% 8660/9047 [01:51<00:04, 77.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  96% 8680/9047 [01:51<00:04, 77.72it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  96% 8700/9047 [01:51<00:04, 77.81it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  96% 8720/9047 [01:51<00:04, 77.90it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  97% 8740/9047 [01:52<00:03, 77.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  97% 8760/9047 [01:52<00:03, 78.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  97% 8780/9047 [01:52<00:03, 78.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  97% 8800/9047 [01:52<00:03, 78.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  97% 8820/9047 [01:52<00:02, 78.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  98% 8840/9047 [01:52<00:02, 78.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  98% 8860/9047 [01:52<00:02, 78.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  98% 8880/9047 [01:53<00:02, 78.57it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  98% 8900/9047 [01:53<00:01, 78.66it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  99% 8920/9047 [01:53<00:01, 78.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  99% 8940/9047 [01:53<00:01, 78.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  99% 8960/9047 [01:53<00:01, 78.91it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  99% 8980/9047 [01:53<00:00, 79.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14:  99% 9000/9047 [01:53<00:00, 79.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14: 100% 9020/9047 [01:53<00:00, 79.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14: 100% 9040/9047 [01:54<00:00, 79.26it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 14: 100% 9047/9047 [01:54<00:00, 79.28it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  80% 7220/9047 [01:37<00:24, 73.97it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  80% 7240/9047 [01:42<00:25, 70.56it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  80% 7260/9047 [01:42<00:25, 70.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  80% 7280/9047 [01:42<00:24, 70.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  81% 7300/9047 [01:43<00:24, 70.87it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  81% 7320/9047 [01:43<00:24, 70.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  81% 7340/9047 [01:43<00:24, 71.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  81% 7360/9047 [01:43<00:23, 71.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  82% 7380/9047 [01:43<00:23, 71.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  82% 7400/9047 [01:43<00:23, 71.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  82% 7420/9047 [01:43<00:22, 71.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  82% 7440/9047 [01:43<00:22, 71.55it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  82% 7460/9047 [01:44<00:22, 71.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  83% 7480/9047 [01:44<00:21, 71.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  83% 7500/9047 [01:44<00:21, 71.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  83% 7520/9047 [01:44<00:21, 71.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  83% 7540/9047 [01:44<00:20, 72.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  84% 7560/9047 [01:44<00:20, 72.13it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  84% 7580/9047 [01:44<00:20, 72.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  84% 7600/9047 [01:45<00:20, 72.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  84% 7620/9047 [01:45<00:19, 72.43it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  84% 7640/9047 [01:45<00:19, 72.53it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  85% 7660/9047 [01:45<00:19, 72.63it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  85% 7680/9047 [01:45<00:18, 72.72it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  85% 7700/9047 [01:45<00:18, 72.82it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  85% 7720/9047 [01:45<00:18, 72.92it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  86% 7740/9047 [01:45<00:17, 73.02it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  86% 7760/9047 [01:46<00:17, 73.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  86% 7780/9047 [01:46<00:17, 73.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  86% 7800/9047 [01:46<00:17, 73.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  86% 7820/9047 [01:46<00:16, 73.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  87% 7840/9047 [01:46<00:16, 73.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  87% 7860/9047 [01:46<00:16, 73.61it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  87% 7880/9047 [01:46<00:15, 73.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  87% 7900/9047 [01:47<00:15, 73.80it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  88% 7920/9047 [01:47<00:15, 73.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  88% 7940/9047 [01:47<00:14, 73.99it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  88% 7960/9047 [01:47<00:14, 74.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  88% 7980/9047 [01:47<00:14, 74.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  88% 8000/9047 [01:47<00:14, 74.26it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  89% 8020/9047 [01:47<00:13, 74.36it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  89% 8040/9047 [01:47<00:13, 74.45it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  89% 8060/9047 [01:48<00:13, 74.54it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  89% 8080/9047 [01:48<00:12, 74.64it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  90% 8100/9047 [01:48<00:12, 74.73it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  90% 8120/9047 [01:48<00:12, 74.81it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  90% 8140/9047 [01:48<00:12, 74.91it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  90% 8160/9047 [01:48<00:11, 75.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  90% 8180/9047 [01:48<00:11, 75.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  91% 8200/9047 [01:49<00:11, 75.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  91% 8220/9047 [01:49<00:10, 75.28it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  91% 8240/9047 [01:49<00:10, 75.37it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  91% 8260/9047 [01:49<00:10, 75.47it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  92% 8280/9047 [01:49<00:10, 75.56it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  92% 8300/9047 [01:49<00:09, 75.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  92% 8320/9047 [01:49<00:09, 75.75it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  92% 8340/9047 [01:49<00:09, 75.84it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  92% 8360/9047 [01:50<00:09, 75.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  93% 8380/9047 [01:50<00:08, 76.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  93% 8400/9047 [01:50<00:08, 76.12it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  93% 8420/9047 [01:50<00:08, 76.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  93% 8440/9047 [01:50<00:07, 76.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  94% 8460/9047 [01:50<00:07, 76.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  94% 8480/9047 [01:50<00:07, 76.50it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  94% 8500/9047 [01:50<00:07, 76.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  94% 8520/9047 [01:51<00:06, 76.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  94% 8540/9047 [01:51<00:06, 76.78it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  95% 8560/9047 [01:51<00:06, 76.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  95% 8580/9047 [01:51<00:06, 76.97it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  95% 8600/9047 [01:51<00:05, 77.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  95% 8620/9047 [01:51<00:05, 77.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  96% 8640/9047 [01:51<00:05, 77.24it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  96% 8660/9047 [01:51<00:05, 77.34it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  96% 8680/9047 [01:52<00:04, 77.43it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  96% 8700/9047 [01:52<00:04, 77.51it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  96% 8720/9047 [01:52<00:04, 77.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  97% 8740/9047 [01:52<00:03, 77.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  97% 8760/9047 [01:52<00:03, 77.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  97% 8780/9047 [01:52<00:03, 77.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  97% 8800/9047 [01:52<00:03, 77.97it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  97% 8820/9047 [01:52<00:02, 78.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  98% 8840/9047 [01:53<00:02, 78.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  98% 8860/9047 [01:53<00:02, 78.23it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  98% 8880/9047 [01:53<00:02, 78.32it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  98% 8900/9047 [01:53<00:01, 78.41it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  99% 8920/9047 [01:53<00:01, 78.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  99% 8940/9047 [01:53<00:01, 78.58it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  99% 8960/9047 [01:53<00:01, 78.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  99% 8980/9047 [01:54<00:00, 78.76it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15:  99% 9000/9047 [01:54<00:00, 78.85it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15: 100% 9020/9047 [01:54<00:00, 78.94it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15: 100% 9040/9047 [01:54<00:00, 79.03it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 15: 100% 9047/9047 [01:54<00:00, 79.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.065\n",
            "Epoch 16:  80% 7220/9047 [01:37<00:24, 73.73it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  80% 7240/9047 [01:43<00:25, 70.16it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  80% 7260/9047 [01:43<00:25, 70.27it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  80% 7280/9047 [01:43<00:25, 70.38it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  81% 7300/9047 [01:43<00:24, 70.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  81% 7320/9047 [01:43<00:24, 70.59it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  81% 7340/9047 [01:43<00:24, 70.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  81% 7360/9047 [01:43<00:23, 70.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  82% 7380/9047 [01:44<00:23, 70.90it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  82% 7400/9047 [01:44<00:23, 71.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  82% 7420/9047 [01:44<00:22, 71.11it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  82% 7440/9047 [01:44<00:22, 71.21it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  82% 7460/9047 [01:44<00:22, 71.31it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  83% 7480/9047 [01:44<00:21, 71.42it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  83% 7500/9047 [01:44<00:21, 71.52it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  83% 7520/9047 [01:44<00:21, 71.63it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  83% 7540/9047 [01:45<00:21, 71.73it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  84% 7560/9047 [01:45<00:20, 71.83it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  84% 7580/9047 [01:45<00:20, 71.93it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  84% 7600/9047 [01:45<00:20, 72.04it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  84% 7620/9047 [01:45<00:19, 72.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  84% 7640/9047 [01:45<00:19, 72.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  85% 7660/9047 [01:45<00:19, 72.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  85% 7680/9047 [01:45<00:18, 72.46it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  85% 7700/9047 [01:46<00:18, 72.56it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  85% 7720/9047 [01:46<00:18, 72.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  86% 7740/9047 [01:46<00:17, 72.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  86% 7760/9047 [01:46<00:17, 72.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  86% 7780/9047 [01:46<00:17, 72.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  86% 7800/9047 [01:46<00:17, 73.08it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  86% 7820/9047 [01:46<00:16, 73.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  87% 7840/9047 [01:46<00:16, 73.29it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  87% 7860/9047 [01:47<00:16, 73.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  87% 7880/9047 [01:47<00:15, 73.49it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  87% 7900/9047 [01:47<00:15, 73.59it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  88% 7920/9047 [01:47<00:15, 73.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  88% 7940/9047 [01:47<00:15, 73.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  88% 7960/9047 [01:47<00:14, 73.90it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  88% 7980/9047 [01:47<00:14, 74.00it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  88% 8000/9047 [01:47<00:14, 74.10it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  89% 8020/9047 [01:48<00:13, 74.20it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  89% 8040/9047 [01:48<00:13, 74.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  89% 8060/9047 [01:48<00:13, 74.40it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  89% 8080/9047 [01:48<00:12, 74.50it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  90% 8100/9047 [01:48<00:12, 74.60it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  90% 8120/9047 [01:48<00:12, 74.69it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  90% 8140/9047 [01:48<00:12, 74.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  90% 8160/9047 [01:48<00:11, 74.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  90% 8180/9047 [01:49<00:11, 74.99it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  91% 8200/9047 [01:49<00:11, 75.09it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  91% 8220/9047 [01:49<00:10, 75.19it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  91% 8240/9047 [01:49<00:10, 75.28it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  91% 8260/9047 [01:49<00:10, 75.38it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  92% 8280/9047 [01:49<00:10, 75.47it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  92% 8300/9047 [01:49<00:09, 75.57it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  92% 8320/9047 [01:49<00:09, 75.67it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  92% 8340/9047 [01:50<00:09, 75.77it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  92% 8360/9047 [01:50<00:09, 75.87it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  93% 8380/9047 [01:50<00:08, 75.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  93% 8400/9047 [01:50<00:08, 76.06it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  93% 8420/9047 [01:50<00:08, 76.15it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  93% 8440/9047 [01:50<00:07, 76.25it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  94% 8460/9047 [01:50<00:07, 76.34it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  94% 8480/9047 [01:50<00:07, 76.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  94% 8500/9047 [01:51<00:07, 76.53it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  94% 8520/9047 [01:51<00:06, 76.62it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  94% 8540/9047 [01:51<00:06, 76.71it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  95% 8560/9047 [01:51<00:06, 76.80it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  95% 8580/9047 [01:51<00:06, 76.89it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  95% 8600/9047 [01:51<00:05, 76.98it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  95% 8620/9047 [01:51<00:05, 77.07it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  96% 8640/9047 [01:51<00:05, 77.17it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  96% 8660/9047 [01:52<00:05, 77.26it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  96% 8680/9047 [01:52<00:04, 77.35it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  96% 8700/9047 [01:52<00:04, 77.44it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  96% 8720/9047 [01:52<00:04, 77.53it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  97% 8740/9047 [01:52<00:03, 77.62it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  97% 8760/9047 [01:52<00:03, 77.70it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  97% 8780/9047 [01:52<00:03, 77.79it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  97% 8800/9047 [01:52<00:03, 77.88it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  97% 8820/9047 [01:53<00:02, 77.96it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  98% 8840/9047 [01:53<00:02, 78.05it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  98% 8860/9047 [01:53<00:02, 78.14it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  98% 8880/9047 [01:53<00:02, 78.22it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  98% 8900/9047 [01:53<00:01, 78.30it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  99% 8920/9047 [01:53<00:01, 78.39it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  99% 8940/9047 [01:53<00:01, 78.48it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  99% 8960/9047 [01:54<00:01, 78.57it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  99% 8980/9047 [01:54<00:00, 78.65it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16:  99% 9000/9047 [01:54<00:00, 78.74it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16: 100% 9020/9047 [01:54<00:00, 78.82it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16: 100% 9040/9047 [01:54<00:00, 78.91it/s, loss=4.06, v_num=40, val_loss=4.070, avg_val_loss=4.070, train_loss=4.060]\n",
            "Epoch 16: 100% 9047/9047 [01:54<00:00, 78.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  80% 7220/9047 [01:37<00:24, 73.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  80% 7240/9047 [01:42<00:25, 70.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  80% 7260/9047 [01:42<00:25, 70.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  80% 7280/9047 [01:42<00:24, 70.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  81% 7300/9047 [01:43<00:24, 70.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  81% 7320/9047 [01:43<00:24, 70.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  81% 7340/9047 [01:43<00:24, 71.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  81% 7360/9047 [01:43<00:23, 71.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  82% 7380/9047 [01:43<00:23, 71.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  82% 7400/9047 [01:43<00:23, 71.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  82% 7420/9047 [01:43<00:22, 71.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  82% 7440/9047 [01:43<00:22, 71.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  82% 7460/9047 [01:44<00:22, 71.64it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  83% 7480/9047 [01:44<00:21, 71.74it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  83% 7500/9047 [01:44<00:21, 71.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  83% 7520/9047 [01:44<00:21, 71.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  83% 7540/9047 [01:44<00:20, 72.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  84% 7560/9047 [01:44<00:20, 72.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  84% 7580/9047 [01:44<00:20, 72.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  84% 7600/9047 [01:45<00:20, 72.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  84% 7620/9047 [01:45<00:19, 72.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  84% 7640/9047 [01:45<00:19, 72.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  85% 7660/9047 [01:45<00:19, 72.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  85% 7680/9047 [01:45<00:18, 72.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  85% 7700/9047 [01:45<00:18, 72.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  85% 7720/9047 [01:45<00:18, 72.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  86% 7740/9047 [01:46<00:17, 72.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  86% 7760/9047 [01:46<00:17, 73.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  86% 7780/9047 [01:46<00:17, 73.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  86% 7800/9047 [01:46<00:17, 73.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  86% 7820/9047 [01:46<00:16, 73.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  87% 7840/9047 [01:46<00:16, 73.47it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  87% 7860/9047 [01:46<00:16, 73.56it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  87% 7880/9047 [01:46<00:15, 73.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  87% 7900/9047 [01:47<00:15, 73.74it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  88% 7920/9047 [01:47<00:15, 73.83it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  88% 7940/9047 [01:47<00:14, 73.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  88% 7960/9047 [01:47<00:14, 74.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  88% 7980/9047 [01:47<00:14, 74.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  88% 8000/9047 [01:47<00:14, 74.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  89% 8020/9047 [01:47<00:13, 74.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  89% 8040/9047 [01:48<00:13, 74.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  89% 8060/9047 [01:48<00:13, 74.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  89% 8080/9047 [01:48<00:12, 74.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  90% 8100/9047 [01:48<00:12, 74.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  90% 8120/9047 [01:48<00:12, 74.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  90% 8140/9047 [01:48<00:12, 74.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  90% 8160/9047 [01:48<00:11, 74.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  90% 8180/9047 [01:49<00:11, 75.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  91% 8200/9047 [01:49<00:11, 75.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  91% 8220/9047 [01:49<00:10, 75.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  91% 8240/9047 [01:49<00:10, 75.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  91% 8260/9047 [01:49<00:10, 75.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  92% 8280/9047 [01:49<00:10, 75.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  92% 8300/9047 [01:49<00:09, 75.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  92% 8320/9047 [01:50<00:09, 75.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  92% 8340/9047 [01:50<00:09, 75.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  92% 8360/9047 [01:50<00:09, 75.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  93% 8380/9047 [01:50<00:08, 75.88it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  93% 8400/9047 [01:50<00:08, 75.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  93% 8420/9047 [01:50<00:08, 76.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  93% 8440/9047 [01:50<00:07, 76.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  94% 8460/9047 [01:50<00:07, 76.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  94% 8480/9047 [01:51<00:07, 76.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  94% 8500/9047 [01:51<00:07, 76.45it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  94% 8520/9047 [01:51<00:06, 76.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  94% 8540/9047 [01:51<00:06, 76.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  95% 8560/9047 [01:51<00:06, 76.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  95% 8580/9047 [01:51<00:06, 76.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  95% 8600/9047 [01:51<00:05, 76.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  95% 8620/9047 [01:51<00:05, 76.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  96% 8640/9047 [01:52<00:05, 77.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  96% 8660/9047 [01:52<00:05, 77.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  96% 8680/9047 [01:52<00:04, 77.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  96% 8700/9047 [01:52<00:04, 77.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  96% 8720/9047 [01:52<00:04, 77.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  97% 8740/9047 [01:52<00:03, 77.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  97% 8760/9047 [01:52<00:03, 77.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  97% 8780/9047 [01:53<00:03, 77.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  97% 8800/9047 [01:53<00:03, 77.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  97% 8820/9047 [01:53<00:02, 77.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  98% 8840/9047 [01:53<00:02, 77.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  98% 8860/9047 [01:53<00:02, 78.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  98% 8880/9047 [01:53<00:02, 78.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  98% 8900/9047 [01:53<00:01, 78.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  99% 8920/9047 [01:53<00:01, 78.27it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  99% 8940/9047 [01:54<00:01, 78.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  99% 8960/9047 [01:54<00:01, 78.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  99% 8980/9047 [01:54<00:00, 78.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17:  99% 9000/9047 [01:54<00:00, 78.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17: 100% 9020/9047 [01:54<00:00, 78.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17: 100% 9040/9047 [01:54<00:00, 78.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 17: 100% 9047/9047 [01:54<00:00, 78.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.064\n",
            "Epoch 18:  80% 7220/9047 [01:37<00:24, 73.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  80% 7240/9047 [01:42<00:25, 70.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  80% 7260/9047 [01:42<00:25, 70.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  80% 7280/9047 [01:43<00:25, 70.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  81% 7300/9047 [01:43<00:24, 70.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  81% 7320/9047 [01:43<00:24, 70.83it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  81% 7340/9047 [01:43<00:24, 70.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  81% 7360/9047 [01:43<00:23, 71.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  82% 7380/9047 [01:43<00:23, 71.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  82% 7400/9047 [01:43<00:23, 71.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  82% 7420/9047 [01:43<00:22, 71.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  82% 7440/9047 [01:44<00:22, 71.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  82% 7460/9047 [01:44<00:22, 71.57it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  83% 7480/9047 [01:44<00:21, 71.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  83% 7500/9047 [01:44<00:21, 71.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  83% 7520/9047 [01:44<00:21, 71.88it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  83% 7540/9047 [01:44<00:20, 71.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  84% 7560/9047 [01:44<00:20, 72.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  84% 7580/9047 [01:44<00:20, 72.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  84% 7600/9047 [01:45<00:20, 72.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  84% 7620/9047 [01:45<00:19, 72.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  84% 7640/9047 [01:45<00:19, 72.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  85% 7660/9047 [01:45<00:19, 72.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  85% 7680/9047 [01:45<00:18, 72.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  85% 7700/9047 [01:45<00:18, 72.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  85% 7720/9047 [01:45<00:18, 72.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  86% 7740/9047 [01:46<00:17, 72.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  86% 7760/9047 [01:46<00:17, 73.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  86% 7780/9047 [01:46<00:17, 73.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  86% 7800/9047 [01:46<00:17, 73.27it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  86% 7820/9047 [01:46<00:16, 73.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  87% 7840/9047 [01:46<00:16, 73.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  87% 7860/9047 [01:46<00:16, 73.56it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  87% 7880/9047 [01:46<00:15, 73.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  87% 7900/9047 [01:47<00:15, 73.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  88% 7920/9047 [01:47<00:15, 73.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  88% 7940/9047 [01:47<00:14, 73.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  88% 7960/9047 [01:47<00:14, 74.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  88% 7980/9047 [01:47<00:14, 74.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  88% 8000/9047 [01:47<00:14, 74.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  89% 8020/9047 [01:47<00:13, 74.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  89% 8040/9047 [01:48<00:13, 74.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  89% 8060/9047 [01:48<00:13, 74.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  89% 8080/9047 [01:48<00:12, 74.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  90% 8100/9047 [01:48<00:12, 74.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  90% 8120/9047 [01:48<00:12, 74.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  90% 8140/9047 [01:48<00:12, 74.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  90% 8160/9047 [01:48<00:11, 74.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  90% 8180/9047 [01:48<00:11, 75.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  91% 8200/9047 [01:49<00:11, 75.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  91% 8220/9047 [01:49<00:10, 75.27it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  91% 8240/9047 [01:49<00:10, 75.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  91% 8260/9047 [01:49<00:10, 75.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  92% 8280/9047 [01:49<00:10, 75.56it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  92% 8300/9047 [01:49<00:09, 75.66it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  92% 8320/9047 [01:49<00:09, 75.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  92% 8340/9047 [01:49<00:09, 75.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  92% 8360/9047 [01:50<00:09, 75.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  93% 8380/9047 [01:50<00:08, 76.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  93% 8400/9047 [01:50<00:08, 76.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  93% 8420/9047 [01:50<00:08, 76.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  93% 8440/9047 [01:50<00:07, 76.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  94% 8460/9047 [01:50<00:07, 76.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  94% 8480/9047 [01:50<00:07, 76.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  94% 8500/9047 [01:51<00:07, 76.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  94% 8520/9047 [01:51<00:06, 76.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  94% 8540/9047 [01:51<00:06, 76.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  95% 8560/9047 [01:51<00:06, 76.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  95% 8580/9047 [01:51<00:06, 76.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  95% 8600/9047 [01:51<00:05, 76.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  95% 8620/9047 [01:51<00:05, 77.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  96% 8640/9047 [01:51<00:05, 77.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  96% 8660/9047 [01:52<00:05, 77.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  96% 8680/9047 [01:52<00:04, 77.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  96% 8700/9047 [01:52<00:04, 77.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  96% 8720/9047 [01:52<00:04, 77.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  97% 8740/9047 [01:52<00:03, 77.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  97% 8760/9047 [01:52<00:03, 77.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  97% 8780/9047 [01:52<00:03, 77.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  97% 8800/9047 [01:53<00:03, 77.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  97% 8820/9047 [01:53<00:02, 77.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  98% 8840/9047 [01:53<00:02, 78.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  98% 8860/9047 [01:53<00:02, 78.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  98% 8880/9047 [01:53<00:02, 78.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  98% 8900/9047 [01:53<00:01, 78.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  99% 8920/9047 [01:53<00:01, 78.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  99% 8940/9047 [01:53<00:01, 78.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  99% 8960/9047 [01:54<00:01, 78.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  99% 8980/9047 [01:54<00:00, 78.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18:  99% 9000/9047 [01:54<00:00, 78.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18: 100% 9020/9047 [01:54<00:00, 78.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18: 100% 9040/9047 [01:54<00:00, 78.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 18: 100% 9047/9047 [01:54<00:00, 78.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  80% 7220/9047 [01:37<00:24, 73.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  80% 7240/9047 [01:43<00:25, 70.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  80% 7260/9047 [01:43<00:25, 70.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  80% 7280/9047 [01:43<00:25, 70.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  81% 7300/9047 [01:43<00:24, 70.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  81% 7320/9047 [01:43<00:24, 70.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  81% 7340/9047 [01:43<00:24, 70.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  81% 7360/9047 [01:43<00:23, 70.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  82% 7380/9047 [01:44<00:23, 70.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  82% 7400/9047 [01:44<00:23, 71.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  82% 7420/9047 [01:44<00:22, 71.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  82% 7440/9047 [01:44<00:22, 71.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  82% 7460/9047 [01:44<00:22, 71.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  83% 7480/9047 [01:44<00:21, 71.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  83% 7500/9047 [01:44<00:21, 71.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  83% 7520/9047 [01:44<00:21, 71.64it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  83% 7540/9047 [01:45<00:21, 71.74it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  84% 7560/9047 [01:45<00:20, 71.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  84% 7580/9047 [01:45<00:20, 71.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  84% 7600/9047 [01:45<00:20, 72.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  84% 7620/9047 [01:45<00:19, 72.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  84% 7640/9047 [01:45<00:19, 72.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  85% 7660/9047 [01:45<00:19, 72.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  85% 7680/9047 [01:46<00:18, 72.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  85% 7700/9047 [01:46<00:18, 72.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  85% 7720/9047 [01:46<00:18, 72.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  86% 7740/9047 [01:46<00:17, 72.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  86% 7760/9047 [01:46<00:17, 72.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  86% 7780/9047 [01:46<00:17, 72.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  86% 7800/9047 [01:46<00:17, 73.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  86% 7820/9047 [01:46<00:16, 73.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  87% 7840/9047 [01:47<00:16, 73.20it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  87% 7860/9047 [01:47<00:16, 73.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  87% 7880/9047 [01:47<00:15, 73.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  87% 7900/9047 [01:47<00:15, 73.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  88% 7920/9047 [01:47<00:15, 73.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  88% 7940/9047 [01:47<00:15, 73.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  88% 7960/9047 [01:47<00:14, 73.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  88% 7980/9047 [01:48<00:14, 73.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  88% 8000/9047 [01:48<00:14, 73.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  89% 8020/9047 [01:48<00:13, 74.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  89% 8040/9047 [01:48<00:13, 74.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  89% 8060/9047 [01:48<00:13, 74.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  89% 8080/9047 [01:48<00:13, 74.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  90% 8100/9047 [01:48<00:12, 74.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  90% 8120/9047 [01:48<00:12, 74.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  90% 8140/9047 [01:49<00:12, 74.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  90% 8160/9047 [01:49<00:11, 74.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  90% 8180/9047 [01:49<00:11, 74.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  91% 8200/9047 [01:49<00:11, 74.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  91% 8220/9047 [01:49<00:11, 74.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  91% 8240/9047 [01:49<00:10, 75.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  91% 8260/9047 [01:49<00:10, 75.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  92% 8280/9047 [01:50<00:10, 75.27it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  92% 8300/9047 [01:50<00:09, 75.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  92% 8320/9047 [01:50<00:09, 75.45it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  92% 8340/9047 [01:50<00:09, 75.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  92% 8360/9047 [01:50<00:09, 75.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  93% 8380/9047 [01:50<00:08, 75.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  93% 8400/9047 [01:50<00:08, 75.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  93% 8420/9047 [01:50<00:08, 75.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  93% 8440/9047 [01:51<00:07, 75.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  94% 8460/9047 [01:51<00:07, 76.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  94% 8480/9047 [01:51<00:07, 76.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  94% 8500/9047 [01:51<00:07, 76.27it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  94% 8520/9047 [01:51<00:06, 76.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  94% 8540/9047 [01:51<00:06, 76.45it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  95% 8560/9047 [01:51<00:06, 76.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  95% 8580/9047 [01:51<00:06, 76.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  95% 8600/9047 [01:52<00:05, 76.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  95% 8620/9047 [01:52<00:05, 76.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  96% 8640/9047 [01:52<00:05, 76.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  96% 8660/9047 [01:52<00:05, 76.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  96% 8680/9047 [01:52<00:04, 77.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  96% 8700/9047 [01:52<00:04, 77.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  96% 8720/9047 [01:52<00:04, 77.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  97% 8740/9047 [01:53<00:03, 77.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  97% 8760/9047 [01:53<00:03, 77.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  97% 8780/9047 [01:53<00:03, 77.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  97% 8800/9047 [01:53<00:03, 77.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  97% 8820/9047 [01:53<00:02, 77.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  98% 8840/9047 [01:53<00:02, 77.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  98% 8860/9047 [01:53<00:02, 77.88it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  98% 8880/9047 [01:53<00:02, 77.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  98% 8900/9047 [01:54<00:01, 77.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  99% 8920/9047 [01:54<00:01, 78.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  99% 8940/9047 [01:54<00:01, 78.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  99% 8960/9047 [01:54<00:01, 78.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  99% 8980/9047 [01:54<00:00, 78.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19:  99% 9000/9047 [01:54<00:00, 78.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19: 100% 9020/9047 [01:54<00:00, 78.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19: 100% 9040/9047 [01:55<00:00, 78.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 19: 100% 9047/9047 [01:55<00:00, 78.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  80% 7220/9047 [01:37<00:24, 74.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  80% 7240/9047 [01:42<00:25, 70.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  80% 7260/9047 [01:42<00:25, 70.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  80% 7280/9047 [01:42<00:24, 70.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  81% 7300/9047 [01:42<00:24, 70.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  81% 7320/9047 [01:43<00:24, 71.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  81% 7340/9047 [01:43<00:24, 71.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  81% 7360/9047 [01:43<00:23, 71.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  82% 7380/9047 [01:43<00:23, 71.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  82% 7400/9047 [01:43<00:23, 71.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  82% 7420/9047 [01:43<00:22, 71.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  82% 7440/9047 [01:43<00:22, 71.66it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  82% 7460/9047 [01:43<00:22, 71.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  83% 7480/9047 [01:44<00:21, 71.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  83% 7500/9047 [01:44<00:21, 71.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  83% 7520/9047 [01:44<00:21, 72.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  83% 7540/9047 [01:44<00:20, 72.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  84% 7560/9047 [01:44<00:20, 72.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  84% 7580/9047 [01:44<00:20, 72.39it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  84% 7600/9047 [01:44<00:19, 72.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  84% 7620/9047 [01:44<00:19, 72.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  84% 7640/9047 [01:45<00:19, 72.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  85% 7660/9047 [01:45<00:19, 72.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  85% 7680/9047 [01:45<00:18, 72.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  85% 7700/9047 [01:45<00:18, 73.00it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  85% 7720/9047 [01:45<00:18, 73.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  86% 7740/9047 [01:45<00:17, 73.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  86% 7760/9047 [01:45<00:17, 73.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  86% 7780/9047 [01:45<00:17, 73.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  86% 7800/9047 [01:46<00:16, 73.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  86% 7820/9047 [01:46<00:16, 73.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  87% 7840/9047 [01:46<00:16, 73.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  87% 7860/9047 [01:46<00:16, 73.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  87% 7880/9047 [01:46<00:15, 73.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  87% 7900/9047 [01:46<00:15, 73.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  88% 7920/9047 [01:46<00:15, 74.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  88% 7940/9047 [01:47<00:14, 74.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  88% 7960/9047 [01:47<00:14, 74.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  88% 7980/9047 [01:47<00:14, 74.39it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  88% 8000/9047 [01:47<00:14, 74.48it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  89% 8020/9047 [01:47<00:13, 74.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  89% 8040/9047 [01:47<00:13, 74.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  89% 8060/9047 [01:47<00:13, 74.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  89% 8080/9047 [01:47<00:12, 74.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  90% 8100/9047 [01:48<00:12, 74.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  90% 8120/9047 [01:48<00:12, 75.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  90% 8140/9047 [01:48<00:12, 75.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  90% 8160/9047 [01:48<00:11, 75.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  90% 8180/9047 [01:48<00:11, 75.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  91% 8200/9047 [01:48<00:11, 75.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  91% 8220/9047 [01:48<00:10, 75.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  91% 8240/9047 [01:48<00:10, 75.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  91% 8260/9047 [01:49<00:10, 75.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  92% 8280/9047 [01:49<00:10, 75.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  92% 8300/9047 [01:49<00:09, 75.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  92% 8320/9047 [01:49<00:09, 75.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  92% 8340/9047 [01:49<00:09, 76.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  92% 8360/9047 [01:49<00:09, 76.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  93% 8380/9047 [01:49<00:08, 76.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  93% 8400/9047 [01:50<00:08, 76.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  93% 8420/9047 [01:50<00:08, 76.39it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  93% 8440/9047 [01:50<00:07, 76.48it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  94% 8460/9047 [01:50<00:07, 76.57it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  94% 8480/9047 [01:50<00:07, 76.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  94% 8500/9047 [01:50<00:07, 76.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  94% 8520/9047 [01:50<00:06, 76.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  94% 8540/9047 [01:50<00:06, 76.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  95% 8560/9047 [01:51<00:06, 77.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  95% 8580/9047 [01:51<00:06, 77.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  95% 8600/9047 [01:51<00:05, 77.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  95% 8620/9047 [01:51<00:05, 77.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  96% 8640/9047 [01:51<00:05, 77.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  96% 8660/9047 [01:51<00:04, 77.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  96% 8680/9047 [01:51<00:04, 77.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  96% 8700/9047 [01:51<00:04, 77.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  96% 8720/9047 [01:52<00:04, 77.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  97% 8740/9047 [01:52<00:03, 77.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  97% 8760/9047 [01:52<00:03, 77.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  97% 8780/9047 [01:52<00:03, 78.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  97% 8800/9047 [01:52<00:03, 78.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  97% 8820/9047 [01:52<00:02, 78.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  98% 8840/9047 [01:52<00:02, 78.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  98% 8860/9047 [01:53<00:02, 78.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  98% 8880/9047 [01:53<00:02, 78.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  98% 8900/9047 [01:53<00:01, 78.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  99% 8920/9047 [01:53<00:01, 78.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  99% 8940/9047 [01:53<00:01, 78.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  99% 8960/9047 [01:53<00:01, 78.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  99% 8980/9047 [01:53<00:00, 78.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20:  99% 9000/9047 [01:53<00:00, 78.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20: 100% 9020/9047 [01:54<00:00, 79.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20: 100% 9040/9047 [01:54<00:00, 79.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 20: 100% 9047/9047 [01:54<00:00, 79.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.063\n",
            "Epoch 21:  80% 7220/9047 [01:37<00:24, 74.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  80% 7240/9047 [01:42<00:25, 70.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  80% 7260/9047 [01:42<00:25, 70.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  80% 7280/9047 [01:42<00:24, 71.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  81% 7300/9047 [01:42<00:24, 71.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  81% 7320/9047 [01:42<00:24, 71.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  81% 7340/9047 [01:42<00:23, 71.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  81% 7360/9047 [01:42<00:23, 71.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  82% 7380/9047 [01:43<00:23, 71.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  82% 7400/9047 [01:43<00:22, 71.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  82% 7420/9047 [01:43<00:22, 71.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  82% 7440/9047 [01:43<00:22, 71.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  82% 7460/9047 [01:43<00:22, 72.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  83% 7480/9047 [01:43<00:21, 72.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  83% 7500/9047 [01:43<00:21, 72.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  83% 7520/9047 [01:43<00:21, 72.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  83% 7540/9047 [01:44<00:20, 72.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  84% 7560/9047 [01:44<00:20, 72.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  84% 7580/9047 [01:44<00:20, 72.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  84% 7600/9047 [01:44<00:19, 72.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  84% 7620/9047 [01:44<00:19, 72.83it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  84% 7640/9047 [01:44<00:19, 72.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  85% 7660/9047 [01:44<00:18, 73.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  85% 7680/9047 [01:45<00:18, 73.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  85% 7700/9047 [01:45<00:18, 73.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  85% 7720/9047 [01:45<00:18, 73.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  86% 7740/9047 [01:45<00:17, 73.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  86% 7760/9047 [01:45<00:17, 73.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  86% 7780/9047 [01:45<00:17, 73.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  86% 7800/9047 [01:45<00:16, 73.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  86% 7820/9047 [01:45<00:16, 73.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  87% 7840/9047 [01:46<00:16, 73.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  87% 7860/9047 [01:46<00:16, 74.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  87% 7880/9047 [01:46<00:15, 74.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  87% 7900/9047 [01:46<00:15, 74.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  88% 7920/9047 [01:46<00:15, 74.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  88% 7940/9047 [01:46<00:14, 74.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  88% 7960/9047 [01:46<00:14, 74.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  88% 7980/9047 [01:46<00:14, 74.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  88% 8000/9047 [01:47<00:14, 74.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  89% 8020/9047 [01:47<00:13, 74.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  89% 8040/9047 [01:47<00:13, 74.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  89% 8060/9047 [01:47<00:13, 74.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  89% 8080/9047 [01:47<00:12, 75.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  90% 8100/9047 [01:47<00:12, 75.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  90% 8120/9047 [01:47<00:12, 75.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  90% 8140/9047 [01:48<00:12, 75.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  90% 8160/9047 [01:48<00:11, 75.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  90% 8180/9047 [01:48<00:11, 75.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  91% 8200/9047 [01:48<00:11, 75.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  91% 8220/9047 [01:48<00:10, 75.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  91% 8240/9047 [01:48<00:10, 75.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  91% 8260/9047 [01:48<00:10, 75.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  92% 8280/9047 [01:49<00:10, 75.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  92% 8300/9047 [01:49<00:09, 76.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  92% 8320/9047 [01:49<00:09, 76.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  92% 8340/9047 [01:49<00:09, 76.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  92% 8360/9047 [01:49<00:08, 76.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  93% 8380/9047 [01:49<00:08, 76.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  93% 8400/9047 [01:49<00:08, 76.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  93% 8420/9047 [01:49<00:08, 76.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  93% 8440/9047 [01:50<00:07, 76.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  94% 8460/9047 [01:50<00:07, 76.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  94% 8480/9047 [01:50<00:07, 76.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  94% 8500/9047 [01:50<00:07, 77.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  94% 8520/9047 [01:50<00:06, 77.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  94% 8540/9047 [01:50<00:06, 77.20it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  95% 8560/9047 [01:50<00:06, 77.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  95% 8580/9047 [01:50<00:06, 77.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  95% 8600/9047 [01:51<00:05, 77.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  95% 8620/9047 [01:51<00:05, 77.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  96% 8640/9047 [01:51<00:05, 77.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  96% 8660/9047 [01:51<00:04, 77.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  96% 8680/9047 [01:51<00:04, 77.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  96% 8700/9047 [01:51<00:04, 77.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  96% 8720/9047 [01:51<00:04, 77.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  97% 8740/9047 [01:51<00:03, 78.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  97% 8760/9047 [01:52<00:03, 78.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  97% 8780/9047 [01:52<00:03, 78.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  97% 8800/9047 [01:52<00:03, 78.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  97% 8820/9047 [01:52<00:02, 78.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  98% 8840/9047 [01:52<00:02, 78.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  98% 8860/9047 [01:52<00:02, 78.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  98% 8880/9047 [01:52<00:02, 78.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  98% 8900/9047 [01:52<00:01, 78.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  99% 8920/9047 [01:53<00:01, 78.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  99% 8940/9047 [01:53<00:01, 78.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  99% 8960/9047 [01:53<00:01, 79.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  99% 8980/9047 [01:53<00:00, 79.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21:  99% 9000/9047 [01:53<00:00, 79.20it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21: 100% 9020/9047 [01:53<00:00, 79.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21: 100% 9040/9047 [01:53<00:00, 79.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 21: 100% 9047/9047 [01:53<00:00, 79.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  80% 7220/9047 [01:37<00:24, 73.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  80% 7240/9047 [01:42<00:25, 70.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  80% 7260/9047 [01:43<00:25, 70.48it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  80% 7280/9047 [01:43<00:25, 70.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  81% 7300/9047 [01:43<00:24, 70.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  81% 7320/9047 [01:43<00:24, 70.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  81% 7340/9047 [01:43<00:24, 70.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  81% 7360/9047 [01:43<00:23, 71.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  82% 7380/9047 [01:43<00:23, 71.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  82% 7400/9047 [01:43<00:23, 71.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  82% 7420/9047 [01:44<00:22, 71.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  82% 7440/9047 [01:44<00:22, 71.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  82% 7460/9047 [01:44<00:22, 71.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  83% 7480/9047 [01:44<00:21, 71.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  83% 7500/9047 [01:44<00:21, 71.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  83% 7520/9047 [01:44<00:21, 71.83it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  83% 7540/9047 [01:44<00:20, 71.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  84% 7560/9047 [01:44<00:20, 72.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  84% 7580/9047 [01:45<00:20, 72.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  84% 7600/9047 [01:45<00:20, 72.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  84% 7620/9047 [01:45<00:19, 72.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  84% 7640/9047 [01:45<00:19, 72.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  85% 7660/9047 [01:45<00:19, 72.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  85% 7680/9047 [01:45<00:18, 72.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  85% 7700/9047 [01:45<00:18, 72.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  85% 7720/9047 [01:46<00:18, 72.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  86% 7740/9047 [01:46<00:17, 72.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  86% 7760/9047 [01:46<00:17, 73.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  86% 7780/9047 [01:46<00:17, 73.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  86% 7800/9047 [01:46<00:17, 73.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  86% 7820/9047 [01:46<00:16, 73.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  87% 7840/9047 [01:46<00:16, 73.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  87% 7860/9047 [01:46<00:16, 73.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  87% 7880/9047 [01:47<00:15, 73.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  87% 7900/9047 [01:47<00:15, 73.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  88% 7920/9047 [01:47<00:15, 73.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  88% 7940/9047 [01:47<00:14, 73.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  88% 7960/9047 [01:47<00:14, 73.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  88% 7980/9047 [01:47<00:14, 74.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  88% 8000/9047 [01:47<00:14, 74.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  89% 8020/9047 [01:48<00:13, 74.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  89% 8040/9047 [01:48<00:13, 74.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  89% 8060/9047 [01:48<00:13, 74.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  89% 8080/9047 [01:48<00:12, 74.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  90% 8100/9047 [01:48<00:12, 74.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  90% 8120/9047 [01:48<00:12, 74.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  90% 8140/9047 [01:48<00:12, 74.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  90% 8160/9047 [01:48<00:11, 74.88it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  90% 8180/9047 [01:49<00:11, 74.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  91% 8200/9047 [01:49<00:11, 75.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  91% 8220/9047 [01:49<00:11, 75.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  91% 8240/9047 [01:49<00:10, 75.25it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  91% 8260/9047 [01:49<00:10, 75.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  92% 8280/9047 [01:49<00:10, 75.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  92% 8300/9047 [01:49<00:09, 75.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  92% 8320/9047 [01:50<00:09, 75.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  92% 8340/9047 [01:50<00:09, 75.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  92% 8360/9047 [01:50<00:09, 75.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  93% 8380/9047 [01:50<00:08, 75.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  93% 8400/9047 [01:50<00:08, 75.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  93% 8420/9047 [01:50<00:08, 76.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  93% 8440/9047 [01:50<00:07, 76.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  94% 8460/9047 [01:50<00:07, 76.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  94% 8480/9047 [01:51<00:07, 76.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  94% 8500/9047 [01:51<00:07, 76.45it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  94% 8520/9047 [01:51<00:06, 76.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  94% 8540/9047 [01:51<00:06, 76.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  95% 8560/9047 [01:51<00:06, 76.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  95% 8580/9047 [01:51<00:06, 76.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  95% 8600/9047 [01:51<00:05, 76.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  95% 8620/9047 [01:52<00:05, 76.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  96% 8640/9047 [01:52<00:05, 77.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  96% 8660/9047 [01:52<00:05, 77.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  96% 8680/9047 [01:52<00:04, 77.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  96% 8700/9047 [01:52<00:04, 77.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  96% 8720/9047 [01:52<00:04, 77.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  97% 8740/9047 [01:52<00:03, 77.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  97% 8760/9047 [01:53<00:03, 77.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  97% 8780/9047 [01:53<00:03, 77.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  97% 8800/9047 [01:53<00:03, 77.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  97% 8820/9047 [01:53<00:02, 77.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  98% 8840/9047 [01:53<00:02, 77.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  98% 8860/9047 [01:53<00:02, 77.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  98% 8880/9047 [01:53<00:02, 78.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  98% 8900/9047 [01:53<00:01, 78.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  99% 8920/9047 [01:54<00:01, 78.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  99% 8940/9047 [01:54<00:01, 78.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  99% 8960/9047 [01:54<00:01, 78.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  99% 8980/9047 [01:54<00:00, 78.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22:  99% 9000/9047 [01:54<00:00, 78.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22: 100% 9020/9047 [01:54<00:00, 78.64it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22: 100% 9040/9047 [01:54<00:00, 78.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 22: 100% 9047/9047 [01:54<00:00, 78.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  80% 7220/9047 [01:37<00:24, 73.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  80% 7240/9047 [01:42<00:25, 70.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  80% 7260/9047 [01:42<00:25, 70.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  80% 7280/9047 [01:43<00:25, 70.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  81% 7300/9047 [01:43<00:24, 70.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  81% 7320/9047 [01:43<00:24, 70.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  81% 7340/9047 [01:43<00:24, 70.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  81% 7360/9047 [01:43<00:23, 71.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  82% 7380/9047 [01:43<00:23, 71.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  82% 7400/9047 [01:43<00:23, 71.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  82% 7420/9047 [01:44<00:22, 71.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  82% 7440/9047 [01:44<00:22, 71.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  82% 7460/9047 [01:44<00:22, 71.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  83% 7480/9047 [01:44<00:21, 71.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  83% 7500/9047 [01:44<00:21, 71.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  83% 7520/9047 [01:44<00:21, 71.83it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  83% 7540/9047 [01:44<00:20, 71.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  84% 7560/9047 [01:44<00:20, 72.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  84% 7580/9047 [01:45<00:20, 72.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  84% 7600/9047 [01:45<00:20, 72.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  84% 7620/9047 [01:45<00:19, 72.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  84% 7640/9047 [01:45<00:19, 72.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  85% 7660/9047 [01:45<00:19, 72.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  85% 7680/9047 [01:45<00:18, 72.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  85% 7700/9047 [01:45<00:18, 72.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  85% 7720/9047 [01:46<00:18, 72.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  86% 7740/9047 [01:46<00:17, 72.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  86% 7760/9047 [01:46<00:17, 73.00it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  86% 7780/9047 [01:46<00:17, 73.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  86% 7800/9047 [01:46<00:17, 73.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  86% 7820/9047 [01:46<00:16, 73.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  87% 7840/9047 [01:46<00:16, 73.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  87% 7860/9047 [01:46<00:16, 73.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  87% 7880/9047 [01:47<00:15, 73.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  87% 7900/9047 [01:47<00:15, 73.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  88% 7920/9047 [01:47<00:15, 73.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  88% 7940/9047 [01:47<00:14, 73.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  88% 7960/9047 [01:47<00:14, 73.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  88% 7980/9047 [01:47<00:14, 74.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  88% 8000/9047 [01:47<00:14, 74.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  89% 8020/9047 [01:47<00:13, 74.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  89% 8040/9047 [01:48<00:13, 74.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  89% 8060/9047 [01:48<00:13, 74.48it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  89% 8080/9047 [01:48<00:12, 74.57it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  90% 8100/9047 [01:48<00:12, 74.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  90% 8120/9047 [01:48<00:12, 74.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  90% 8140/9047 [01:48<00:12, 74.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  90% 8160/9047 [01:48<00:11, 74.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  90% 8180/9047 [01:48<00:11, 75.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  91% 8200/9047 [01:49<00:11, 75.16it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  91% 8220/9047 [01:49<00:10, 75.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  91% 8240/9047 [01:49<00:10, 75.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  91% 8260/9047 [01:49<00:10, 75.45it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  92% 8280/9047 [01:49<00:10, 75.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  92% 8300/9047 [01:49<00:09, 75.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  92% 8320/9047 [01:49<00:09, 75.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  92% 8340/9047 [01:49<00:09, 75.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  92% 8360/9047 [01:50<00:09, 75.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  93% 8380/9047 [01:50<00:08, 76.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  93% 8400/9047 [01:50<00:08, 76.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  93% 8420/9047 [01:50<00:08, 76.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  93% 8440/9047 [01:50<00:07, 76.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  94% 8460/9047 [01:50<00:07, 76.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  94% 8480/9047 [01:50<00:07, 76.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  94% 8500/9047 [01:50<00:07, 76.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  94% 8520/9047 [01:51<00:06, 76.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  94% 8540/9047 [01:51<00:06, 76.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  95% 8560/9047 [01:51<00:06, 76.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  95% 8580/9047 [01:51<00:06, 76.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  95% 8600/9047 [01:51<00:05, 77.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  95% 8620/9047 [01:51<00:05, 77.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  96% 8640/9047 [01:51<00:05, 77.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  96% 8660/9047 [01:52<00:05, 77.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  96% 8680/9047 [01:52<00:04, 77.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  96% 8700/9047 [01:52<00:04, 77.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  96% 8720/9047 [01:52<00:04, 77.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  97% 8740/9047 [01:52<00:03, 77.66it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  97% 8760/9047 [01:52<00:03, 77.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  97% 8780/9047 [01:52<00:03, 77.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  97% 8800/9047 [01:52<00:03, 77.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  97% 8820/9047 [01:53<00:02, 78.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  98% 8840/9047 [01:53<00:02, 78.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  98% 8860/9047 [01:53<00:02, 78.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  98% 8880/9047 [01:53<00:02, 78.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  98% 8900/9047 [01:53<00:01, 78.36it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  99% 8920/9047 [01:53<00:01, 78.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  99% 8940/9047 [01:53<00:01, 78.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  99% 8960/9047 [01:53<00:01, 78.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  99% 8980/9047 [01:54<00:00, 78.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23:  99% 9000/9047 [01:54<00:00, 78.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23: 100% 9020/9047 [01:54<00:00, 78.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23: 100% 9040/9047 [01:54<00:00, 78.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 23: 100% 9047/9047 [01:54<00:00, 78.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  80% 7220/9047 [01:37<00:24, 74.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  80% 7240/9047 [01:42<00:25, 70.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  80% 7260/9047 [01:42<00:25, 70.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  80% 7280/9047 [01:42<00:24, 70.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  81% 7300/9047 [01:42<00:24, 70.97it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  81% 7320/9047 [01:42<00:24, 71.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  81% 7340/9047 [01:43<00:23, 71.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  81% 7360/9047 [01:43<00:23, 71.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  82% 7380/9047 [01:43<00:23, 71.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  82% 7400/9047 [01:43<00:23, 71.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  82% 7420/9047 [01:43<00:22, 71.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  82% 7440/9047 [01:43<00:22, 71.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  82% 7460/9047 [01:43<00:22, 71.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  83% 7480/9047 [01:44<00:21, 71.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  83% 7500/9047 [01:44<00:21, 72.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  83% 7520/9047 [01:44<00:21, 72.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  83% 7540/9047 [01:44<00:20, 72.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  84% 7560/9047 [01:44<00:20, 72.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  84% 7580/9047 [01:44<00:20, 72.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  84% 7600/9047 [01:44<00:19, 72.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  84% 7620/9047 [01:44<00:19, 72.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  84% 7640/9047 [01:45<00:19, 72.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  85% 7660/9047 [01:45<00:19, 72.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  85% 7680/9047 [01:45<00:18, 72.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  85% 7700/9047 [01:45<00:18, 73.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  85% 7720/9047 [01:45<00:18, 73.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  86% 7740/9047 [01:45<00:17, 73.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  86% 7760/9047 [01:45<00:17, 73.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  86% 7780/9047 [01:45<00:17, 73.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  86% 7800/9047 [01:46<00:16, 73.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  86% 7820/9047 [01:46<00:16, 73.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  87% 7840/9047 [01:46<00:16, 73.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  87% 7860/9047 [01:46<00:16, 73.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  87% 7880/9047 [01:46<00:15, 73.88it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  87% 7900/9047 [01:46<00:15, 73.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  88% 7920/9047 [01:46<00:15, 74.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  88% 7940/9047 [01:47<00:14, 74.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  88% 7960/9047 [01:47<00:14, 74.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  88% 7980/9047 [01:47<00:14, 74.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  88% 8000/9047 [01:47<00:14, 74.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  89% 8020/9047 [01:47<00:13, 74.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  89% 8040/9047 [01:47<00:13, 74.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  89% 8060/9047 [01:47<00:13, 74.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  89% 8080/9047 [01:48<00:12, 74.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  90% 8100/9047 [01:48<00:12, 74.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  90% 8120/9047 [01:48<00:12, 74.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  90% 8140/9047 [01:48<00:12, 75.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  90% 8160/9047 [01:48<00:11, 75.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  90% 8180/9047 [01:48<00:11, 75.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  91% 8200/9047 [01:48<00:11, 75.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  91% 8220/9047 [01:48<00:10, 75.47it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  91% 8240/9047 [01:49<00:10, 75.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  91% 8260/9047 [01:49<00:10, 75.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  92% 8280/9047 [01:49<00:10, 75.74it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  92% 8300/9047 [01:49<00:09, 75.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  92% 8320/9047 [01:49<00:09, 75.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  92% 8340/9047 [01:49<00:09, 76.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  92% 8360/9047 [01:49<00:09, 76.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  93% 8380/9047 [01:49<00:08, 76.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  93% 8400/9047 [01:50<00:08, 76.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  93% 8420/9047 [01:50<00:08, 76.39it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  93% 8440/9047 [01:50<00:07, 76.47it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  94% 8460/9047 [01:50<00:07, 76.56it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  94% 8480/9047 [01:50<00:07, 76.64it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  94% 8500/9047 [01:50<00:07, 76.73it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  94% 8520/9047 [01:50<00:06, 76.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  94% 8540/9047 [01:51<00:06, 76.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  95% 8560/9047 [01:51<00:06, 77.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  95% 8580/9047 [01:51<00:06, 77.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  95% 8600/9047 [01:51<00:05, 77.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  95% 8620/9047 [01:51<00:05, 77.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  96% 8640/9047 [01:51<00:05, 77.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  96% 8660/9047 [01:51<00:04, 77.43it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  96% 8680/9047 [01:51<00:04, 77.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  96% 8700/9047 [01:52<00:04, 77.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  96% 8720/9047 [01:52<00:04, 77.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  97% 8740/9047 [01:52<00:03, 77.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  97% 8760/9047 [01:52<00:03, 77.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  97% 8780/9047 [01:52<00:03, 77.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  97% 8800/9047 [01:52<00:03, 78.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  97% 8820/9047 [01:52<00:02, 78.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  98% 8840/9047 [01:53<00:02, 78.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  98% 8860/9047 [01:53<00:02, 78.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  98% 8880/9047 [01:53<00:02, 78.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  98% 8900/9047 [01:53<00:01, 78.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  99% 8920/9047 [01:53<00:01, 78.57it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  99% 8940/9047 [01:53<00:01, 78.66it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  99% 8960/9047 [01:53<00:01, 78.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  99% 8980/9047 [01:53<00:00, 78.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24:  99% 9000/9047 [01:54<00:00, 78.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24: 100% 9020/9047 [01:54<00:00, 79.01it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24: 100% 9040/9047 [01:54<00:00, 79.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 24: 100% 9047/9047 [01:54<00:00, 79.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  80% 7220/9047 [01:37<00:24, 73.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  80% 7240/9047 [01:43<00:25, 70.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  80% 7260/9047 [01:43<00:25, 70.34it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  80% 7280/9047 [01:43<00:25, 70.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  81% 7300/9047 [01:43<00:24, 70.55it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  81% 7320/9047 [01:43<00:24, 70.65it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  81% 7340/9047 [01:43<00:24, 70.75it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  81% 7360/9047 [01:43<00:23, 70.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  82% 7380/9047 [01:43<00:23, 70.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  82% 7400/9047 [01:44<00:23, 71.07it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  82% 7420/9047 [01:44<00:22, 71.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  82% 7440/9047 [01:44<00:22, 71.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  82% 7460/9047 [01:44<00:22, 71.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  83% 7480/9047 [01:44<00:21, 71.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  83% 7500/9047 [01:44<00:21, 71.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  83% 7520/9047 [01:44<00:21, 71.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  83% 7540/9047 [01:45<00:20, 71.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  84% 7560/9047 [01:45<00:20, 71.90it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  84% 7580/9047 [01:45<00:20, 72.00it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  84% 7600/9047 [01:45<00:20, 72.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  84% 7620/9047 [01:45<00:19, 72.20it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  84% 7640/9047 [01:45<00:19, 72.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  85% 7660/9047 [01:45<00:19, 72.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  85% 7680/9047 [01:45<00:18, 72.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  85% 7700/9047 [01:46<00:18, 72.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  85% 7720/9047 [01:46<00:18, 72.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  86% 7740/9047 [01:46<00:17, 72.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  86% 7760/9047 [01:46<00:17, 72.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  86% 7780/9047 [01:46<00:17, 72.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  86% 7800/9047 [01:46<00:17, 73.09it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  86% 7820/9047 [01:46<00:16, 73.18it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  87% 7840/9047 [01:46<00:16, 73.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  87% 7860/9047 [01:47<00:16, 73.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  87% 7880/9047 [01:47<00:15, 73.48it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  87% 7900/9047 [01:47<00:15, 73.57it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  88% 7920/9047 [01:47<00:15, 73.66it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  88% 7940/9047 [01:47<00:15, 73.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  88% 7960/9047 [01:47<00:14, 73.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  88% 7980/9047 [01:47<00:14, 73.94it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  88% 8000/9047 [01:48<00:14, 74.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  89% 8020/9047 [01:48<00:13, 74.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  89% 8040/9047 [01:48<00:13, 74.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  89% 8060/9047 [01:48<00:13, 74.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  89% 8080/9047 [01:48<00:12, 74.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  90% 8100/9047 [01:48<00:12, 74.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  90% 8120/9047 [01:48<00:12, 74.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  90% 8140/9047 [01:48<00:12, 74.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  90% 8160/9047 [01:49<00:11, 74.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  90% 8180/9047 [01:49<00:11, 74.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  91% 8200/9047 [01:49<00:11, 74.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  91% 8220/9047 [01:49<00:11, 75.05it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  91% 8240/9047 [01:49<00:10, 75.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  91% 8260/9047 [01:49<00:10, 75.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  92% 8280/9047 [01:49<00:10, 75.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  92% 8300/9047 [01:50<00:09, 75.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  92% 8320/9047 [01:50<00:09, 75.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  92% 8340/9047 [01:50<00:09, 75.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  92% 8360/9047 [01:50<00:09, 75.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  93% 8380/9047 [01:50<00:08, 75.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  93% 8400/9047 [01:50<00:08, 75.85it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  93% 8420/9047 [01:50<00:08, 75.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  93% 8440/9047 [01:51<00:07, 76.03it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  94% 8460/9047 [01:51<00:07, 76.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  94% 8480/9047 [01:51<00:07, 76.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  94% 8500/9047 [01:51<00:07, 76.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  94% 8520/9047 [01:51<00:06, 76.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  94% 8540/9047 [01:51<00:06, 76.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  95% 8560/9047 [01:51<00:06, 76.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  95% 8580/9047 [01:51<00:06, 76.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  95% 8600/9047 [01:52<00:05, 76.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  95% 8620/9047 [01:52<00:05, 76.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  96% 8640/9047 [01:52<00:05, 76.93it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  96% 8660/9047 [01:52<00:05, 77.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  96% 8680/9047 [01:52<00:04, 77.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  96% 8700/9047 [01:52<00:04, 77.19it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  96% 8720/9047 [01:52<00:04, 77.28it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  97% 8740/9047 [01:52<00:03, 77.37it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  97% 8760/9047 [01:53<00:03, 77.46it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  97% 8780/9047 [01:53<00:03, 77.54it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  97% 8800/9047 [01:53<00:03, 77.63it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  97% 8820/9047 [01:53<00:02, 77.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  98% 8840/9047 [01:53<00:02, 77.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  98% 8860/9047 [01:53<00:02, 77.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  98% 8880/9047 [01:53<00:02, 77.98it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  98% 8900/9047 [01:54<00:01, 78.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  99% 8920/9047 [01:54<00:01, 78.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  99% 8940/9047 [01:54<00:01, 78.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  99% 8960/9047 [01:54<00:01, 78.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  99% 8980/9047 [01:54<00:00, 78.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25:  99% 9000/9047 [01:54<00:00, 78.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25: 100% 9020/9047 [01:54<00:00, 78.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25: 100% 9040/9047 [01:54<00:00, 78.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 25: 100% 9047/9047 [01:54<00:00, 78.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.062\n",
            "Epoch 26:  80% 7220/9047 [01:37<00:24, 73.84it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  80% 7240/9047 [01:42<00:25, 70.38it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  80% 7260/9047 [01:42<00:25, 70.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  80% 7280/9047 [01:43<00:25, 70.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  81% 7300/9047 [01:43<00:24, 70.69it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  81% 7320/9047 [01:43<00:24, 70.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  81% 7340/9047 [01:43<00:24, 70.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  81% 7360/9047 [01:43<00:23, 70.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  82% 7380/9047 [01:43<00:23, 71.10it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  82% 7400/9047 [01:43<00:23, 71.20it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  82% 7420/9047 [01:44<00:22, 71.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  82% 7440/9047 [01:44<00:22, 71.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  82% 7460/9047 [01:44<00:22, 71.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  83% 7480/9047 [01:44<00:21, 71.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  83% 7500/9047 [01:44<00:21, 71.71it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  83% 7520/9047 [01:44<00:21, 71.81it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  83% 7540/9047 [01:44<00:20, 71.91it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  84% 7560/9047 [01:44<00:20, 72.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  84% 7580/9047 [01:45<00:20, 72.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  84% 7600/9047 [01:45<00:20, 72.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  84% 7620/9047 [01:45<00:19, 72.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  84% 7640/9047 [01:45<00:19, 72.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  85% 7660/9047 [01:45<00:19, 72.52it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  85% 7680/9047 [01:45<00:18, 72.62it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  85% 7700/9047 [01:45<00:18, 72.72it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  85% 7720/9047 [01:46<00:18, 72.82it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  86% 7740/9047 [01:46<00:17, 72.92it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  86% 7760/9047 [01:46<00:17, 73.02it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  86% 7780/9047 [01:46<00:17, 73.11it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  86% 7800/9047 [01:46<00:17, 73.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  86% 7820/9047 [01:46<00:16, 73.30it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  87% 7840/9047 [01:46<00:16, 73.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  87% 7860/9047 [01:46<00:16, 73.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  87% 7880/9047 [01:47<00:15, 73.60it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  87% 7900/9047 [01:47<00:15, 73.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  88% 7920/9047 [01:47<00:15, 73.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  88% 7940/9047 [01:47<00:14, 73.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  88% 7960/9047 [01:47<00:14, 73.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  88% 7980/9047 [01:47<00:14, 74.06it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  88% 8000/9047 [01:47<00:14, 74.15it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  89% 8020/9047 [01:48<00:13, 74.24it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  89% 8040/9047 [01:48<00:13, 74.33it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  89% 8060/9047 [01:48<00:13, 74.42it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  89% 8080/9047 [01:48<00:12, 74.51it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  90% 8100/9047 [01:48<00:12, 74.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  90% 8120/9047 [01:48<00:12, 74.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  90% 8140/9047 [01:48<00:12, 74.80it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  90% 8160/9047 [01:48<00:11, 74.89it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  90% 8180/9047 [01:49<00:11, 74.99it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  91% 8200/9047 [01:49<00:11, 75.08it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  91% 8220/9047 [01:49<00:11, 75.17it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  91% 8240/9047 [01:49<00:10, 75.26it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  91% 8260/9047 [01:49<00:10, 75.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  92% 8280/9047 [01:49<00:10, 75.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  92% 8300/9047 [01:49<00:09, 75.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  92% 8320/9047 [01:50<00:09, 75.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  92% 8340/9047 [01:50<00:09, 75.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  92% 8360/9047 [01:50<00:09, 75.78it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  93% 8380/9047 [01:50<00:08, 75.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  93% 8400/9047 [01:50<00:08, 75.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  93% 8420/9047 [01:50<00:08, 76.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  93% 8440/9047 [01:50<00:07, 76.14it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  94% 8460/9047 [01:50<00:07, 76.23it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  94% 8480/9047 [01:51<00:07, 76.32it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  94% 8500/9047 [01:51<00:07, 76.41it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  94% 8520/9047 [01:51<00:06, 76.50it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  94% 8540/9047 [01:51<00:06, 76.59it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  95% 8560/9047 [01:51<00:06, 76.68it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  95% 8580/9047 [01:51<00:06, 76.77it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  95% 8600/9047 [01:51<00:05, 76.86it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  95% 8620/9047 [01:52<00:05, 76.95it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  96% 8640/9047 [01:52<00:05, 77.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  96% 8660/9047 [01:52<00:05, 77.12it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  96% 8680/9047 [01:52<00:04, 77.21it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  96% 8700/9047 [01:52<00:04, 77.29it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  96% 8720/9047 [01:52<00:04, 77.35it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  97% 8740/9047 [01:52<00:03, 77.44it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  97% 8760/9047 [01:52<00:03, 77.53it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  97% 8780/9047 [01:53<00:03, 77.61it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  97% 8800/9047 [01:53<00:03, 77.70it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  97% 8820/9047 [01:53<00:02, 77.79it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  98% 8840/9047 [01:53<00:02, 77.87it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  98% 8860/9047 [01:53<00:02, 77.96it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  98% 8880/9047 [01:53<00:02, 78.04it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  98% 8900/9047 [01:53<00:01, 78.13it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  99% 8920/9047 [01:54<00:01, 78.22it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  99% 8940/9047 [01:54<00:01, 78.31it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  99% 8960/9047 [01:54<00:01, 78.40it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  99% 8980/9047 [01:54<00:00, 78.49it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26:  99% 9000/9047 [01:54<00:00, 78.58it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26: 100% 9020/9047 [01:54<00:00, 78.67it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26: 100% 9040/9047 [01:54<00:00, 78.76it/s, loss=4.06, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 26: 100% 9047/9047 [01:54<00:00, 78.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  80% 7220/9047 [01:38<00:24, 73.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  80% 7240/9047 [01:43<00:25, 69.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  80% 7260/9047 [01:43<00:25, 70.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  80% 7280/9047 [01:43<00:25, 70.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  81% 7300/9047 [01:43<00:24, 70.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  81% 7320/9047 [01:44<00:24, 70.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  81% 7340/9047 [01:44<00:24, 70.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  81% 7360/9047 [01:44<00:23, 70.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  82% 7380/9047 [01:44<00:23, 70.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  82% 7400/9047 [01:44<00:23, 70.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  82% 7420/9047 [01:44<00:22, 70.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  82% 7440/9047 [01:44<00:22, 70.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  82% 7460/9047 [01:44<00:22, 71.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  83% 7480/9047 [01:45<00:22, 71.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  83% 7500/9047 [01:45<00:21, 71.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  83% 7520/9047 [01:45<00:21, 71.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  83% 7540/9047 [01:45<00:21, 71.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  84% 7560/9047 [01:45<00:20, 71.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  84% 7580/9047 [01:45<00:20, 71.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  84% 7600/9047 [01:45<00:20, 71.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  84% 7620/9047 [01:46<00:19, 71.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  84% 7640/9047 [01:46<00:19, 71.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  85% 7660/9047 [01:46<00:19, 72.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  85% 7680/9047 [01:46<00:18, 72.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  85% 7700/9047 [01:46<00:18, 72.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  85% 7720/9047 [01:46<00:18, 72.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  86% 7740/9047 [01:46<00:18, 72.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  86% 7760/9047 [01:46<00:17, 72.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  86% 7780/9047 [01:47<00:17, 72.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  86% 7800/9047 [01:47<00:17, 72.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  86% 7820/9047 [01:47<00:16, 72.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  87% 7840/9047 [01:47<00:16, 72.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  87% 7860/9047 [01:47<00:16, 73.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  87% 7880/9047 [01:47<00:15, 73.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  87% 7900/9047 [01:47<00:15, 73.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  88% 7920/9047 [01:48<00:15, 73.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  88% 7940/9047 [01:48<00:15, 73.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  88% 7960/9047 [01:48<00:14, 73.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  88% 7980/9047 [01:48<00:14, 73.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  88% 8000/9047 [01:48<00:14, 73.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  89% 8020/9047 [01:48<00:13, 73.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  89% 8040/9047 [01:48<00:13, 73.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  89% 8060/9047 [01:48<00:13, 73.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  89% 8080/9047 [01:49<00:13, 74.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  90% 8100/9047 [01:49<00:12, 74.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  90% 8120/9047 [01:49<00:12, 74.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  90% 8140/9047 [01:49<00:12, 74.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  90% 8160/9047 [01:49<00:11, 74.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  90% 8180/9047 [01:49<00:11, 74.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  91% 8200/9047 [01:49<00:11, 74.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  91% 8220/9047 [01:50<00:11, 74.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  91% 8240/9047 [01:50<00:10, 74.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  91% 8260/9047 [01:50<00:10, 74.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  92% 8280/9047 [01:50<00:10, 74.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  92% 8300/9047 [01:50<00:09, 75.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  92% 8320/9047 [01:50<00:09, 75.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  92% 8340/9047 [01:50<00:09, 75.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  92% 8360/9047 [01:51<00:09, 75.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  93% 8380/9047 [01:51<00:08, 75.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  93% 8400/9047 [01:51<00:08, 75.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  93% 8420/9047 [01:51<00:08, 75.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  93% 8440/9047 [01:51<00:08, 75.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  94% 8460/9047 [01:51<00:07, 75.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  94% 8480/9047 [01:51<00:07, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  94% 8500/9047 [01:52<00:07, 75.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  94% 8520/9047 [01:52<00:06, 75.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  94% 8540/9047 [01:52<00:06, 76.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  95% 8560/9047 [01:52<00:06, 76.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  95% 8580/9047 [01:52<00:06, 76.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  95% 8600/9047 [01:52<00:05, 76.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  95% 8620/9047 [01:52<00:05, 76.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  96% 8640/9047 [01:52<00:05, 76.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  96% 8660/9047 [01:53<00:05, 76.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  96% 8680/9047 [01:53<00:04, 76.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  96% 8700/9047 [01:53<00:04, 76.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  96% 8720/9047 [01:53<00:04, 76.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  97% 8740/9047 [01:53<00:03, 76.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  97% 8760/9047 [01:53<00:03, 77.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  97% 8780/9047 [01:53<00:03, 77.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  97% 8800/9047 [01:54<00:03, 77.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  97% 8820/9047 [01:54<00:02, 77.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  98% 8840/9047 [01:54<00:02, 77.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  98% 8860/9047 [01:54<00:02, 77.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  98% 8880/9047 [01:54<00:02, 77.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  98% 8900/9047 [01:54<00:01, 77.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  99% 8920/9047 [01:54<00:01, 77.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  99% 8940/9047 [01:54<00:01, 77.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  99% 8960/9047 [01:55<00:01, 77.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  99% 8980/9047 [01:55<00:00, 77.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27:  99% 9000/9047 [01:55<00:00, 78.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27: 100% 9020/9047 [01:55<00:00, 78.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27: 100% 9040/9047 [01:55<00:00, 78.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 27: 100% 9047/9047 [01:55<00:00, 78.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.060]\n",
            "Epoch 28:  80% 7220/9047 [01:37<00:24, 73.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  80% 7240/9047 [01:42<00:25, 70.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  80% 7260/9047 [01:42<00:25, 70.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  80% 7280/9047 [01:42<00:24, 70.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  81% 7300/9047 [01:43<00:24, 70.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  81% 7320/9047 [01:43<00:24, 70.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  81% 7340/9047 [01:43<00:24, 71.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  81% 7360/9047 [01:43<00:23, 71.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  82% 7380/9047 [01:43<00:23, 71.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  82% 7400/9047 [01:43<00:23, 71.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  82% 7420/9047 [01:43<00:22, 71.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  82% 7440/9047 [01:44<00:22, 71.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  82% 7460/9047 [01:44<00:22, 71.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  83% 7480/9047 [01:44<00:21, 71.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  83% 7500/9047 [01:44<00:21, 71.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  83% 7520/9047 [01:44<00:21, 71.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  83% 7540/9047 [01:44<00:20, 72.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  84% 7560/9047 [01:44<00:20, 72.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  84% 7580/9047 [01:44<00:20, 72.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  84% 7600/9047 [01:45<00:20, 72.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  84% 7620/9047 [01:45<00:19, 72.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  84% 7640/9047 [01:45<00:19, 72.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  85% 7660/9047 [01:45<00:19, 72.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  85% 7680/9047 [01:45<00:18, 72.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  85% 7700/9047 [01:45<00:18, 72.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  85% 7720/9047 [01:45<00:18, 72.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  86% 7740/9047 [01:45<00:17, 73.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  86% 7760/9047 [01:46<00:17, 73.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  86% 7780/9047 [01:46<00:17, 73.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  86% 7800/9047 [01:46<00:17, 73.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  86% 7820/9047 [01:46<00:16, 73.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  87% 7840/9047 [01:46<00:16, 73.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  87% 7860/9047 [01:46<00:16, 73.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  87% 7880/9047 [01:46<00:15, 73.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  87% 7900/9047 [01:47<00:15, 73.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  88% 7920/9047 [01:47<00:15, 73.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  88% 7940/9047 [01:47<00:14, 73.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  88% 7960/9047 [01:47<00:14, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  88% 7980/9047 [01:47<00:14, 74.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  88% 8000/9047 [01:47<00:14, 74.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  89% 8020/9047 [01:47<00:13, 74.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  89% 8040/9047 [01:48<00:13, 74.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  89% 8060/9047 [01:48<00:13, 74.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  89% 8080/9047 [01:48<00:12, 74.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  90% 8100/9047 [01:48<00:12, 74.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  90% 8120/9047 [01:48<00:12, 74.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  90% 8140/9047 [01:48<00:12, 74.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  90% 8160/9047 [01:48<00:11, 74.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  90% 8180/9047 [01:48<00:11, 75.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  91% 8200/9047 [01:49<00:11, 75.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  91% 8220/9047 [01:49<00:10, 75.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  91% 8240/9047 [01:49<00:10, 75.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  91% 8260/9047 [01:49<00:10, 75.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  92% 8280/9047 [01:49<00:10, 75.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  92% 8300/9047 [01:49<00:09, 75.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  92% 8320/9047 [01:49<00:09, 75.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  92% 8340/9047 [01:50<00:09, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  92% 8360/9047 [01:50<00:09, 75.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  93% 8380/9047 [01:50<00:08, 75.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  93% 8400/9047 [01:50<00:08, 76.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  93% 8420/9047 [01:50<00:08, 76.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  93% 8440/9047 [01:50<00:07, 76.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  94% 8460/9047 [01:50<00:07, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  94% 8480/9047 [01:50<00:07, 76.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  94% 8500/9047 [01:51<00:07, 76.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  94% 8520/9047 [01:51<00:06, 76.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  94% 8540/9047 [01:51<00:06, 76.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  95% 8560/9047 [01:51<00:06, 76.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  95% 8580/9047 [01:51<00:06, 76.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  95% 8600/9047 [01:51<00:05, 76.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  95% 8620/9047 [01:51<00:05, 77.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  96% 8640/9047 [01:52<00:05, 77.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  96% 8660/9047 [01:52<00:05, 77.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  96% 8680/9047 [01:52<00:04, 77.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  96% 8700/9047 [01:52<00:04, 77.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  96% 8720/9047 [01:52<00:04, 77.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  97% 8740/9047 [01:52<00:03, 77.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  97% 8760/9047 [01:52<00:03, 77.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  97% 8780/9047 [01:52<00:03, 77.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  97% 8800/9047 [01:53<00:03, 77.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  97% 8820/9047 [01:53<00:02, 77.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  98% 8840/9047 [01:53<00:02, 78.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  98% 8860/9047 [01:53<00:02, 78.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  98% 8880/9047 [01:53<00:02, 78.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  98% 8900/9047 [01:53<00:01, 78.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  99% 8920/9047 [01:53<00:01, 78.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  99% 8940/9047 [01:53<00:01, 78.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  99% 8960/9047 [01:54<00:01, 78.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  99% 8980/9047 [01:54<00:00, 78.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28:  99% 9000/9047 [01:54<00:00, 78.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28: 100% 9020/9047 [01:54<00:00, 78.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28: 100% 9040/9047 [01:54<00:00, 78.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 28: 100% 9047/9047 [01:54<00:00, 78.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  80% 7220/9047 [01:37<00:24, 73.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  80% 7240/9047 [01:43<00:25, 70.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  80% 7260/9047 [01:43<00:25, 70.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  80% 7280/9047 [01:43<00:25, 70.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  81% 7300/9047 [01:43<00:24, 70.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  81% 7320/9047 [01:43<00:24, 70.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  81% 7340/9047 [01:43<00:24, 70.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  81% 7360/9047 [01:43<00:23, 70.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  82% 7380/9047 [01:44<00:23, 70.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  82% 7400/9047 [01:44<00:23, 70.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  82% 7420/9047 [01:44<00:22, 71.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  82% 7440/9047 [01:44<00:22, 71.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  82% 7460/9047 [01:44<00:22, 71.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  83% 7480/9047 [01:44<00:21, 71.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  83% 7500/9047 [01:44<00:21, 71.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  83% 7520/9047 [01:45<00:21, 71.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  83% 7540/9047 [01:45<00:21, 71.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  84% 7560/9047 [01:45<00:20, 71.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  84% 7580/9047 [01:45<00:20, 71.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  84% 7600/9047 [01:45<00:20, 71.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  84% 7620/9047 [01:45<00:19, 72.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  84% 7640/9047 [01:45<00:19, 72.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  85% 7660/9047 [01:45<00:19, 72.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  85% 7680/9047 [01:46<00:18, 72.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  85% 7700/9047 [01:46<00:18, 72.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  85% 7720/9047 [01:46<00:18, 72.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  86% 7740/9047 [01:46<00:17, 72.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  86% 7760/9047 [01:46<00:17, 72.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  86% 7780/9047 [01:46<00:17, 72.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  86% 7800/9047 [01:46<00:17, 72.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  86% 7820/9047 [01:47<00:16, 73.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  87% 7840/9047 [01:47<00:16, 73.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  87% 7860/9047 [01:47<00:16, 73.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  87% 7880/9047 [01:47<00:15, 73.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  87% 7900/9047 [01:47<00:15, 73.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  88% 7920/9047 [01:47<00:15, 73.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  88% 7940/9047 [01:47<00:15, 73.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  88% 7960/9047 [01:47<00:14, 73.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  88% 7980/9047 [01:48<00:14, 73.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  88% 8000/9047 [01:48<00:14, 73.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  89% 8020/9047 [01:48<00:13, 74.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  89% 8040/9047 [01:48<00:13, 74.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  89% 8060/9047 [01:48<00:13, 74.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  89% 8080/9047 [01:48<00:13, 74.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  90% 8100/9047 [01:48<00:12, 74.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  90% 8120/9047 [01:48<00:12, 74.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  90% 8140/9047 [01:49<00:12, 74.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  90% 8160/9047 [01:49<00:11, 74.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  90% 8180/9047 [01:49<00:11, 74.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  91% 8200/9047 [01:49<00:11, 74.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  91% 8220/9047 [01:49<00:11, 75.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  91% 8240/9047 [01:49<00:10, 75.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  91% 8260/9047 [01:49<00:10, 75.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  92% 8280/9047 [01:49<00:10, 75.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  92% 8300/9047 [01:50<00:09, 75.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  92% 8320/9047 [01:50<00:09, 75.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  92% 8340/9047 [01:50<00:09, 75.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  92% 8360/9047 [01:50<00:09, 75.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  93% 8380/9047 [01:50<00:08, 75.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  93% 8400/9047 [01:50<00:08, 75.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  93% 8420/9047 [01:50<00:08, 76.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  93% 8440/9047 [01:50<00:07, 76.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  94% 8460/9047 [01:51<00:07, 76.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  94% 8480/9047 [01:51<00:07, 76.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  94% 8500/9047 [01:51<00:07, 76.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  94% 8520/9047 [01:51<00:06, 76.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  94% 8540/9047 [01:51<00:06, 76.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  95% 8560/9047 [01:51<00:06, 76.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  95% 8580/9047 [01:51<00:06, 76.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  95% 8600/9047 [01:51<00:05, 76.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  95% 8620/9047 [01:52<00:05, 76.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  96% 8640/9047 [01:52<00:05, 77.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  96% 8660/9047 [01:52<00:05, 77.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  96% 8680/9047 [01:52<00:04, 77.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  96% 8700/9047 [01:52<00:04, 77.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  96% 8720/9047 [01:52<00:04, 77.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  97% 8740/9047 [01:52<00:03, 77.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  97% 8760/9047 [01:53<00:03, 77.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  97% 8780/9047 [01:53<00:03, 77.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  97% 8800/9047 [01:53<00:03, 77.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  97% 8820/9047 [01:53<00:02, 77.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  98% 8840/9047 [01:53<00:02, 77.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  98% 8860/9047 [01:53<00:02, 77.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  98% 8880/9047 [01:53<00:02, 77.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  98% 8900/9047 [01:53<00:01, 78.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  99% 8920/9047 [01:54<00:01, 78.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  99% 8940/9047 [01:54<00:01, 78.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  99% 8960/9047 [01:54<00:01, 78.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  99% 8980/9047 [01:54<00:00, 78.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29:  99% 9000/9047 [01:54<00:00, 78.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29: 100% 9020/9047 [01:54<00:00, 78.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29: 100% 9040/9047 [01:54<00:00, 78.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 29: 100% 9047/9047 [01:54<00:00, 78.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  80% 7220/9047 [01:38<00:24, 73.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  80% 7240/9047 [01:44<00:25, 69.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  80% 7260/9047 [01:44<00:25, 69.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  80% 7280/9047 [01:44<00:25, 69.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  81% 7300/9047 [01:44<00:24, 69.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  81% 7320/9047 [01:44<00:24, 69.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  81% 7340/9047 [01:44<00:24, 70.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  81% 7360/9047 [01:44<00:24, 70.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  82% 7380/9047 [01:44<00:23, 70.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  82% 7400/9047 [01:45<00:23, 70.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  82% 7420/9047 [01:45<00:23, 70.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  82% 7440/9047 [01:45<00:22, 70.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  82% 7460/9047 [01:45<00:22, 70.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  83% 7480/9047 [01:45<00:22, 70.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  83% 7500/9047 [01:45<00:21, 70.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  83% 7520/9047 [01:45<00:21, 71.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  83% 7540/9047 [01:46<00:21, 71.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  84% 7560/9047 [01:46<00:20, 71.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  84% 7580/9047 [01:46<00:20, 71.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  84% 7600/9047 [01:46<00:20, 71.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  84% 7620/9047 [01:46<00:19, 71.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  84% 7640/9047 [01:46<00:19, 71.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  85% 7660/9047 [01:46<00:19, 71.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  85% 7680/9047 [01:46<00:19, 71.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  85% 7700/9047 [01:47<00:18, 71.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  85% 7720/9047 [01:47<00:18, 72.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  86% 7740/9047 [01:47<00:18, 72.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  86% 7760/9047 [01:47<00:17, 72.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  86% 7780/9047 [01:47<00:17, 72.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  86% 7800/9047 [01:47<00:17, 72.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  86% 7820/9047 [01:47<00:16, 72.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  87% 7840/9047 [01:47<00:16, 72.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  87% 7860/9047 [01:48<00:16, 72.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  87% 7880/9047 [01:48<00:16, 72.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  87% 7900/9047 [01:48<00:15, 72.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  88% 7920/9047 [01:48<00:15, 73.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  88% 7940/9047 [01:48<00:15, 73.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  88% 7960/9047 [01:48<00:14, 73.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  88% 7980/9047 [01:48<00:14, 73.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  88% 8000/9047 [01:49<00:14, 73.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  89% 8020/9047 [01:49<00:13, 73.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  89% 8040/9047 [01:49<00:13, 73.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  89% 8060/9047 [01:49<00:13, 73.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  89% 8080/9047 [01:49<00:13, 73.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  90% 8100/9047 [01:49<00:12, 73.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  90% 8120/9047 [01:49<00:12, 73.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  90% 8140/9047 [01:49<00:12, 74.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  90% 8160/9047 [01:50<00:11, 74.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  90% 8180/9047 [01:50<00:11, 74.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  91% 8200/9047 [01:50<00:11, 74.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  91% 8220/9047 [01:50<00:11, 74.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  91% 8240/9047 [01:50<00:10, 74.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  91% 8260/9047 [01:50<00:10, 74.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  92% 8280/9047 [01:50<00:10, 74.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  92% 8300/9047 [01:50<00:09, 74.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  92% 8320/9047 [01:51<00:09, 74.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  92% 8340/9047 [01:51<00:09, 75.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  92% 8360/9047 [01:51<00:09, 75.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  93% 8380/9047 [01:51<00:08, 75.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  93% 8400/9047 [01:51<00:08, 75.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  93% 8420/9047 [01:51<00:08, 75.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  93% 8440/9047 [01:51<00:08, 75.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  94% 8460/9047 [01:51<00:07, 75.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  94% 8480/9047 [01:52<00:07, 75.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  94% 8500/9047 [01:52<00:07, 75.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  94% 8520/9047 [01:52<00:06, 75.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  94% 8540/9047 [01:52<00:06, 75.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  95% 8560/9047 [01:52<00:06, 76.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  95% 8580/9047 [01:52<00:06, 76.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  95% 8600/9047 [01:52<00:05, 76.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  95% 8620/9047 [01:52<00:05, 76.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  96% 8640/9047 [01:53<00:05, 76.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  96% 8660/9047 [01:53<00:05, 76.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  96% 8680/9047 [01:53<00:04, 76.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  96% 8700/9047 [01:53<00:04, 76.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  96% 8720/9047 [01:53<00:04, 76.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  97% 8740/9047 [01:53<00:03, 76.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  97% 8760/9047 [01:53<00:03, 76.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  97% 8780/9047 [01:53<00:03, 77.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  97% 8800/9047 [01:54<00:03, 77.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  97% 8820/9047 [01:54<00:02, 77.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  98% 8840/9047 [01:54<00:02, 77.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  98% 8860/9047 [01:54<00:02, 77.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  98% 8880/9047 [01:54<00:02, 77.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  98% 8900/9047 [01:54<00:01, 77.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  99% 8920/9047 [01:54<00:01, 77.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  99% 8940/9047 [01:55<00:01, 77.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  99% 8960/9047 [01:55<00:01, 77.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  99% 8980/9047 [01:55<00:00, 77.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30:  99% 9000/9047 [01:55<00:00, 77.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30: 100% 9020/9047 [01:55<00:00, 78.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30: 100% 9040/9047 [01:55<00:00, 78.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 30: 100% 9047/9047 [01:55<00:00, 78.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  80% 7220/9047 [01:37<00:24, 74.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  80% 7240/9047 [01:42<00:25, 70.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  80% 7260/9047 [01:42<00:25, 70.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  80% 7280/9047 [01:42<00:24, 70.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  81% 7300/9047 [01:42<00:24, 70.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  81% 7320/9047 [01:43<00:24, 71.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  81% 7340/9047 [01:43<00:24, 71.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  81% 7360/9047 [01:43<00:23, 71.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  82% 7380/9047 [01:43<00:23, 71.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  82% 7400/9047 [01:43<00:23, 71.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  82% 7420/9047 [01:43<00:22, 71.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  82% 7440/9047 [01:43<00:22, 71.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  82% 7460/9047 [01:44<00:22, 71.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  83% 7480/9047 [01:44<00:21, 71.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  83% 7500/9047 [01:44<00:21, 71.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  83% 7520/9047 [01:44<00:21, 71.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  83% 7540/9047 [01:44<00:20, 72.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  84% 7560/9047 [01:44<00:20, 72.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  84% 7580/9047 [01:44<00:20, 72.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  84% 7600/9047 [01:45<00:20, 72.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  84% 7620/9047 [01:45<00:19, 72.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  84% 7640/9047 [01:45<00:19, 72.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  85% 7660/9047 [01:45<00:19, 72.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  85% 7680/9047 [01:45<00:18, 72.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  85% 7700/9047 [01:45<00:18, 72.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  85% 7720/9047 [01:45<00:18, 72.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  86% 7740/9047 [01:45<00:17, 73.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  86% 7760/9047 [01:46<00:17, 73.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  86% 7780/9047 [01:46<00:17, 73.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  86% 7800/9047 [01:46<00:16, 73.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  86% 7820/9047 [01:46<00:16, 73.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  87% 7840/9047 [01:46<00:16, 73.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  87% 7860/9047 [01:46<00:16, 73.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  87% 7880/9047 [01:46<00:15, 73.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  87% 7900/9047 [01:46<00:15, 73.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  88% 7920/9047 [01:47<00:15, 73.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  88% 7940/9047 [01:47<00:14, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  88% 7960/9047 [01:47<00:14, 74.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  88% 7980/9047 [01:47<00:14, 74.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  88% 8000/9047 [01:47<00:14, 74.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  89% 8020/9047 [01:47<00:13, 74.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  89% 8040/9047 [01:47<00:13, 74.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  89% 8060/9047 [01:47<00:13, 74.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  89% 8080/9047 [01:48<00:12, 74.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  90% 8100/9047 [01:48<00:12, 74.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  90% 8120/9047 [01:48<00:12, 74.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  90% 8140/9047 [01:48<00:12, 75.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  90% 8160/9047 [01:48<00:11, 75.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  90% 8180/9047 [01:48<00:11, 75.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  91% 8200/9047 [01:48<00:11, 75.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  91% 8220/9047 [01:48<00:10, 75.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  91% 8240/9047 [01:49<00:10, 75.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  91% 8260/9047 [01:49<00:10, 75.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  92% 8280/9047 [01:49<00:10, 75.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  92% 8300/9047 [01:49<00:09, 75.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  92% 8320/9047 [01:49<00:09, 75.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  92% 8340/9047 [01:49<00:09, 75.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  92% 8360/9047 [01:49<00:09, 76.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  93% 8380/9047 [01:49<00:08, 76.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  93% 8400/9047 [01:50<00:08, 76.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  93% 8420/9047 [01:50<00:08, 76.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  93% 8440/9047 [01:50<00:07, 76.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  94% 8460/9047 [01:50<00:07, 76.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  94% 8480/9047 [01:50<00:07, 76.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  94% 8500/9047 [01:50<00:07, 76.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  94% 8520/9047 [01:50<00:06, 76.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  94% 8540/9047 [01:50<00:06, 76.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  95% 8560/9047 [01:51<00:06, 77.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  95% 8580/9047 [01:51<00:06, 77.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  95% 8600/9047 [01:51<00:05, 77.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  95% 8620/9047 [01:51<00:05, 77.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  96% 8640/9047 [01:51<00:05, 77.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  96% 8660/9047 [01:51<00:04, 77.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  96% 8680/9047 [01:51<00:04, 77.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  96% 8700/9047 [01:52<00:04, 77.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  96% 8720/9047 [01:52<00:04, 77.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  97% 8740/9047 [01:52<00:03, 77.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  97% 8760/9047 [01:52<00:03, 77.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  97% 8780/9047 [01:52<00:03, 77.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  97% 8800/9047 [01:52<00:03, 78.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  97% 8820/9047 [01:52<00:02, 78.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  98% 8840/9047 [01:52<00:02, 78.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  98% 8860/9047 [01:53<00:02, 78.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  98% 8880/9047 [01:53<00:02, 78.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  98% 8900/9047 [01:53<00:01, 78.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  99% 8920/9047 [01:53<00:01, 78.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  99% 8940/9047 [01:53<00:01, 78.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  99% 8960/9047 [01:53<00:01, 78.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  99% 8980/9047 [01:53<00:00, 78.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31:  99% 9000/9047 [01:54<00:00, 78.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31: 100% 9020/9047 [01:54<00:00, 78.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31: 100% 9040/9047 [01:54<00:00, 79.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 31: 100% 9047/9047 [01:54<00:00, 79.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  80% 7220/9047 [01:36<00:24, 74.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  80% 7240/9047 [01:42<00:25, 70.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  80% 7260/9047 [01:42<00:25, 70.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  80% 7280/9047 [01:42<00:24, 70.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  81% 7300/9047 [01:42<00:24, 71.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  81% 7320/9047 [01:42<00:24, 71.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  81% 7340/9047 [01:43<00:23, 71.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  81% 7360/9047 [01:43<00:23, 71.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  82% 7380/9047 [01:43<00:23, 71.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  82% 7400/9047 [01:43<00:23, 71.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  82% 7420/9047 [01:43<00:22, 71.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  82% 7440/9047 [01:43<00:22, 71.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  82% 7460/9047 [01:43<00:22, 71.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  83% 7480/9047 [01:43<00:21, 71.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  83% 7500/9047 [01:44<00:21, 72.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  83% 7520/9047 [01:44<00:21, 72.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  83% 7540/9047 [01:44<00:20, 72.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  84% 7560/9047 [01:44<00:20, 72.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  84% 7580/9047 [01:44<00:20, 72.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  84% 7600/9047 [01:44<00:19, 72.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  84% 7620/9047 [01:44<00:19, 72.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  84% 7640/9047 [01:44<00:19, 72.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  85% 7660/9047 [01:45<00:19, 72.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  85% 7680/9047 [01:45<00:18, 72.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  85% 7700/9047 [01:45<00:18, 73.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  85% 7720/9047 [01:45<00:18, 73.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  86% 7740/9047 [01:45<00:17, 73.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  86% 7760/9047 [01:45<00:17, 73.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  86% 7780/9047 [01:45<00:17, 73.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  86% 7800/9047 [01:45<00:16, 73.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  86% 7820/9047 [01:46<00:16, 73.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  87% 7840/9047 [01:46<00:16, 73.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  87% 7860/9047 [01:46<00:16, 73.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  87% 7880/9047 [01:46<00:15, 74.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  87% 7900/9047 [01:46<00:15, 74.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  88% 7920/9047 [01:46<00:15, 74.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  88% 7940/9047 [01:46<00:14, 74.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  88% 7960/9047 [01:46<00:14, 74.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  88% 7980/9047 [01:47<00:14, 74.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  88% 8000/9047 [01:47<00:14, 74.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  89% 8020/9047 [01:47<00:13, 74.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  89% 8040/9047 [01:47<00:13, 74.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  89% 8060/9047 [01:47<00:13, 74.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  89% 8080/9047 [01:47<00:12, 74.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  90% 8100/9047 [01:47<00:12, 75.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  90% 8120/9047 [01:48<00:12, 75.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  90% 8140/9047 [01:48<00:12, 75.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  90% 8160/9047 [01:48<00:11, 75.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  90% 8180/9047 [01:48<00:11, 75.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  91% 8200/9047 [01:48<00:11, 75.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  91% 8220/9047 [01:48<00:10, 75.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  91% 8240/9047 [01:48<00:10, 75.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  91% 8260/9047 [01:48<00:10, 75.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  92% 8280/9047 [01:49<00:10, 75.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  92% 8300/9047 [01:49<00:09, 76.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  92% 8320/9047 [01:49<00:09, 76.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  92% 8340/9047 [01:49<00:09, 76.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  92% 8360/9047 [01:49<00:09, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  93% 8380/9047 [01:49<00:08, 76.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  93% 8400/9047 [01:49<00:08, 76.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  93% 8420/9047 [01:49<00:08, 76.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  93% 8440/9047 [01:50<00:07, 76.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  94% 8460/9047 [01:50<00:07, 76.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  94% 8480/9047 [01:50<00:07, 76.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  94% 8500/9047 [01:50<00:07, 76.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  94% 8520/9047 [01:50<00:06, 77.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  94% 8540/9047 [01:50<00:06, 77.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  95% 8560/9047 [01:50<00:06, 77.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  95% 8580/9047 [01:50<00:06, 77.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  95% 8600/9047 [01:51<00:05, 77.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  95% 8620/9047 [01:51<00:05, 77.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  96% 8640/9047 [01:51<00:05, 77.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  96% 8660/9047 [01:51<00:04, 77.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  96% 8680/9047 [01:51<00:04, 77.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  96% 8700/9047 [01:51<00:04, 77.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  96% 8720/9047 [01:51<00:04, 77.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  97% 8740/9047 [01:51<00:03, 78.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  97% 8760/9047 [01:52<00:03, 78.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  97% 8780/9047 [01:52<00:03, 78.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  97% 8800/9047 [01:52<00:03, 78.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  97% 8820/9047 [01:52<00:02, 78.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  98% 8840/9047 [01:52<00:02, 78.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  98% 8860/9047 [01:52<00:02, 78.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  98% 8880/9047 [01:52<00:02, 78.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  98% 8900/9047 [01:53<00:01, 78.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  99% 8920/9047 [01:53<00:01, 78.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  99% 8940/9047 [01:53<00:01, 78.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  99% 8960/9047 [01:53<00:01, 78.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  99% 8980/9047 [01:53<00:00, 79.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32:  99% 9000/9047 [01:53<00:00, 79.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32: 100% 9020/9047 [01:53<00:00, 79.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32: 100% 9040/9047 [01:54<00:00, 79.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 32: 100% 9047/9047 [01:54<00:00, 79.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  80% 7220/9047 [01:37<00:24, 74.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  80% 7240/9047 [01:42<00:25, 70.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  80% 7260/9047 [01:42<00:25, 70.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  80% 7280/9047 [01:42<00:24, 70.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  81% 7300/9047 [01:42<00:24, 70.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  81% 7320/9047 [01:42<00:24, 71.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  81% 7340/9047 [01:43<00:23, 71.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  81% 7360/9047 [01:43<00:23, 71.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  82% 7380/9047 [01:43<00:23, 71.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  82% 7400/9047 [01:43<00:23, 71.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  82% 7420/9047 [01:43<00:22, 71.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  82% 7440/9047 [01:43<00:22, 71.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  82% 7460/9047 [01:43<00:22, 71.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  83% 7480/9047 [01:44<00:21, 71.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  83% 7500/9047 [01:44<00:21, 71.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  83% 7520/9047 [01:44<00:21, 72.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  83% 7540/9047 [01:44<00:20, 72.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  84% 7560/9047 [01:44<00:20, 72.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  84% 7580/9047 [01:44<00:20, 72.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  84% 7600/9047 [01:44<00:19, 72.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  84% 7620/9047 [01:45<00:19, 72.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  84% 7640/9047 [01:45<00:19, 72.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  85% 7660/9047 [01:45<00:19, 72.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  85% 7680/9047 [01:45<00:18, 72.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  85% 7700/9047 [01:45<00:18, 72.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  85% 7720/9047 [01:45<00:18, 73.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  86% 7740/9047 [01:45<00:17, 73.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  86% 7760/9047 [01:45<00:17, 73.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  86% 7780/9047 [01:46<00:17, 73.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  86% 7800/9047 [01:46<00:16, 73.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  86% 7820/9047 [01:46<00:16, 73.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  87% 7840/9047 [01:46<00:16, 73.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  87% 7860/9047 [01:46<00:16, 73.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  87% 7880/9047 [01:46<00:15, 73.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  87% 7900/9047 [01:46<00:15, 73.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  88% 7920/9047 [01:47<00:15, 73.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  88% 7940/9047 [01:47<00:14, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  88% 7960/9047 [01:47<00:14, 74.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  88% 7980/9047 [01:47<00:14, 74.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  88% 8000/9047 [01:47<00:14, 74.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  89% 8020/9047 [01:47<00:13, 74.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  89% 8040/9047 [01:47<00:13, 74.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  89% 8060/9047 [01:48<00:13, 74.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  89% 8080/9047 [01:48<00:12, 74.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  90% 8100/9047 [01:48<00:12, 74.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  90% 8120/9047 [01:48<00:12, 74.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  90% 8140/9047 [01:48<00:12, 75.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  90% 8160/9047 [01:48<00:11, 75.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  90% 8180/9047 [01:48<00:11, 75.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  91% 8200/9047 [01:48<00:11, 75.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  91% 8220/9047 [01:49<00:10, 75.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  91% 8240/9047 [01:49<00:10, 75.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  91% 8260/9047 [01:49<00:10, 75.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  92% 8280/9047 [01:49<00:10, 75.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  92% 8300/9047 [01:49<00:09, 75.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  92% 8320/9047 [01:49<00:09, 75.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  92% 8340/9047 [01:49<00:09, 75.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  92% 8360/9047 [01:49<00:09, 76.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  93% 8380/9047 [01:50<00:08, 76.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  93% 8400/9047 [01:50<00:08, 76.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  93% 8420/9047 [01:50<00:08, 76.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  93% 8440/9047 [01:50<00:07, 76.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  94% 8460/9047 [01:50<00:07, 76.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  94% 8480/9047 [01:50<00:07, 76.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  94% 8500/9047 [01:50<00:07, 76.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  94% 8520/9047 [01:50<00:06, 76.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  94% 8540/9047 [01:51<00:06, 76.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  95% 8560/9047 [01:51<00:06, 76.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  95% 8580/9047 [01:51<00:06, 77.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  95% 8600/9047 [01:51<00:05, 77.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  95% 8620/9047 [01:51<00:05, 77.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  96% 8640/9047 [01:51<00:05, 77.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  96% 8660/9047 [01:51<00:04, 77.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  96% 8680/9047 [01:52<00:04, 77.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  96% 8700/9047 [01:52<00:04, 77.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  96% 8720/9047 [01:52<00:04, 77.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  97% 8740/9047 [01:52<00:03, 77.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  97% 8760/9047 [01:52<00:03, 77.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  97% 8780/9047 [01:52<00:03, 77.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  97% 8800/9047 [01:52<00:03, 78.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  97% 8820/9047 [01:52<00:02, 78.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  98% 8840/9047 [01:53<00:02, 78.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  98% 8860/9047 [01:53<00:02, 78.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  98% 8880/9047 [01:53<00:02, 78.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  98% 8900/9047 [01:53<00:01, 78.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  99% 8920/9047 [01:53<00:01, 78.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  99% 8940/9047 [01:53<00:01, 78.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  99% 8960/9047 [01:53<00:01, 78.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  99% 8980/9047 [01:54<00:00, 78.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33:  99% 9000/9047 [01:54<00:00, 78.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33: 100% 9020/9047 [01:54<00:00, 78.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33: 100% 9040/9047 [01:54<00:00, 79.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 33: 100% 9047/9047 [01:54<00:00, 79.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  80% 7220/9047 [01:38<00:24, 73.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  80% 7240/9047 [01:43<00:25, 70.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  80% 7260/9047 [01:43<00:25, 70.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  80% 7280/9047 [01:43<00:25, 70.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  81% 7300/9047 [01:43<00:24, 70.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  81% 7320/9047 [01:43<00:24, 70.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  81% 7340/9047 [01:44<00:24, 70.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  81% 7360/9047 [01:44<00:23, 70.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  82% 7380/9047 [01:44<00:23, 70.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  82% 7400/9047 [01:44<00:23, 70.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  82% 7420/9047 [01:44<00:22, 70.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  82% 7440/9047 [01:44<00:22, 71.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  82% 7460/9047 [01:44<00:22, 71.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  83% 7480/9047 [01:44<00:21, 71.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  83% 7500/9047 [01:45<00:21, 71.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  83% 7520/9047 [01:45<00:21, 71.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  83% 7540/9047 [01:45<00:21, 71.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  84% 7560/9047 [01:45<00:20, 71.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  84% 7580/9047 [01:45<00:20, 71.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  84% 7600/9047 [01:45<00:20, 71.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  84% 7620/9047 [01:45<00:19, 71.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  84% 7640/9047 [01:46<00:19, 72.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  85% 7660/9047 [01:46<00:19, 72.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  85% 7680/9047 [01:46<00:18, 72.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  85% 7700/9047 [01:46<00:18, 72.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  85% 7720/9047 [01:46<00:18, 72.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  86% 7740/9047 [01:46<00:18, 72.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  86% 7760/9047 [01:46<00:17, 72.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  86% 7780/9047 [01:47<00:17, 72.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  86% 7800/9047 [01:47<00:17, 72.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  86% 7820/9047 [01:47<00:16, 72.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  87% 7840/9047 [01:47<00:16, 72.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  87% 7860/9047 [01:47<00:16, 73.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  87% 7880/9047 [01:47<00:15, 73.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  87% 7900/9047 [01:47<00:15, 73.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  88% 7920/9047 [01:47<00:15, 73.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  88% 7940/9047 [01:48<00:15, 73.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  88% 7960/9047 [01:48<00:14, 73.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  88% 7980/9047 [01:48<00:14, 73.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  88% 8000/9047 [01:48<00:14, 73.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  89% 8020/9047 [01:48<00:13, 73.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  89% 8040/9047 [01:48<00:13, 73.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  89% 8060/9047 [01:48<00:13, 73.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  89% 8080/9047 [01:49<00:13, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  90% 8100/9047 [01:49<00:12, 74.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  90% 8120/9047 [01:49<00:12, 74.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  90% 8140/9047 [01:49<00:12, 74.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  90% 8160/9047 [01:49<00:11, 74.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  90% 8180/9047 [01:49<00:11, 74.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  91% 8200/9047 [01:49<00:11, 74.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  91% 8220/9047 [01:50<00:11, 74.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  91% 8240/9047 [01:50<00:10, 74.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  91% 8260/9047 [01:50<00:10, 74.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  92% 8280/9047 [01:50<00:10, 74.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  92% 8300/9047 [01:50<00:09, 75.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  92% 8320/9047 [01:50<00:09, 75.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  92% 8340/9047 [01:50<00:09, 75.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  92% 8360/9047 [01:51<00:09, 75.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  93% 8380/9047 [01:51<00:08, 75.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  93% 8400/9047 [01:51<00:08, 75.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  93% 8420/9047 [01:51<00:08, 75.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  93% 8440/9047 [01:51<00:08, 75.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  94% 8460/9047 [01:51<00:07, 75.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  94% 8480/9047 [01:51<00:07, 75.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  94% 8500/9047 [01:51<00:07, 75.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  94% 8520/9047 [01:52<00:06, 76.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  94% 8540/9047 [01:52<00:06, 76.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  95% 8560/9047 [01:52<00:06, 76.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  95% 8580/9047 [01:52<00:06, 76.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  95% 8600/9047 [01:52<00:05, 76.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  95% 8620/9047 [01:52<00:05, 76.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  96% 8640/9047 [01:52<00:05, 76.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  96% 8660/9047 [01:52<00:05, 76.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  96% 8680/9047 [01:53<00:04, 76.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  96% 8700/9047 [01:53<00:04, 76.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  96% 8720/9047 [01:53<00:04, 76.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  97% 8740/9047 [01:53<00:03, 77.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  97% 8760/9047 [01:53<00:03, 77.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  97% 8780/9047 [01:53<00:03, 77.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  97% 8800/9047 [01:53<00:03, 77.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  97% 8820/9047 [01:53<00:02, 77.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  98% 8840/9047 [01:54<00:02, 77.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  98% 8860/9047 [01:54<00:02, 77.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  98% 8880/9047 [01:54<00:02, 77.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  98% 8900/9047 [01:54<00:01, 77.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  99% 8920/9047 [01:54<00:01, 77.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  99% 8940/9047 [01:54<00:01, 77.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  99% 8960/9047 [01:54<00:01, 78.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  99% 8980/9047 [01:54<00:00, 78.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34:  99% 9000/9047 [01:55<00:00, 78.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34: 100% 9020/9047 [01:55<00:00, 78.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34: 100% 9040/9047 [01:55<00:00, 78.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 34: 100% 9047/9047 [01:55<00:00, 78.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.061\n",
            "Epoch 35:  80% 7220/9047 [01:37<00:24, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  80% 7240/9047 [01:42<00:25, 70.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  80% 7260/9047 [01:42<00:25, 70.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  80% 7280/9047 [01:42<00:24, 70.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  81% 7300/9047 [01:43<00:24, 70.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  81% 7320/9047 [01:43<00:24, 70.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  81% 7340/9047 [01:43<00:24, 71.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  81% 7360/9047 [01:43<00:23, 71.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  82% 7380/9047 [01:43<00:23, 71.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  82% 7400/9047 [01:43<00:23, 71.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  82% 7420/9047 [01:43<00:22, 71.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  82% 7440/9047 [01:43<00:22, 71.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  82% 7460/9047 [01:44<00:22, 71.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  83% 7480/9047 [01:44<00:21, 71.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  83% 7500/9047 [01:44<00:21, 71.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  83% 7520/9047 [01:44<00:21, 71.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  83% 7540/9047 [01:44<00:20, 72.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  84% 7560/9047 [01:44<00:20, 72.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  84% 7580/9047 [01:44<00:20, 72.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  84% 7600/9047 [01:45<00:19, 72.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  84% 7620/9047 [01:45<00:19, 72.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  84% 7640/9047 [01:45<00:19, 72.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  85% 7660/9047 [01:45<00:19, 72.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  85% 7680/9047 [01:45<00:18, 72.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  85% 7700/9047 [01:45<00:18, 72.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  85% 7720/9047 [01:45<00:18, 72.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  86% 7740/9047 [01:45<00:17, 73.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  86% 7760/9047 [01:46<00:17, 73.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  86% 7780/9047 [01:46<00:17, 73.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  86% 7800/9047 [01:46<00:17, 73.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  86% 7820/9047 [01:46<00:16, 73.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  87% 7840/9047 [01:46<00:16, 73.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  87% 7860/9047 [01:46<00:16, 73.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  87% 7880/9047 [01:46<00:15, 73.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  87% 7900/9047 [01:46<00:15, 73.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  88% 7920/9047 [01:47<00:15, 73.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  88% 7940/9047 [01:47<00:14, 74.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  88% 7960/9047 [01:47<00:14, 74.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  88% 7980/9047 [01:47<00:14, 74.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  88% 8000/9047 [01:47<00:14, 74.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  89% 8020/9047 [01:47<00:13, 74.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  89% 8040/9047 [01:47<00:13, 74.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  89% 8060/9047 [01:48<00:13, 74.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  89% 8080/9047 [01:48<00:12, 74.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  90% 8100/9047 [01:48<00:12, 74.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  90% 8120/9047 [01:48<00:12, 74.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  90% 8140/9047 [01:48<00:12, 74.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  90% 8160/9047 [01:48<00:11, 75.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  90% 8180/9047 [01:48<00:11, 75.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  91% 8200/9047 [01:49<00:11, 75.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  91% 8220/9047 [01:49<00:10, 75.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  91% 8240/9047 [01:49<00:10, 75.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  91% 8260/9047 [01:49<00:10, 75.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  92% 8280/9047 [01:49<00:10, 75.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  92% 8300/9047 [01:49<00:09, 75.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  92% 8320/9047 [01:49<00:09, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  92% 8340/9047 [01:49<00:09, 75.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  92% 8360/9047 [01:50<00:09, 75.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  93% 8380/9047 [01:50<00:08, 76.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  93% 8400/9047 [01:50<00:08, 76.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  93% 8420/9047 [01:50<00:08, 76.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  93% 8440/9047 [01:50<00:07, 76.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  94% 8460/9047 [01:50<00:07, 76.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  94% 8480/9047 [01:50<00:07, 76.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  94% 8500/9047 [01:50<00:07, 76.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  94% 8520/9047 [01:51<00:06, 76.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  94% 8540/9047 [01:51<00:06, 76.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  95% 8560/9047 [01:51<00:06, 76.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  95% 8580/9047 [01:51<00:06, 76.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  95% 8600/9047 [01:51<00:05, 77.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  95% 8620/9047 [01:51<00:05, 77.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  96% 8640/9047 [01:51<00:05, 77.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  96% 8660/9047 [01:51<00:05, 77.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  96% 8680/9047 [01:52<00:04, 77.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  96% 8700/9047 [01:52<00:04, 77.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  96% 8720/9047 [01:52<00:04, 77.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  97% 8740/9047 [01:52<00:03, 77.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  97% 8760/9047 [01:52<00:03, 77.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  97% 8780/9047 [01:52<00:03, 77.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  97% 8800/9047 [01:52<00:03, 77.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  97% 8820/9047 [01:53<00:02, 78.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  98% 8840/9047 [01:53<00:02, 78.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  98% 8860/9047 [01:53<00:02, 78.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  98% 8880/9047 [01:53<00:02, 78.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  98% 8900/9047 [01:53<00:01, 78.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  99% 8920/9047 [01:53<00:01, 78.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  99% 8940/9047 [01:53<00:01, 78.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  99% 8960/9047 [01:54<00:01, 78.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  99% 8980/9047 [01:54<00:00, 78.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35:  99% 9000/9047 [01:54<00:00, 78.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35: 100% 9020/9047 [01:54<00:00, 78.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35: 100% 9040/9047 [01:54<00:00, 78.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 35: 100% 9047/9047 [01:54<00:00, 78.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  80% 7220/9047 [01:37<00:24, 73.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  80% 7240/9047 [01:42<00:25, 70.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  80% 7260/9047 [01:42<00:25, 70.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  80% 7280/9047 [01:42<00:24, 70.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  81% 7300/9047 [01:43<00:24, 70.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  81% 7320/9047 [01:43<00:24, 70.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  81% 7340/9047 [01:43<00:24, 71.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  81% 7360/9047 [01:43<00:23, 71.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  82% 7380/9047 [01:43<00:23, 71.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  82% 7400/9047 [01:43<00:23, 71.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  82% 7420/9047 [01:43<00:22, 71.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  82% 7440/9047 [01:44<00:22, 71.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  82% 7460/9047 [01:44<00:22, 71.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  83% 7480/9047 [01:44<00:21, 71.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  83% 7500/9047 [01:44<00:21, 71.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  83% 7520/9047 [01:44<00:21, 71.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  83% 7540/9047 [01:44<00:20, 72.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  84% 7560/9047 [01:44<00:20, 72.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  84% 7580/9047 [01:44<00:20, 72.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  84% 7600/9047 [01:45<00:20, 72.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  84% 7620/9047 [01:45<00:19, 72.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  84% 7640/9047 [01:45<00:19, 72.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  85% 7660/9047 [01:45<00:19, 72.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  85% 7680/9047 [01:45<00:18, 72.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  85% 7700/9047 [01:45<00:18, 72.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  85% 7720/9047 [01:45<00:18, 72.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  86% 7740/9047 [01:45<00:17, 73.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  86% 7760/9047 [01:46<00:17, 73.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  86% 7780/9047 [01:46<00:17, 73.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  86% 7800/9047 [01:46<00:17, 73.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  86% 7820/9047 [01:46<00:16, 73.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  87% 7840/9047 [01:46<00:16, 73.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  87% 7860/9047 [01:46<00:16, 73.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  87% 7880/9047 [01:46<00:15, 73.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  87% 7900/9047 [01:47<00:15, 73.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  88% 7920/9047 [01:47<00:15, 73.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  88% 7940/9047 [01:47<00:14, 73.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  88% 7960/9047 [01:47<00:14, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  88% 7980/9047 [01:47<00:14, 74.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  88% 8000/9047 [01:47<00:14, 74.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  89% 8020/9047 [01:47<00:13, 74.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  89% 8040/9047 [01:47<00:13, 74.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  89% 8060/9047 [01:48<00:13, 74.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  89% 8080/9047 [01:48<00:12, 74.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  90% 8100/9047 [01:48<00:12, 74.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  90% 8120/9047 [01:48<00:12, 74.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  90% 8140/9047 [01:48<00:12, 74.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  90% 8160/9047 [01:48<00:11, 75.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  90% 8180/9047 [01:48<00:11, 75.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  91% 8200/9047 [01:49<00:11, 75.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  91% 8220/9047 [01:49<00:10, 75.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  91% 8240/9047 [01:49<00:10, 75.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  91% 8260/9047 [01:49<00:10, 75.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  92% 8280/9047 [01:49<00:10, 75.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  92% 8300/9047 [01:49<00:09, 75.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  92% 8320/9047 [01:49<00:09, 75.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  92% 8340/9047 [01:49<00:09, 75.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  92% 8360/9047 [01:50<00:09, 75.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  93% 8380/9047 [01:50<00:08, 76.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  93% 8400/9047 [01:50<00:08, 76.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  93% 8420/9047 [01:50<00:08, 76.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  93% 8440/9047 [01:50<00:07, 76.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  94% 8460/9047 [01:50<00:07, 76.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  94% 8480/9047 [01:50<00:07, 76.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  94% 8500/9047 [01:51<00:07, 76.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  94% 8520/9047 [01:51<00:06, 76.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  94% 8540/9047 [01:51<00:06, 76.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  95% 8560/9047 [01:51<00:06, 76.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  95% 8580/9047 [01:51<00:06, 76.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  95% 8600/9047 [01:51<00:05, 77.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  95% 8620/9047 [01:51<00:05, 77.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  96% 8640/9047 [01:51<00:05, 77.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  96% 8660/9047 [01:52<00:05, 77.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  96% 8680/9047 [01:52<00:04, 77.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  96% 8700/9047 [01:52<00:04, 77.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  96% 8720/9047 [01:52<00:04, 77.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  97% 8740/9047 [01:52<00:03, 77.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  97% 8760/9047 [01:52<00:03, 77.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  97% 8780/9047 [01:52<00:03, 77.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  97% 8800/9047 [01:53<00:03, 77.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  97% 8820/9047 [01:53<00:02, 77.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  98% 8840/9047 [01:53<00:02, 78.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  98% 8860/9047 [01:53<00:02, 78.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  98% 8880/9047 [01:53<00:02, 78.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  98% 8900/9047 [01:53<00:01, 78.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  99% 8920/9047 [01:53<00:01, 78.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  99% 8940/9047 [01:53<00:01, 78.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  99% 8960/9047 [01:54<00:01, 78.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  99% 8980/9047 [01:54<00:00, 78.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36:  99% 9000/9047 [01:54<00:00, 78.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36: 100% 9020/9047 [01:54<00:00, 78.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36: 100% 9040/9047 [01:54<00:00, 78.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 36: 100% 9047/9047 [01:54<00:00, 78.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  80% 7220/9047 [01:37<00:24, 74.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  80% 7240/9047 [01:42<00:25, 70.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  80% 7260/9047 [01:42<00:25, 70.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  80% 7280/9047 [01:42<00:24, 70.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  81% 7300/9047 [01:43<00:24, 70.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  81% 7320/9047 [01:43<00:24, 70.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  81% 7340/9047 [01:43<00:24, 71.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  81% 7360/9047 [01:43<00:23, 71.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  82% 7380/9047 [01:43<00:23, 71.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  82% 7400/9047 [01:43<00:23, 71.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  82% 7420/9047 [01:43<00:22, 71.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  82% 7440/9047 [01:44<00:22, 71.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  82% 7460/9047 [01:44<00:22, 71.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  83% 7480/9047 [01:44<00:21, 71.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  83% 7500/9047 [01:44<00:21, 71.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  83% 7520/9047 [01:44<00:21, 71.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  83% 7540/9047 [01:44<00:20, 72.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  84% 7560/9047 [01:44<00:20, 72.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  84% 7580/9047 [01:44<00:20, 72.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  84% 7600/9047 [01:45<00:20, 72.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  84% 7620/9047 [01:45<00:19, 72.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  84% 7640/9047 [01:45<00:19, 72.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  85% 7660/9047 [01:45<00:19, 72.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  85% 7680/9047 [01:45<00:18, 72.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  85% 7700/9047 [01:45<00:18, 72.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  85% 7720/9047 [01:45<00:18, 72.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  86% 7740/9047 [01:46<00:17, 73.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  86% 7760/9047 [01:46<00:17, 73.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  86% 7780/9047 [01:46<00:17, 73.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  86% 7800/9047 [01:46<00:17, 73.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  86% 7820/9047 [01:46<00:16, 73.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  87% 7840/9047 [01:46<00:16, 73.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  87% 7860/9047 [01:46<00:16, 73.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  87% 7880/9047 [01:46<00:15, 73.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  87% 7900/9047 [01:47<00:15, 73.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  88% 7920/9047 [01:47<00:15, 73.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  88% 7940/9047 [01:47<00:14, 73.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  88% 7960/9047 [01:47<00:14, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  88% 7980/9047 [01:47<00:14, 74.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  88% 8000/9047 [01:47<00:14, 74.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  89% 8020/9047 [01:47<00:13, 74.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  89% 8040/9047 [01:47<00:13, 74.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  89% 8060/9047 [01:48<00:13, 74.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  89% 8080/9047 [01:48<00:12, 74.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  90% 8100/9047 [01:48<00:12, 74.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  90% 8120/9047 [01:48<00:12, 74.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  90% 8140/9047 [01:48<00:12, 74.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  90% 8160/9047 [01:48<00:11, 75.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  90% 8180/9047 [01:48<00:11, 75.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  91% 8200/9047 [01:49<00:11, 75.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  91% 8220/9047 [01:49<00:10, 75.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  91% 8240/9047 [01:49<00:10, 75.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  91% 8260/9047 [01:49<00:10, 75.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  92% 8280/9047 [01:49<00:10, 75.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  92% 8300/9047 [01:49<00:09, 75.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  92% 8320/9047 [01:49<00:09, 75.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  92% 8340/9047 [01:49<00:09, 75.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  92% 8360/9047 [01:50<00:09, 75.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  93% 8380/9047 [01:50<00:08, 76.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  93% 8400/9047 [01:50<00:08, 76.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  93% 8420/9047 [01:50<00:08, 76.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  93% 8440/9047 [01:50<00:07, 76.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  94% 8460/9047 [01:50<00:07, 76.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  94% 8480/9047 [01:50<00:07, 76.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  94% 8500/9047 [01:50<00:07, 76.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  94% 8520/9047 [01:51<00:06, 76.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  94% 8540/9047 [01:51<00:06, 76.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  95% 8560/9047 [01:51<00:06, 76.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  95% 8580/9047 [01:51<00:06, 76.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  95% 8600/9047 [01:51<00:05, 77.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  95% 8620/9047 [01:51<00:05, 77.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  96% 8640/9047 [01:51<00:05, 77.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  96% 8660/9047 [01:52<00:05, 77.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  96% 8680/9047 [01:52<00:04, 77.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  96% 8700/9047 [01:52<00:04, 77.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  96% 8720/9047 [01:52<00:04, 77.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  97% 8740/9047 [01:52<00:03, 77.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  97% 8760/9047 [01:52<00:03, 77.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  97% 8780/9047 [01:52<00:03, 77.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  97% 8800/9047 [01:53<00:03, 77.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  97% 8820/9047 [01:53<00:02, 77.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  98% 8840/9047 [01:53<00:02, 78.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  98% 8860/9047 [01:53<00:02, 78.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  98% 8880/9047 [01:53<00:02, 78.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  98% 8900/9047 [01:53<00:01, 78.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  99% 8920/9047 [01:53<00:01, 78.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  99% 8940/9047 [01:53<00:01, 78.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  99% 8960/9047 [01:54<00:01, 78.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  99% 8980/9047 [01:54<00:00, 78.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37:  99% 9000/9047 [01:54<00:00, 78.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37: 100% 9020/9047 [01:54<00:00, 78.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37: 100% 9040/9047 [01:54<00:00, 78.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 37: 100% 9047/9047 [01:54<00:00, 78.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  80% 7220/9047 [01:38<00:24, 73.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  80% 7240/9047 [01:43<00:25, 70.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  80% 7260/9047 [01:43<00:25, 70.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  80% 7280/9047 [01:43<00:25, 70.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  81% 7300/9047 [01:43<00:24, 70.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  81% 7320/9047 [01:43<00:24, 70.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  81% 7340/9047 [01:43<00:24, 70.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  81% 7360/9047 [01:44<00:23, 70.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  82% 7380/9047 [01:44<00:23, 70.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  82% 7400/9047 [01:44<00:23, 70.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  82% 7420/9047 [01:44<00:22, 71.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  82% 7440/9047 [01:44<00:22, 71.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  82% 7460/9047 [01:44<00:22, 71.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  83% 7480/9047 [01:44<00:21, 71.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  83% 7500/9047 [01:44<00:21, 71.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  83% 7520/9047 [01:45<00:21, 71.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  83% 7540/9047 [01:45<00:21, 71.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  84% 7560/9047 [01:45<00:20, 71.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  84% 7580/9047 [01:45<00:20, 71.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  84% 7600/9047 [01:45<00:20, 71.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  84% 7620/9047 [01:45<00:19, 72.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  84% 7640/9047 [01:45<00:19, 72.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  85% 7660/9047 [01:46<00:19, 72.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  85% 7680/9047 [01:46<00:18, 72.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  85% 7700/9047 [01:46<00:18, 72.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  85% 7720/9047 [01:46<00:18, 72.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  86% 7740/9047 [01:46<00:18, 72.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  86% 7760/9047 [01:46<00:17, 72.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  86% 7780/9047 [01:46<00:17, 72.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  86% 7800/9047 [01:47<00:17, 72.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  86% 7820/9047 [01:47<00:16, 72.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  87% 7840/9047 [01:47<00:16, 73.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  87% 7860/9047 [01:47<00:16, 73.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  87% 7880/9047 [01:47<00:15, 73.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  87% 7900/9047 [01:47<00:15, 73.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  88% 7920/9047 [01:47<00:15, 73.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  88% 7940/9047 [01:47<00:15, 73.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  88% 7960/9047 [01:48<00:14, 73.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  88% 7980/9047 [01:48<00:14, 73.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  88% 8000/9047 [01:48<00:14, 73.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  89% 8020/9047 [01:48<00:13, 73.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  89% 8040/9047 [01:48<00:13, 74.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  89% 8060/9047 [01:48<00:13, 74.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  89% 8080/9047 [01:48<00:13, 74.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  90% 8100/9047 [01:49<00:12, 74.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  90% 8120/9047 [01:49<00:12, 74.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  90% 8140/9047 [01:49<00:12, 74.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  90% 8160/9047 [01:49<00:11, 74.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  90% 8180/9047 [01:49<00:11, 74.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  91% 8200/9047 [01:49<00:11, 74.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  91% 8220/9047 [01:49<00:11, 74.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  91% 8240/9047 [01:49<00:10, 74.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  91% 8260/9047 [01:50<00:10, 75.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  92% 8280/9047 [01:50<00:10, 75.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  92% 8300/9047 [01:50<00:09, 75.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  92% 8320/9047 [01:50<00:09, 75.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  92% 8340/9047 [01:50<00:09, 75.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  92% 8360/9047 [01:50<00:09, 75.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  93% 8380/9047 [01:50<00:08, 75.62it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  93% 8400/9047 [01:50<00:08, 75.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  93% 8420/9047 [01:51<00:08, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  93% 8440/9047 [01:51<00:07, 75.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  94% 8460/9047 [01:51<00:07, 75.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  94% 8480/9047 [01:51<00:07, 76.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  94% 8500/9047 [01:51<00:07, 76.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  94% 8520/9047 [01:51<00:06, 76.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  94% 8540/9047 [01:51<00:06, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  95% 8560/9047 [01:52<00:06, 76.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  95% 8580/9047 [01:52<00:06, 76.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  95% 8600/9047 [01:52<00:05, 76.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  95% 8620/9047 [01:52<00:05, 76.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  96% 8640/9047 [01:52<00:05, 76.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  96% 8660/9047 [01:52<00:05, 76.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  96% 8680/9047 [01:52<00:04, 76.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  96% 8700/9047 [01:52<00:04, 77.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  96% 8720/9047 [01:53<00:04, 77.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  97% 8740/9047 [01:53<00:03, 77.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  97% 8760/9047 [01:53<00:03, 77.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  97% 8780/9047 [01:53<00:03, 77.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  97% 8800/9047 [01:53<00:03, 77.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  97% 8820/9047 [01:53<00:02, 77.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  98% 8840/9047 [01:53<00:02, 77.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  98% 8860/9047 [01:54<00:02, 77.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  98% 8880/9047 [01:54<00:02, 77.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  98% 8900/9047 [01:54<00:01, 77.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  99% 8920/9047 [01:54<00:01, 77.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  99% 8940/9047 [01:54<00:01, 78.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  99% 8960/9047 [01:54<00:01, 78.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  99% 8980/9047 [01:54<00:00, 78.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38:  99% 9000/9047 [01:54<00:00, 78.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38: 100% 9020/9047 [01:55<00:00, 78.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38: 100% 9040/9047 [01:55<00:00, 78.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 38: 100% 9047/9047 [01:55<00:00, 78.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  80% 7220/9047 [01:37<00:24, 74.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  80% 7240/9047 [01:42<00:25, 70.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  80% 7260/9047 [01:42<00:25, 70.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  80% 7280/9047 [01:42<00:24, 70.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  81% 7300/9047 [01:43<00:24, 70.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  81% 7320/9047 [01:43<00:24, 70.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  81% 7340/9047 [01:43<00:24, 71.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  81% 7360/9047 [01:43<00:23, 71.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  82% 7380/9047 [01:43<00:23, 71.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  82% 7400/9047 [01:43<00:23, 71.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  82% 7420/9047 [01:43<00:22, 71.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  82% 7440/9047 [01:44<00:22, 71.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  82% 7460/9047 [01:44<00:22, 71.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  83% 7480/9047 [01:44<00:21, 71.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  83% 7500/9047 [01:44<00:21, 71.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  83% 7520/9047 [01:44<00:21, 71.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  83% 7540/9047 [01:44<00:20, 72.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  84% 7560/9047 [01:44<00:20, 72.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  84% 7580/9047 [01:44<00:20, 72.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  84% 7600/9047 [01:45<00:20, 72.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  84% 7620/9047 [01:45<00:19, 72.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  84% 7640/9047 [01:45<00:19, 72.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  85% 7660/9047 [01:45<00:19, 72.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  85% 7680/9047 [01:45<00:18, 72.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  85% 7700/9047 [01:45<00:18, 72.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  85% 7720/9047 [01:45<00:18, 72.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  86% 7740/9047 [01:46<00:17, 72.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  86% 7760/9047 [01:46<00:17, 73.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  86% 7780/9047 [01:46<00:17, 73.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  86% 7800/9047 [01:46<00:17, 73.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  86% 7820/9047 [01:46<00:16, 73.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  87% 7840/9047 [01:46<00:16, 73.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  87% 7860/9047 [01:46<00:16, 73.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  87% 7880/9047 [01:47<00:15, 73.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  87% 7900/9047 [01:47<00:15, 73.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  88% 7920/9047 [01:47<00:15, 73.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  88% 7940/9047 [01:47<00:14, 73.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  88% 7960/9047 [01:47<00:14, 74.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  88% 7980/9047 [01:47<00:14, 74.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  88% 8000/9047 [01:47<00:14, 74.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  89% 8020/9047 [01:47<00:13, 74.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  89% 8040/9047 [01:48<00:13, 74.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  89% 8060/9047 [01:48<00:13, 74.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  89% 8080/9047 [01:48<00:12, 74.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  90% 8100/9047 [01:48<00:12, 74.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  90% 8120/9047 [01:48<00:12, 74.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  90% 8140/9047 [01:48<00:12, 74.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  90% 8160/9047 [01:48<00:11, 74.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  90% 8180/9047 [01:49<00:11, 75.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  91% 8200/9047 [01:49<00:11, 75.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  91% 8220/9047 [01:49<00:11, 75.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  91% 8240/9047 [01:49<00:10, 75.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  91% 8260/9047 [01:49<00:10, 75.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  92% 8280/9047 [01:49<00:10, 75.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  92% 8300/9047 [01:49<00:09, 75.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  92% 8320/9047 [01:50<00:09, 75.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  92% 8340/9047 [01:50<00:09, 75.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  92% 8360/9047 [01:50<00:09, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  93% 8380/9047 [01:50<00:08, 75.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  93% 8400/9047 [01:50<00:08, 75.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  93% 8420/9047 [01:50<00:08, 76.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  93% 8440/9047 [01:50<00:07, 76.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  94% 8460/9047 [01:50<00:07, 76.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  94% 8480/9047 [01:51<00:07, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  94% 8500/9047 [01:51<00:07, 76.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  94% 8520/9047 [01:51<00:06, 76.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  94% 8540/9047 [01:51<00:06, 76.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  95% 8560/9047 [01:51<00:06, 76.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  95% 8580/9047 [01:51<00:06, 76.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  95% 8600/9047 [01:51<00:05, 76.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  95% 8620/9047 [01:52<00:05, 76.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  96% 8640/9047 [01:52<00:05, 77.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  96% 8660/9047 [01:52<00:05, 77.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  96% 8680/9047 [01:52<00:04, 77.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  96% 8700/9047 [01:52<00:04, 77.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  96% 8720/9047 [01:52<00:04, 77.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  97% 8740/9047 [01:52<00:03, 77.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  97% 8760/9047 [01:53<00:03, 77.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  97% 8780/9047 [01:53<00:03, 77.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  97% 8800/9047 [01:53<00:03, 77.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  97% 8820/9047 [01:53<00:02, 77.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  98% 8840/9047 [01:53<00:02, 77.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  98% 8860/9047 [01:53<00:02, 77.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  98% 8880/9047 [01:53<00:02, 78.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  98% 8900/9047 [01:53<00:01, 78.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  99% 8920/9047 [01:54<00:01, 78.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  99% 8940/9047 [01:54<00:01, 78.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  99% 8960/9047 [01:54<00:01, 78.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  99% 8980/9047 [01:54<00:00, 78.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39:  99% 9000/9047 [01:54<00:00, 78.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39: 100% 9020/9047 [01:54<00:00, 78.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39: 100% 9040/9047 [01:54<00:00, 78.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 39: 100% 9047/9047 [01:54<00:00, 78.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  80% 7220/9047 [01:37<00:24, 73.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  80% 7240/9047 [01:43<00:25, 70.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  80% 7260/9047 [01:43<00:25, 70.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  80% 7280/9047 [01:43<00:25, 70.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  81% 7300/9047 [01:43<00:24, 70.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  81% 7320/9047 [01:43<00:24, 70.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  81% 7340/9047 [01:43<00:24, 70.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  81% 7360/9047 [01:44<00:23, 70.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  82% 7380/9047 [01:44<00:23, 70.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  82% 7400/9047 [01:44<00:23, 70.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  82% 7420/9047 [01:44<00:22, 71.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  82% 7440/9047 [01:44<00:22, 71.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  82% 7460/9047 [01:44<00:22, 71.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  83% 7480/9047 [01:44<00:21, 71.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  83% 7500/9047 [01:44<00:21, 71.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  83% 7520/9047 [01:45<00:21, 71.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  83% 7540/9047 [01:45<00:21, 71.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  84% 7560/9047 [01:45<00:20, 71.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  84% 7580/9047 [01:45<00:20, 71.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  84% 7600/9047 [01:45<00:20, 71.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  84% 7620/9047 [01:45<00:19, 72.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  84% 7640/9047 [01:45<00:19, 72.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  85% 7660/9047 [01:46<00:19, 72.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  85% 7680/9047 [01:46<00:18, 72.34it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  85% 7700/9047 [01:46<00:18, 72.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  85% 7720/9047 [01:46<00:18, 72.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  86% 7740/9047 [01:46<00:17, 72.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  86% 7760/9047 [01:46<00:17, 72.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  86% 7780/9047 [01:46<00:17, 72.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  86% 7800/9047 [01:46<00:17, 72.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  86% 7820/9047 [01:47<00:16, 73.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  87% 7840/9047 [01:47<00:16, 73.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  87% 7860/9047 [01:47<00:16, 73.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  87% 7880/9047 [01:47<00:15, 73.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  87% 7900/9047 [01:47<00:15, 73.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  88% 7920/9047 [01:47<00:15, 73.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  88% 7940/9047 [01:47<00:15, 73.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  88% 7960/9047 [01:48<00:14, 73.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  88% 7980/9047 [01:48<00:14, 73.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  88% 8000/9047 [01:48<00:14, 73.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  89% 8020/9047 [01:48<00:13, 73.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  89% 8040/9047 [01:48<00:13, 74.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  89% 8060/9047 [01:48<00:13, 74.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  89% 8080/9047 [01:48<00:13, 74.27it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  90% 8100/9047 [01:48<00:12, 74.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  90% 8120/9047 [01:49<00:12, 74.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  90% 8140/9047 [01:49<00:12, 74.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  90% 8160/9047 [01:49<00:11, 74.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  90% 8180/9047 [01:49<00:11, 74.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  91% 8200/9047 [01:49<00:11, 74.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  91% 8220/9047 [01:49<00:11, 74.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  91% 8240/9047 [01:49<00:10, 75.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  91% 8260/9047 [01:49<00:10, 75.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  92% 8280/9047 [01:50<00:10, 75.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  92% 8300/9047 [01:50<00:09, 75.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  92% 8320/9047 [01:50<00:09, 75.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  92% 8340/9047 [01:50<00:09, 75.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  92% 8360/9047 [01:50<00:09, 75.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  93% 8380/9047 [01:50<00:08, 75.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  93% 8400/9047 [01:50<00:08, 75.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  93% 8420/9047 [01:50<00:08, 75.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  93% 8440/9047 [01:51<00:07, 75.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  94% 8460/9047 [01:51<00:07, 76.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  94% 8480/9047 [01:51<00:07, 76.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  94% 8500/9047 [01:51<00:07, 76.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  94% 8520/9047 [01:51<00:06, 76.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  94% 8540/9047 [01:51<00:06, 76.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  95% 8560/9047 [01:51<00:06, 76.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  95% 8580/9047 [01:51<00:06, 76.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  95% 8600/9047 [01:52<00:05, 76.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  95% 8620/9047 [01:52<00:05, 76.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  96% 8640/9047 [01:52<00:05, 76.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  96% 8660/9047 [01:52<00:05, 76.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  96% 8680/9047 [01:52<00:04, 77.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  96% 8700/9047 [01:52<00:04, 77.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  96% 8720/9047 [01:52<00:04, 77.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  97% 8740/9047 [01:52<00:03, 77.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  97% 8760/9047 [01:53<00:03, 77.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  97% 8780/9047 [01:53<00:03, 77.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  97% 8800/9047 [01:53<00:03, 77.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  97% 8820/9047 [01:53<00:02, 77.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  98% 8840/9047 [01:53<00:02, 77.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  98% 8860/9047 [01:53<00:02, 77.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  98% 8880/9047 [01:53<00:02, 77.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  98% 8900/9047 [01:54<00:01, 78.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  99% 8920/9047 [01:54<00:01, 78.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  99% 8940/9047 [01:54<00:01, 78.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  99% 8960/9047 [01:54<00:01, 78.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  99% 8980/9047 [01:54<00:00, 78.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40:  99% 9000/9047 [01:54<00:00, 78.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40: 100% 9020/9047 [01:54<00:00, 78.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40: 100% 9040/9047 [01:55<00:00, 78.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 40: 100% 9047/9047 [01:55<00:00, 78.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  80% 7220/9047 [01:37<00:24, 73.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  80% 7240/9047 [01:42<00:25, 70.35it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  80% 7260/9047 [01:43<00:25, 70.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  80% 7280/9047 [01:43<00:25, 70.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  81% 7300/9047 [01:43<00:24, 70.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  81% 7320/9047 [01:43<00:24, 70.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  81% 7340/9047 [01:43<00:24, 70.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  81% 7360/9047 [01:43<00:23, 70.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  82% 7380/9047 [01:43<00:23, 71.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  82% 7400/9047 [01:43<00:23, 71.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  82% 7420/9047 [01:44<00:22, 71.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  82% 7440/9047 [01:44<00:22, 71.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  82% 7460/9047 [01:44<00:22, 71.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  83% 7480/9047 [01:44<00:21, 71.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  83% 7500/9047 [01:44<00:21, 71.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  83% 7520/9047 [01:44<00:21, 71.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  83% 7540/9047 [01:44<00:20, 71.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  84% 7560/9047 [01:45<00:20, 71.98it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  84% 7580/9047 [01:45<00:20, 72.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  84% 7600/9047 [01:45<00:20, 72.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  84% 7620/9047 [01:45<00:19, 72.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  84% 7640/9047 [01:45<00:19, 72.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  85% 7660/9047 [01:45<00:19, 72.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  85% 7680/9047 [01:45<00:18, 72.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  85% 7700/9047 [01:45<00:18, 72.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  85% 7720/9047 [01:46<00:18, 72.79it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  86% 7740/9047 [01:46<00:17, 72.89it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  86% 7760/9047 [01:46<00:17, 72.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  86% 7780/9047 [01:46<00:17, 73.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  86% 7800/9047 [01:46<00:17, 73.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  86% 7820/9047 [01:46<00:16, 73.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  87% 7840/9047 [01:46<00:16, 73.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  87% 7860/9047 [01:46<00:16, 73.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  87% 7880/9047 [01:47<00:15, 73.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  87% 7900/9047 [01:47<00:15, 73.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  88% 7920/9047 [01:47<00:15, 73.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  88% 7940/9047 [01:47<00:14, 73.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  88% 7960/9047 [01:47<00:14, 73.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  88% 7980/9047 [01:47<00:14, 74.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  88% 8000/9047 [01:47<00:14, 74.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  89% 8020/9047 [01:48<00:13, 74.24it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  89% 8040/9047 [01:48<00:13, 74.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  89% 8060/9047 [01:48<00:13, 74.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  89% 8080/9047 [01:48<00:12, 74.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  90% 8100/9047 [01:48<00:12, 74.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  90% 8120/9047 [01:48<00:12, 74.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  90% 8140/9047 [01:48<00:12, 74.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  90% 8160/9047 [01:48<00:11, 74.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  90% 8180/9047 [01:49<00:11, 74.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  91% 8200/9047 [01:49<00:11, 75.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  91% 8220/9047 [01:49<00:11, 75.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  91% 8240/9047 [01:49<00:10, 75.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  91% 8260/9047 [01:49<00:10, 75.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  92% 8280/9047 [01:49<00:10, 75.47it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  92% 8300/9047 [01:49<00:09, 75.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  92% 8320/9047 [01:49<00:09, 75.65it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  92% 8340/9047 [01:50<00:09, 75.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  92% 8360/9047 [01:50<00:09, 75.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  93% 8380/9047 [01:50<00:08, 75.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  93% 8400/9047 [01:50<00:08, 76.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  93% 8420/9047 [01:50<00:08, 76.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  93% 8440/9047 [01:50<00:07, 76.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  94% 8460/9047 [01:50<00:07, 76.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  94% 8480/9047 [01:51<00:07, 76.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  94% 8500/9047 [01:51<00:07, 76.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  94% 8520/9047 [01:51<00:06, 76.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  94% 8540/9047 [01:51<00:06, 76.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  95% 8560/9047 [01:51<00:06, 76.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  95% 8580/9047 [01:51<00:06, 76.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  95% 8600/9047 [01:51<00:05, 76.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  95% 8620/9047 [01:51<00:05, 77.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  96% 8640/9047 [01:52<00:05, 77.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  96% 8660/9047 [01:52<00:05, 77.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  96% 8680/9047 [01:52<00:04, 77.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  96% 8700/9047 [01:52<00:04, 77.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  96% 8720/9047 [01:52<00:04, 77.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  97% 8740/9047 [01:52<00:03, 77.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  97% 8760/9047 [01:52<00:03, 77.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  97% 8780/9047 [01:52<00:03, 77.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  97% 8800/9047 [01:53<00:03, 77.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  97% 8820/9047 [01:53<00:02, 77.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  98% 8840/9047 [01:53<00:02, 78.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  98% 8860/9047 [01:53<00:02, 78.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  98% 8880/9047 [01:53<00:02, 78.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  98% 8900/9047 [01:53<00:01, 78.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  99% 8920/9047 [01:53<00:01, 78.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  99% 8940/9047 [01:53<00:01, 78.44it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  99% 8960/9047 [01:54<00:01, 78.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  99% 8980/9047 [01:54<00:00, 78.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41:  99% 9000/9047 [01:54<00:00, 78.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41: 100% 9020/9047 [01:54<00:00, 78.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41: 100% 9040/9047 [01:54<00:00, 78.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 41: 100% 9047/9047 [01:54<00:00, 78.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  80% 7220/9047 [01:38<00:24, 73.52it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  80% 7240/9047 [01:43<00:25, 70.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  80% 7260/9047 [01:43<00:25, 70.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  80% 7280/9047 [01:43<00:25, 70.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  81% 7300/9047 [01:43<00:24, 70.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  81% 7320/9047 [01:43<00:24, 70.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  81% 7340/9047 [01:44<00:24, 70.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  81% 7360/9047 [01:44<00:23, 70.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  82% 7380/9047 [01:44<00:23, 70.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  82% 7400/9047 [01:44<00:23, 70.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  82% 7420/9047 [01:44<00:22, 70.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  82% 7440/9047 [01:44<00:22, 71.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  82% 7460/9047 [01:44<00:22, 71.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  83% 7480/9047 [01:45<00:22, 71.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  83% 7500/9047 [01:45<00:21, 71.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  83% 7520/9047 [01:45<00:21, 71.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  83% 7540/9047 [01:45<00:21, 71.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  84% 7560/9047 [01:45<00:20, 71.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  84% 7580/9047 [01:45<00:20, 71.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  84% 7600/9047 [01:45<00:20, 71.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  84% 7620/9047 [01:45<00:19, 71.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  84% 7640/9047 [01:46<00:19, 72.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  85% 7660/9047 [01:46<00:19, 72.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  85% 7680/9047 [01:46<00:18, 72.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  85% 7700/9047 [01:46<00:18, 72.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  85% 7720/9047 [01:46<00:18, 72.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  86% 7740/9047 [01:46<00:18, 72.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  86% 7760/9047 [01:46<00:17, 72.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  86% 7780/9047 [01:47<00:17, 72.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  86% 7800/9047 [01:47<00:17, 72.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  86% 7820/9047 [01:47<00:16, 72.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  87% 7840/9047 [01:47<00:16, 72.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  87% 7860/9047 [01:47<00:16, 73.06it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  87% 7880/9047 [01:47<00:15, 73.16it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  87% 7900/9047 [01:47<00:15, 73.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  88% 7920/9047 [01:47<00:15, 73.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  88% 7940/9047 [01:48<00:15, 73.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  88% 7960/9047 [01:48<00:14, 73.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  88% 7980/9047 [01:48<00:14, 73.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  88% 8000/9047 [01:48<00:14, 73.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  89% 8020/9047 [01:48<00:13, 73.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  89% 8040/9047 [01:48<00:13, 73.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  89% 8060/9047 [01:48<00:13, 74.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  89% 8080/9047 [01:49<00:13, 74.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  90% 8100/9047 [01:49<00:12, 74.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  90% 8120/9047 [01:49<00:12, 74.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  90% 8140/9047 [01:49<00:12, 74.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  90% 8160/9047 [01:49<00:11, 74.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  90% 8180/9047 [01:49<00:11, 74.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  91% 8200/9047 [01:49<00:11, 74.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  91% 8220/9047 [01:49<00:11, 74.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  91% 8240/9047 [01:50<00:10, 74.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  91% 8260/9047 [01:50<00:10, 74.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  92% 8280/9047 [01:50<00:10, 75.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  92% 8300/9047 [01:50<00:09, 75.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  92% 8320/9047 [01:50<00:09, 75.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  92% 8340/9047 [01:50<00:09, 75.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  92% 8360/9047 [01:50<00:09, 75.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  93% 8380/9047 [01:51<00:08, 75.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  93% 8400/9047 [01:51<00:08, 75.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  93% 8420/9047 [01:51<00:08, 75.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  93% 8440/9047 [01:51<00:08, 75.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  94% 8460/9047 [01:51<00:07, 75.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  94% 8480/9047 [01:51<00:07, 75.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  94% 8500/9047 [01:51<00:07, 76.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  94% 8520/9047 [01:51<00:06, 76.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  94% 8540/9047 [01:52<00:06, 76.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  95% 8560/9047 [01:52<00:06, 76.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  95% 8580/9047 [01:52<00:06, 76.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  95% 8600/9047 [01:52<00:05, 76.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  95% 8620/9047 [01:52<00:05, 76.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  96% 8640/9047 [01:52<00:05, 76.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  96% 8660/9047 [01:52<00:05, 76.75it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  96% 8680/9047 [01:52<00:04, 76.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  96% 8700/9047 [01:53<00:04, 76.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  96% 8720/9047 [01:53<00:04, 77.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  97% 8740/9047 [01:53<00:03, 77.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  97% 8760/9047 [01:53<00:03, 77.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  97% 8780/9047 [01:53<00:03, 77.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  97% 8800/9047 [01:53<00:03, 77.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  97% 8820/9047 [01:53<00:02, 77.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  98% 8840/9047 [01:53<00:02, 77.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  98% 8860/9047 [01:54<00:02, 77.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  98% 8880/9047 [01:54<00:02, 77.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  98% 8900/9047 [01:54<00:01, 77.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  99% 8920/9047 [01:54<00:01, 77.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  99% 8940/9047 [01:54<00:01, 77.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  99% 8960/9047 [01:54<00:01, 78.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  99% 8980/9047 [01:54<00:00, 78.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42:  99% 9000/9047 [01:55<00:00, 78.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42: 100% 9020/9047 [01:55<00:00, 78.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42: 100% 9040/9047 [01:55<00:00, 78.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 42: 100% 9047/9047 [01:55<00:00, 78.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  80% 7220/9047 [01:37<00:24, 74.01it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  80% 7240/9047 [01:42<00:25, 70.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  80% 7260/9047 [01:42<00:25, 70.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  80% 7280/9047 [01:42<00:24, 70.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  81% 7300/9047 [01:43<00:24, 70.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  81% 7320/9047 [01:43<00:24, 70.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  81% 7340/9047 [01:43<00:24, 71.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  81% 7360/9047 [01:43<00:23, 71.15it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  82% 7380/9047 [01:43<00:23, 71.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  82% 7400/9047 [01:43<00:23, 71.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  82% 7420/9047 [01:43<00:22, 71.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  82% 7440/9047 [01:43<00:22, 71.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  82% 7460/9047 [01:44<00:22, 71.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  83% 7480/9047 [01:44<00:21, 71.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  83% 7500/9047 [01:44<00:21, 71.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  83% 7520/9047 [01:44<00:21, 71.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  83% 7540/9047 [01:44<00:20, 72.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  84% 7560/9047 [01:44<00:20, 72.18it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  84% 7580/9047 [01:44<00:20, 72.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  84% 7600/9047 [01:44<00:19, 72.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  84% 7620/9047 [01:45<00:19, 72.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  84% 7640/9047 [01:45<00:19, 72.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  85% 7660/9047 [01:45<00:19, 72.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  85% 7680/9047 [01:45<00:18, 72.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  85% 7700/9047 [01:45<00:18, 72.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  85% 7720/9047 [01:45<00:18, 72.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  86% 7740/9047 [01:45<00:17, 73.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  86% 7760/9047 [01:46<00:17, 73.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  86% 7780/9047 [01:46<00:17, 73.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  86% 7800/9047 [01:46<00:16, 73.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  86% 7820/9047 [01:46<00:16, 73.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  87% 7840/9047 [01:46<00:16, 73.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  87% 7860/9047 [01:46<00:16, 73.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  87% 7880/9047 [01:46<00:15, 73.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  87% 7900/9047 [01:46<00:15, 73.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  88% 7920/9047 [01:47<00:15, 73.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  88% 7940/9047 [01:47<00:14, 74.07it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  88% 7960/9047 [01:47<00:14, 74.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  88% 7980/9047 [01:47<00:14, 74.26it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  88% 8000/9047 [01:47<00:14, 74.36it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  89% 8020/9047 [01:47<00:13, 74.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  89% 8040/9047 [01:47<00:13, 74.54it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  89% 8060/9047 [01:47<00:13, 74.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  89% 8080/9047 [01:48<00:12, 74.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  90% 8100/9047 [01:48<00:12, 74.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  90% 8120/9047 [01:48<00:12, 74.92it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  90% 8140/9047 [01:48<00:12, 75.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  90% 8160/9047 [01:48<00:11, 75.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  90% 8180/9047 [01:48<00:11, 75.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  91% 8200/9047 [01:48<00:11, 75.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  91% 8220/9047 [01:49<00:10, 75.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  91% 8240/9047 [01:49<00:10, 75.49it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  91% 8260/9047 [01:49<00:10, 75.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  92% 8280/9047 [01:49<00:10, 75.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  92% 8300/9047 [01:49<00:09, 75.77it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  92% 8320/9047 [01:49<00:09, 75.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  92% 8340/9047 [01:49<00:09, 75.96it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  92% 8360/9047 [01:49<00:09, 76.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  93% 8380/9047 [01:50<00:08, 76.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  93% 8400/9047 [01:50<00:08, 76.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  93% 8420/9047 [01:50<00:08, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  93% 8440/9047 [01:50<00:07, 76.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  94% 8460/9047 [01:50<00:07, 76.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  94% 8480/9047 [01:50<00:07, 76.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  94% 8500/9047 [01:50<00:07, 76.68it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  94% 8520/9047 [01:50<00:06, 76.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  94% 8540/9047 [01:51<00:06, 76.84it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  95% 8560/9047 [01:51<00:06, 76.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  95% 8580/9047 [01:51<00:06, 77.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  95% 8600/9047 [01:51<00:05, 77.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  95% 8620/9047 [01:51<00:05, 77.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  96% 8640/9047 [01:51<00:05, 77.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  96% 8660/9047 [01:51<00:05, 77.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  96% 8680/9047 [01:52<00:04, 77.46it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  96% 8700/9047 [01:52<00:04, 77.55it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  96% 8720/9047 [01:52<00:04, 77.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  97% 8740/9047 [01:52<00:03, 77.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  97% 8760/9047 [01:52<00:03, 77.82it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  97% 8780/9047 [01:52<00:03, 77.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  97% 8800/9047 [01:52<00:03, 77.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  97% 8820/9047 [01:52<00:02, 78.08it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  98% 8840/9047 [01:53<00:02, 78.17it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  98% 8860/9047 [01:53<00:02, 78.25it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  98% 8880/9047 [01:53<00:02, 78.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  98% 8900/9047 [01:53<00:01, 78.42it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  99% 8920/9047 [01:53<00:01, 78.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  99% 8940/9047 [01:53<00:01, 78.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  99% 8960/9047 [01:53<00:01, 78.66it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  99% 8980/9047 [01:54<00:00, 78.74it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43:  99% 9000/9047 [01:54<00:00, 78.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43: 100% 9020/9047 [01:54<00:00, 78.91it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43: 100% 9040/9047 [01:54<00:00, 79.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 43: 100% 9047/9047 [01:54<00:00, 79.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  80% 7220/9047 [01:37<00:24, 73.72it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1810 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  80% 7240/9047 [01:43<00:25, 70.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  80% 7260/9047 [01:43<00:25, 70.22it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  80% 7280/9047 [01:43<00:25, 70.33it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  81% 7300/9047 [01:43<00:24, 70.43it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  81% 7320/9047 [01:43<00:24, 70.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  81% 7340/9047 [01:43<00:24, 70.63it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  81% 7360/9047 [01:44<00:23, 70.73it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  82% 7380/9047 [01:44<00:23, 70.83it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  82% 7400/9047 [01:44<00:23, 70.93it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  82% 7420/9047 [01:44<00:22, 71.02it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  82% 7440/9047 [01:44<00:22, 71.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  82% 7460/9047 [01:44<00:22, 71.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  83% 7480/9047 [01:44<00:21, 71.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  83% 7500/9047 [01:45<00:21, 71.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  83% 7520/9047 [01:45<00:21, 71.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  83% 7540/9047 [01:45<00:21, 71.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  84% 7560/9047 [01:45<00:20, 71.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  84% 7580/9047 [01:45<00:20, 71.81it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  84% 7600/9047 [01:45<00:20, 71.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  84% 7620/9047 [01:45<00:19, 72.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  84% 7640/9047 [01:45<00:19, 72.10it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  85% 7660/9047 [01:46<00:19, 72.20it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  85% 7680/9047 [01:46<00:18, 72.30it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  85% 7700/9047 [01:46<00:18, 72.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  85% 7720/9047 [01:46<00:18, 72.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  86% 7740/9047 [01:46<00:18, 72.60it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  86% 7760/9047 [01:46<00:17, 72.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  86% 7780/9047 [01:46<00:17, 72.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  86% 7800/9047 [01:46<00:17, 72.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  86% 7820/9047 [01:47<00:16, 73.00it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  87% 7840/9047 [01:47<00:16, 73.11it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  87% 7860/9047 [01:47<00:16, 73.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  87% 7880/9047 [01:47<00:15, 73.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  87% 7900/9047 [01:47<00:15, 73.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  88% 7920/9047 [01:47<00:15, 73.51it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  88% 7940/9047 [01:47<00:15, 73.61it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  88% 7960/9047 [01:48<00:14, 73.70it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  88% 7980/9047 [01:48<00:14, 73.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  88% 8000/9047 [01:48<00:14, 73.90it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  89% 8020/9047 [01:48<00:13, 73.99it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  89% 8040/9047 [01:48<00:13, 74.09it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  89% 8060/9047 [01:48<00:13, 74.19it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  89% 8080/9047 [01:48<00:13, 74.28it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  90% 8100/9047 [01:48<00:12, 74.38it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  90% 8120/9047 [01:49<00:12, 74.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  90% 8140/9047 [01:49<00:12, 74.57it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  90% 8160/9047 [01:49<00:11, 74.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  90% 8180/9047 [01:49<00:11, 74.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  91% 8200/9047 [01:49<00:11, 74.85it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  91% 8220/9047 [01:49<00:11, 74.94it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  91% 8240/9047 [01:49<00:10, 75.03it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  91% 8260/9047 [01:49<00:10, 75.12it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  92% 8280/9047 [01:50<00:10, 75.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  92% 8300/9047 [01:50<00:09, 75.31it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  92% 8320/9047 [01:50<00:09, 75.39it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  92% 8340/9047 [01:50<00:09, 75.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  92% 8360/9047 [01:50<00:09, 75.58it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  93% 8380/9047 [01:50<00:08, 75.67it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  93% 8400/9047 [01:50<00:08, 75.76it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  93% 8420/9047 [01:50<00:08, 75.86it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  93% 8440/9047 [01:51<00:07, 75.95it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  94% 8460/9047 [01:51<00:07, 76.04it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  94% 8480/9047 [01:51<00:07, 76.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  94% 8500/9047 [01:51<00:07, 76.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  94% 8520/9047 [01:51<00:06, 76.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  94% 8540/9047 [01:51<00:06, 76.41it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  95% 8560/9047 [01:51<00:06, 76.50it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  95% 8580/9047 [01:52<00:06, 76.59it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  95% 8600/9047 [01:52<00:05, 76.69it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  95% 8620/9047 [01:52<00:05, 76.78it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  96% 8640/9047 [01:52<00:05, 76.87it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  96% 8660/9047 [01:52<00:05, 76.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  96% 8680/9047 [01:52<00:04, 77.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  96% 8700/9047 [01:52<00:04, 77.14it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  96% 8720/9047 [01:52<00:04, 77.23it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  97% 8740/9047 [01:53<00:03, 77.32it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  97% 8760/9047 [01:53<00:03, 77.40it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  97% 8780/9047 [01:53<00:03, 77.48it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  97% 8800/9047 [01:53<00:03, 77.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  97% 8820/9047 [01:53<00:02, 77.64it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  98% 8840/9047 [01:53<00:02, 77.71it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  98% 8860/9047 [01:53<00:02, 77.80it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  98% 8880/9047 [01:54<00:02, 77.88it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  98% 8900/9047 [01:54<00:01, 77.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  99% 8920/9047 [01:54<00:01, 78.05it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  99% 8940/9047 [01:54<00:01, 78.13it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  99% 8960/9047 [01:54<00:01, 78.21it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  99% 8980/9047 [01:54<00:00, 78.29it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44:  99% 9000/9047 [01:54<00:00, 78.37it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44: 100% 9020/9047 [01:54<00:00, 78.45it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44: 100% 9040/9047 [01:55<00:00, 78.53it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "Epoch 44: 100% 9047/9047 [01:55<00:00, 78.56it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "                                                                 \u001b[AMonitored metric avg_val_loss did not improve in the last 10 records. Best score: 4.061. Signaling Trainer to stop.\n",
            "Epoch 44: 100% 9047/9047 [02:02<00:00, 73.97it/s, loss=4.05, v_num=40, val_loss=4.060, avg_val_loss=4.060, train_loss=4.050]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 86\n",
            "Shape of the feature representation generated by the base encoder: (17054, 64)\n",
            "end time: 1728428474.3311253\n",
            "Execution time: 1.53 hours\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_500_train.h5ad\" \\\n",
        "                                              --batch_size 200 \\\n",
        "                                              --size_factor 0.7 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1008_train_test_top500_final\" \\\n",
        "                                              --train_test 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXLxalfnumD"
      },
      "source": [
        "**5. extract the features**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8HHCBIOWxgQ",
        "outputId": "4b311db4-4f42-4e6b-8d96-ad0d1bd855d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "usage: feature_extraction.py [-h] [--inputFilePath INPUTFILEPATH] [--batch_size BATCH_SIZE]\n",
            "                             [--size_factor SIZE_FACTOR] [--temperature TEMPERATURE]\n",
            "                             [--patience PATIENCE] [--min_delta MIN_DELTA] [--max_epoch MAX_EPOCH]\n",
            "                             --output_dir OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                             [--hidden_dims HIDDEN_DIMS] [--embedding_size EMBEDDING_SIZE]\n",
            "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_500_test.h5ad\" \\\n",
        "                                              --batch_size 200 \\\n",
        "                                              --size_factor 0.7 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1008_train_test_top500_final\" \\\n",
        "                                              --train_test 1 \\\n",
        "                                              --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1008_train_test_top500_final/saved_models/scContrastiveLearn_last.ckpt\" \\\n",
        "\n",
        "\n",
        "# \"/content/drive/MyDrive/Colab Notebooks/data/Larry_41093_2000_norm_log.h5ad\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGQKLhnAYTs4",
        "outputId": "8bd026bf-5d88-4167-f6df-3475b8725d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "/content/drive/MyDrive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1008_train_test_top500_final/saved_models/scContrastiveLearn_last.ckpt was loaded\n",
            "Shape of the feature representation generated by the base encoder: (2177, 64)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
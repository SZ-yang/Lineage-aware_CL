{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "cb0d6514-2d95-4641-e371-44c090d448ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFzoXTPCVub",
        "outputId": "13894a9e-f435-4c8e-c1b9-bed8575ff3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "e4c7ca4f-68ce-4bc0-b4da-083cca4c42bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.7 pytorch-lightning-2.4.0 torchmetrics-1.4.2\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
            "Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: array-api-compat, anndata\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.26.4)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.4.2)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.7)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.19.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.17.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (2.1.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.4.0\n",
            "    Uninstalling pytorch-lightning-2.4.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.4.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.10.9)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.5)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.9)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=cf5db3bff1ce1c3b745cee65a35a2a3cc4ab98c887303a04fcaef453d1fdec44\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, session-info, pynndescent, umap-learn, scanpy\n",
            "Successfully installed legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "45c6fb9c-e861-4b39-cd55-4d216e654984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1728368845.0451155\n",
            "usage: scContrastiveLearning_Main_709_ckpt_epoch.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                                                    [--batch_size BATCH_SIZE]\n",
            "                                                    [--size_factor SIZE_FACTOR]\n",
            "                                                    [--temperature TEMPERATURE]\n",
            "                                                    [--patience PATIENCE] [--min_delta MIN_DELTA]\n",
            "                                                    [--max_epoch MAX_EPOCH] --output_dir\n",
            "                                                    OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                                                    [--hidden_dims HIDDEN_DIMS]\n",
            "                                                    [--embedding_size EMBEDDING_SIZE]\n",
            "                                                    [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V30WCiJDs9I",
        "outputId": "a6289918-0b18-44be-bd4b-4d92961e0f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1728371387.3688452\n",
            "-------------------------------INFO-------------------------------\n",
            "Anndata Info:  /content/drive/MyDrive/Colab Notebooks/data/Larry_41093_2000_norm_log.h5ad\n",
            "batch_size:  270\n",
            "size_factor:  0.4\n",
            "temperature:  0.5\n",
            "number of epochs:  100\n",
            "train_test_ratio:  0.8\n",
            "input_dim:  2000\n",
            "hidden_dims:  [1024, 256, 64]\n",
            "embedding_size:  32\n",
            "The range of number of cells in a lineage: (5, 177), average of number of cells in a lineage 14.61\n",
            "number of batches:  6492\n",
            "total number of pairs:  1752840\n",
            "num_workers(number of available CPU cores):  12\n",
            "Training the data with validation set\n",
            "-------------------------------Dataloading-------------------------------\n",
            "number of total batch: 6492\n",
            "number of training batch: 5193\n",
            "number of validation batch: 1299\n",
            "\n",
            "lineage_info shape: (1752840, 1)\n",
            "lineage_info shape of training data: (1402110, 1)\n",
            "lineage_info shape of validation data: (350730, 1)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/content/drive/MyDrive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:105: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=self.config.epochs)\n",
            "\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | AddProjectionMLP | 2.3 M \n",
            "1 | loss  | ContrastiveLoss  | 0     \n",
            "-------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.348     Total estimated model params size (MB)\n",
            "2024-10-08 07:10:54.474777: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-08 07:10:55.050433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-08 07:10:55.313859: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-08 07:10:55.381242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-08 07:10:55.809978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-08 07:10:57.890343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.03it/s]/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 0:  80% 5180/6492 [01:15<00:19, 68.19it/s, loss=6.26, v_num=35]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 5200/6492 [01:19<00:19, 65.07it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  80% 5220/6492 [01:20<00:19, 65.20it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  81% 5240/6492 [01:20<00:19, 65.33it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  81% 5260/6492 [01:20<00:18, 65.46it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  81% 5280/6492 [01:20<00:18, 65.59it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  82% 5300/6492 [01:20<00:18, 65.72it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  82% 5320/6492 [01:20<00:17, 65.84it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  82% 5340/6492 [01:20<00:17, 65.97it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  83% 5360/6492 [01:21<00:17, 66.10it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  83% 5380/6492 [01:21<00:16, 66.22it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  83% 5400/6492 [01:21<00:16, 66.35it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  83% 5420/6492 [01:21<00:16, 66.47it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  84% 5440/6492 [01:21<00:15, 66.60it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  84% 5460/6492 [01:21<00:15, 66.72it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  84% 5480/6492 [01:21<00:15, 66.84it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  85% 5500/6492 [01:22<00:14, 66.96it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  85% 5520/6492 [01:22<00:14, 67.09it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  85% 5540/6492 [01:22<00:14, 67.21it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  86% 5560/6492 [01:22<00:13, 67.33it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  86% 5580/6492 [01:22<00:13, 67.45it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  86% 5600/6492 [01:22<00:13, 67.57it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  87% 5620/6492 [01:23<00:12, 67.69it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  87% 5640/6492 [01:23<00:12, 67.81it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  87% 5660/6492 [01:23<00:12, 67.93it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  87% 5680/6492 [01:23<00:11, 68.05it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  88% 5700/6492 [01:23<00:11, 68.16it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  88% 5720/6492 [01:23<00:11, 68.29it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  88% 5740/6492 [01:23<00:10, 68.40it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  89% 5760/6492 [01:24<00:10, 68.52it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  89% 5780/6492 [01:24<00:10, 68.64it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  89% 5800/6492 [01:24<00:10, 68.75it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  90% 5820/6492 [01:24<00:09, 68.87it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  90% 5840/6492 [01:24<00:09, 68.99it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  90% 5860/6492 [01:24<00:09, 69.11it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  91% 5880/6492 [01:24<00:08, 69.22it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  91% 5900/6492 [01:25<00:08, 69.34it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  91% 5920/6492 [01:25<00:08, 69.45it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  91% 5940/6492 [01:25<00:07, 69.57it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  92% 5960/6492 [01:25<00:07, 69.69it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  92% 5980/6492 [01:25<00:07, 69.81it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  92% 6000/6492 [01:25<00:07, 69.93it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  93% 6020/6492 [01:25<00:06, 70.04it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  93% 6040/6492 [01:26<00:06, 70.16it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  93% 6060/6492 [01:26<00:06, 70.27it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  94% 6080/6492 [01:26<00:05, 70.39it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  94% 6100/6492 [01:26<00:05, 70.50it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  94% 6120/6492 [01:26<00:05, 70.61it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  95% 6140/6492 [01:26<00:04, 70.73it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  95% 6160/6492 [01:26<00:04, 70.84it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  95% 6180/6492 [01:27<00:04, 70.95it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  96% 6200/6492 [01:27<00:04, 71.06it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  96% 6220/6492 [01:27<00:03, 71.17it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  96% 6240/6492 [01:27<00:03, 71.28it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  96% 6260/6492 [01:27<00:03, 71.39it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  97% 6280/6492 [01:27<00:02, 71.50it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  97% 6300/6492 [01:27<00:02, 71.61it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  97% 6320/6492 [01:28<00:02, 71.71it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  98% 6340/6492 [01:28<00:02, 71.82it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  98% 6360/6492 [01:28<00:01, 71.93it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  98% 6380/6492 [01:28<00:01, 72.04it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  99% 6400/6492 [01:28<00:01, 72.14it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  99% 6420/6492 [01:28<00:00, 72.25it/s, loss=6.26, v_num=35]\n",
            "Epoch 0:  99% 6440/6492 [01:28<00:00, 72.36it/s, loss=6.26, v_num=35]\n",
            "Epoch 0: 100% 6460/6492 [01:29<00:00, 72.47it/s, loss=6.26, v_num=35]\n",
            "Epoch 0: 100% 6480/6492 [01:29<00:00, 72.58it/s, loss=6.26, v_num=35]\n",
            "Epoch 0: 100% 6492/6492 [01:29<00:00, 72.64it/s, loss=6.26, v_num=35, val_loss=6.260, avg_val_loss=6.260]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved. New best score: 6.260\n",
            "Epoch 1:  80% 5180/6492 [01:19<00:20, 65.11it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]   \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 5200/6492 [01:23<00:20, 62.11it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  80% 5220/6492 [01:23<00:20, 62.24it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  81% 5240/6492 [01:24<00:20, 62.37it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  81% 5260/6492 [01:24<00:19, 62.50it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  81% 5280/6492 [01:24<00:19, 62.62it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  82% 5300/6492 [01:24<00:18, 62.75it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  82% 5320/6492 [01:24<00:18, 62.87it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  82% 5340/6492 [01:24<00:18, 63.00it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  83% 5360/6492 [01:24<00:17, 63.12it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  83% 5380/6492 [01:25<00:17, 63.25it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  83% 5400/6492 [01:25<00:17, 63.37it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  83% 5420/6492 [01:25<00:16, 63.49it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  84% 5440/6492 [01:25<00:16, 63.62it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  84% 5460/6492 [01:25<00:16, 63.74it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  84% 5480/6492 [01:25<00:15, 63.87it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  85% 5500/6492 [01:25<00:15, 64.00it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  85% 5520/6492 [01:26<00:15, 64.12it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  85% 5540/6492 [01:26<00:14, 64.24it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  86% 5560/6492 [01:26<00:14, 64.36it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  86% 5580/6492 [01:26<00:14, 64.48it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  86% 5600/6492 [01:26<00:13, 64.60it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  87% 5620/6492 [01:26<00:13, 64.72it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  87% 5640/6492 [01:26<00:13, 64.83it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  87% 5660/6492 [01:27<00:12, 64.94it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  87% 5680/6492 [01:27<00:12, 65.06it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  88% 5700/6492 [01:27<00:12, 65.17it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  88% 5720/6492 [01:27<00:11, 65.28it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  88% 5740/6492 [01:27<00:11, 65.39it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  89% 5760/6492 [01:27<00:11, 65.50it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  89% 5780/6492 [01:28<00:10, 65.61it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  89% 5800/6492 [01:28<00:10, 65.72it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  90% 5820/6492 [01:28<00:10, 65.83it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  90% 5840/6492 [01:28<00:09, 65.94it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  90% 5860/6492 [01:28<00:09, 66.05it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  91% 5880/6492 [01:28<00:09, 66.16it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  91% 5900/6492 [01:29<00:08, 66.27it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  91% 5920/6492 [01:29<00:08, 66.38it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  91% 5940/6492 [01:29<00:08, 66.49it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  92% 5960/6492 [01:29<00:07, 66.60it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  92% 5980/6492 [01:29<00:07, 66.72it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  92% 6000/6492 [01:29<00:07, 66.84it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  93% 6020/6492 [01:29<00:07, 66.95it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  93% 6040/6492 [01:30<00:06, 67.07it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  93% 6060/6492 [01:30<00:06, 67.18it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  94% 6080/6492 [01:30<00:06, 67.29it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  94% 6100/6492 [01:30<00:05, 67.39it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  94% 6120/6492 [01:30<00:05, 67.50it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  95% 6140/6492 [01:30<00:05, 67.61it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  95% 6160/6492 [01:30<00:04, 67.71it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  95% 6180/6492 [01:31<00:04, 67.82it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  96% 6200/6492 [01:31<00:04, 67.92it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  96% 6220/6492 [01:31<00:03, 68.03it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  96% 6240/6492 [01:31<00:03, 68.14it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  96% 6260/6492 [01:31<00:03, 68.25it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  97% 6280/6492 [01:31<00:03, 68.36it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  97% 6300/6492 [01:32<00:02, 68.45it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  97% 6320/6492 [01:32<00:02, 68.56it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  98% 6340/6492 [01:32<00:02, 68.66it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  98% 6360/6492 [01:32<00:01, 68.75it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  98% 6380/6492 [01:32<00:01, 68.86it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  99% 6400/6492 [01:32<00:01, 68.95it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  99% 6420/6492 [01:32<00:01, 69.05it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1:  99% 6440/6492 [01:33<00:00, 69.14it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1: 100% 6460/6492 [01:33<00:00, 69.25it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1: 100% 6480/6492 [01:33<00:00, 69.35it/s, loss=5, v_num=35, val_loss=6.260, avg_val_loss=6.260, train_loss=6.260]\n",
            "Epoch 1: 100% 6492/6492 [01:33<00:00, 69.41it/s, loss=4.97, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=6.260]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 1.278 >= min_delta = 0.001. New best score: 4.982\n",
            "Epoch 2:  80% 5180/6492 [01:19<00:20, 65.35it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 5200/6492 [01:23<00:20, 62.21it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  80% 5220/6492 [01:23<00:20, 62.34it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  81% 5240/6492 [01:23<00:20, 62.47it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  81% 5260/6492 [01:24<00:19, 62.59it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  81% 5280/6492 [01:24<00:19, 62.72it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  82% 5300/6492 [01:24<00:18, 62.85it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  82% 5320/6492 [01:24<00:18, 62.97it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  82% 5340/6492 [01:24<00:18, 63.09it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  83% 5360/6492 [01:24<00:17, 63.22it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  83% 5380/6492 [01:24<00:17, 63.34it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  83% 5400/6492 [01:25<00:17, 63.47it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  83% 5420/6492 [01:25<00:16, 63.59it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  84% 5440/6492 [01:25<00:16, 63.72it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  84% 5460/6492 [01:25<00:16, 63.83it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  84% 5480/6492 [01:25<00:15, 63.95it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  85% 5500/6492 [01:25<00:15, 64.07it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  85% 5520/6492 [01:25<00:15, 64.19it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  85% 5540/6492 [01:26<00:14, 64.30it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  86% 5560/6492 [01:26<00:14, 64.41it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  86% 5580/6492 [01:26<00:14, 64.52it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  86% 5600/6492 [01:26<00:13, 64.63it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  87% 5620/6492 [01:26<00:13, 64.75it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  87% 5640/6492 [01:26<00:13, 64.86it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  87% 5660/6492 [01:27<00:12, 64.96it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  87% 5680/6492 [01:27<00:12, 65.07it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  88% 5700/6492 [01:27<00:12, 65.18it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  88% 5720/6492 [01:27<00:11, 65.29it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  88% 5740/6492 [01:27<00:11, 65.40it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  89% 5760/6492 [01:27<00:11, 65.51it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  89% 5780/6492 [01:28<00:10, 65.61it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  89% 5800/6492 [01:28<00:10, 65.72it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  90% 5820/6492 [01:28<00:10, 65.83it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  90% 5840/6492 [01:28<00:09, 65.94it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  90% 5860/6492 [01:28<00:09, 66.05it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  91% 5880/6492 [01:28<00:09, 66.16it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  91% 5900/6492 [01:29<00:08, 66.27it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  91% 5920/6492 [01:29<00:08, 66.39it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  91% 5940/6492 [01:29<00:08, 66.50it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  92% 5960/6492 [01:29<00:07, 66.62it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  92% 5980/6492 [01:29<00:07, 66.74it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  92% 6000/6492 [01:29<00:07, 66.85it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  93% 6020/6492 [01:29<00:07, 66.97it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  93% 6040/6492 [01:30<00:06, 67.08it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  93% 6060/6492 [01:30<00:06, 67.19it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  94% 6080/6492 [01:30<00:06, 67.31it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  94% 6100/6492 [01:30<00:05, 67.42it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  94% 6120/6492 [01:30<00:05, 67.53it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  95% 6140/6492 [01:30<00:05, 67.64it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  95% 6160/6492 [01:30<00:04, 67.75it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  95% 6180/6492 [01:31<00:04, 67.86it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  96% 6200/6492 [01:31<00:04, 67.96it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  96% 6220/6492 [01:31<00:03, 68.05it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  96% 6240/6492 [01:31<00:03, 68.15it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  96% 6260/6492 [01:31<00:03, 68.25it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  97% 6280/6492 [01:31<00:03, 68.36it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  97% 6300/6492 [01:32<00:02, 68.46it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  97% 6320/6492 [01:32<00:02, 68.57it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  98% 6340/6492 [01:32<00:02, 68.68it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  98% 6360/6492 [01:32<00:01, 68.78it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  98% 6380/6492 [01:32<00:01, 68.89it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  99% 6400/6492 [01:32<00:01, 69.00it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  99% 6420/6492 [01:32<00:01, 69.11it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2:  99% 6440/6492 [01:33<00:00, 69.22it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2: 100% 6460/6492 [01:33<00:00, 69.33it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2: 100% 6480/6492 [01:33<00:00, 69.44it/s, loss=4.67, v_num=35, val_loss=4.980, avg_val_loss=4.980, train_loss=5.230]\n",
            "Epoch 2: 100% 6492/6492 [01:33<00:00, 69.49it/s, loss=4.66, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=5.230]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.317 >= min_delta = 0.001. New best score: 4.665\n",
            "Epoch 3:  80% 5180/6492 [01:19<00:20, 65.38it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 5200/6492 [01:23<00:20, 62.22it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  80% 5220/6492 [01:23<00:20, 62.35it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  81% 5240/6492 [01:23<00:20, 62.48it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  81% 5260/6492 [01:24<00:19, 62.61it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  81% 5280/6492 [01:24<00:19, 62.74it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  82% 5300/6492 [01:24<00:18, 62.86it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  82% 5320/6492 [01:24<00:18, 62.99it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  82% 5340/6492 [01:24<00:18, 63.12it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  83% 5360/6492 [01:24<00:17, 63.24it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  83% 5380/6492 [01:24<00:17, 63.36it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  83% 5400/6492 [01:25<00:17, 63.48it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  83% 5420/6492 [01:25<00:16, 63.60it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  84% 5440/6492 [01:25<00:16, 63.72it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  84% 5460/6492 [01:25<00:16, 63.85it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  84% 5480/6492 [01:25<00:15, 63.97it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  85% 5500/6492 [01:25<00:15, 64.09it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  85% 5520/6492 [01:25<00:15, 64.22it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  85% 5540/6492 [01:26<00:14, 64.34it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  86% 5560/6492 [01:26<00:14, 64.47it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  86% 5580/6492 [01:26<00:14, 64.60it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  86% 5600/6492 [01:26<00:13, 64.72it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  87% 5620/6492 [01:26<00:13, 64.84it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  87% 5640/6492 [01:26<00:13, 64.97it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  87% 5660/6492 [01:26<00:12, 65.06it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  87% 5680/6492 [01:27<00:12, 65.19it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  88% 5700/6492 [01:27<00:12, 65.31it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  88% 5720/6492 [01:27<00:11, 65.42it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  88% 5740/6492 [01:27<00:11, 65.55it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  89% 5760/6492 [01:27<00:11, 65.67it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  89% 5780/6492 [01:27<00:10, 65.78it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  89% 5800/6492 [01:28<00:10, 65.90it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  90% 5820/6492 [01:28<00:10, 66.02it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  90% 5840/6492 [01:28<00:09, 66.14it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  90% 5860/6492 [01:28<00:09, 66.26it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  91% 5880/6492 [01:28<00:09, 66.37it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  91% 5900/6492 [01:28<00:08, 66.49it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  91% 5920/6492 [01:28<00:08, 66.60it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  91% 5940/6492 [01:29<00:08, 66.72it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  92% 5960/6492 [01:29<00:07, 66.83it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  92% 5980/6492 [01:29<00:07, 66.94it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  92% 6000/6492 [01:29<00:07, 67.05it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  93% 6020/6492 [01:29<00:07, 67.17it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  93% 6040/6492 [01:29<00:06, 67.28it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  93% 6060/6492 [01:29<00:06, 67.39it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  94% 6080/6492 [01:30<00:06, 67.51it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  94% 6100/6492 [01:30<00:05, 67.62it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  94% 6120/6492 [01:30<00:05, 67.73it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  95% 6140/6492 [01:30<00:05, 67.84it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  95% 6160/6492 [01:30<00:04, 67.95it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  95% 6180/6492 [01:30<00:04, 68.06it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  96% 6200/6492 [01:30<00:04, 68.17it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  96% 6220/6492 [01:31<00:03, 68.28it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  96% 6240/6492 [01:31<00:03, 68.39it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  96% 6260/6492 [01:31<00:03, 68.51it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  97% 6280/6492 [01:31<00:03, 68.62it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  97% 6300/6492 [01:31<00:02, 68.73it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  97% 6320/6492 [01:31<00:02, 68.84it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  98% 6340/6492 [01:31<00:02, 68.96it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  98% 6360/6492 [01:32<00:01, 69.07it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  98% 6380/6492 [01:32<00:01, 69.18it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  99% 6400/6492 [01:32<00:01, 69.29it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  99% 6420/6492 [01:32<00:01, 69.40it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3:  99% 6440/6492 [01:32<00:00, 69.50it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3: 100% 6460/6492 [01:32<00:00, 69.62it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3: 100% 6480/6492 [01:32<00:00, 69.73it/s, loss=4.55, v_num=35, val_loss=4.660, avg_val_loss=4.660, train_loss=4.790]\n",
            "Epoch 3: 100% 6492/6492 [01:33<00:00, 69.79it/s, loss=4.54, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.790]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.117 >= min_delta = 0.001. New best score: 4.548\n",
            "Epoch 4:  80% 5180/6492 [01:19<00:20, 65.51it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 5200/6492 [01:23<00:20, 62.31it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  80% 5220/6492 [01:23<00:20, 62.44it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  81% 5240/6492 [01:23<00:20, 62.56it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  81% 5260/6492 [01:23<00:19, 62.68it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  81% 5280/6492 [01:24<00:19, 62.80it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  82% 5300/6492 [01:24<00:18, 62.92it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  82% 5320/6492 [01:24<00:18, 63.05it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  82% 5340/6492 [01:24<00:18, 63.17it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  83% 5360/6492 [01:24<00:17, 63.29it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  83% 5380/6492 [01:24<00:17, 63.41it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  83% 5400/6492 [01:24<00:17, 63.53it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  83% 5420/6492 [01:25<00:16, 63.65it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  84% 5440/6492 [01:25<00:16, 63.77it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  84% 5460/6492 [01:25<00:16, 63.89it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  84% 5480/6492 [01:25<00:15, 64.01it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  85% 5500/6492 [01:25<00:15, 64.14it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  85% 5520/6492 [01:25<00:15, 64.26it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  85% 5540/6492 [01:26<00:14, 64.37it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  86% 5560/6492 [01:26<00:14, 64.49it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  86% 5580/6492 [01:26<00:14, 64.60it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  86% 5600/6492 [01:26<00:13, 64.72it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  87% 5620/6492 [01:26<00:13, 64.84it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  87% 5640/6492 [01:26<00:13, 64.95it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  87% 5660/6492 [01:26<00:12, 65.07it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  87% 5680/6492 [01:27<00:12, 65.18it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  88% 5700/6492 [01:27<00:12, 65.29it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  88% 5720/6492 [01:27<00:11, 65.41it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  88% 5740/6492 [01:27<00:11, 65.52it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  89% 5760/6492 [01:27<00:11, 65.63it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  89% 5780/6492 [01:27<00:10, 65.73it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  89% 5800/6492 [01:28<00:10, 65.83it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  90% 5820/6492 [01:28<00:10, 65.94it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  90% 5840/6492 [01:28<00:09, 66.05it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  90% 5860/6492 [01:28<00:09, 66.16it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  91% 5880/6492 [01:28<00:09, 66.28it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  91% 5900/6492 [01:28<00:08, 66.38it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  91% 5920/6492 [01:29<00:08, 66.50it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  91% 5940/6492 [01:29<00:08, 66.62it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  92% 5960/6492 [01:29<00:07, 66.73it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  92% 5980/6492 [01:29<00:07, 66.85it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  92% 6000/6492 [01:29<00:07, 66.96it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  93% 6020/6492 [01:29<00:07, 67.08it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  93% 6040/6492 [01:29<00:06, 67.19it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  93% 6060/6492 [01:30<00:06, 67.30it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  94% 6080/6492 [01:30<00:06, 67.42it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  94% 6100/6492 [01:30<00:05, 67.53it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  94% 6120/6492 [01:30<00:05, 67.64it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  95% 6140/6492 [01:30<00:05, 67.76it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  95% 6160/6492 [01:30<00:04, 67.87it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  95% 6180/6492 [01:30<00:04, 67.99it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  96% 6200/6492 [01:31<00:04, 68.10it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  96% 6220/6492 [01:31<00:03, 68.21it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  96% 6240/6492 [01:31<00:03, 68.33it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  96% 6260/6492 [01:31<00:03, 68.44it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  97% 6280/6492 [01:31<00:03, 68.55it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  97% 6300/6492 [01:31<00:02, 68.66it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  97% 6320/6492 [01:31<00:02, 68.78it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  98% 6340/6492 [01:32<00:02, 68.89it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  98% 6360/6492 [01:32<00:01, 69.00it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  98% 6380/6492 [01:32<00:01, 69.01it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  99% 6400/6492 [01:32<00:01, 69.12it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  99% 6420/6492 [01:32<00:01, 69.22it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4:  99% 6440/6492 [01:32<00:00, 69.32it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4: 100% 6460/6492 [01:33<00:00, 69.42it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4: 100% 6480/6492 [01:33<00:00, 69.52it/s, loss=4.5, v_num=35, val_loss=4.550, avg_val_loss=4.550, train_loss=4.600]\n",
            "Epoch 4: 100% 6492/6492 [01:33<00:00, 69.58it/s, loss=4.49, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.600]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.053 >= min_delta = 0.001. New best score: 4.495\n",
            "Epoch 5:  80% 5180/6492 [01:19<00:20, 65.06it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 5200/6492 [01:24<00:20, 61.80it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  80% 5220/6492 [01:24<00:20, 61.94it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  81% 5240/6492 [01:24<00:20, 62.07it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  81% 5260/6492 [01:24<00:19, 62.19it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  81% 5280/6492 [01:24<00:19, 62.32it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  82% 5300/6492 [01:24<00:19, 62.45it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  82% 5320/6492 [01:25<00:18, 62.57it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  82% 5340/6492 [01:25<00:18, 62.70it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  83% 5360/6492 [01:25<00:18, 62.83it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  83% 5380/6492 [01:25<00:17, 62.95it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  83% 5400/6492 [01:25<00:17, 63.08it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  83% 5420/6492 [01:25<00:16, 63.21it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  84% 5440/6492 [01:25<00:16, 63.33it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  84% 5460/6492 [01:26<00:16, 63.46it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  84% 5480/6492 [01:26<00:15, 63.58it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  85% 5500/6492 [01:26<00:15, 63.70it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  85% 5520/6492 [01:26<00:15, 63.82it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  85% 5540/6492 [01:26<00:14, 63.94it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  86% 5560/6492 [01:26<00:14, 64.06it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  86% 5580/6492 [01:26<00:14, 64.19it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  86% 5600/6492 [01:27<00:13, 64.31it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  87% 5620/6492 [01:27<00:13, 64.43it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  87% 5640/6492 [01:27<00:13, 64.55it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  87% 5660/6492 [01:27<00:12, 64.67it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  87% 5680/6492 [01:27<00:12, 64.79it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  88% 5700/6492 [01:27<00:12, 64.91it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  88% 5720/6492 [01:27<00:11, 65.03it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  88% 5740/6492 [01:28<00:11, 65.15it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  89% 5760/6492 [01:28<00:11, 65.27it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  89% 5780/6492 [01:28<00:10, 65.39it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  89% 5800/6492 [01:28<00:10, 65.51it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  90% 5820/6492 [01:28<00:10, 65.62it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  90% 5840/6492 [01:28<00:09, 65.74it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  90% 5860/6492 [01:28<00:09, 65.86it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  91% 5880/6492 [01:29<00:09, 65.97it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  91% 5900/6492 [01:29<00:08, 66.09it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  91% 5920/6492 [01:29<00:08, 66.20it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  91% 5940/6492 [01:29<00:08, 66.32it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  92% 5960/6492 [01:29<00:08, 66.43it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  92% 5980/6492 [01:29<00:07, 66.55it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  92% 6000/6492 [01:30<00:07, 66.66it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  93% 6020/6492 [01:30<00:07, 66.77it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  93% 6040/6492 [01:30<00:06, 66.89it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  93% 6060/6492 [01:30<00:06, 67.00it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  94% 6080/6492 [01:30<00:06, 67.12it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  94% 6100/6492 [01:30<00:05, 67.23it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  94% 6120/6492 [01:30<00:05, 67.34it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  95% 6140/6492 [01:31<00:05, 67.45it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  95% 6160/6492 [01:31<00:04, 67.56it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  95% 6180/6492 [01:31<00:04, 67.67it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  96% 6200/6492 [01:31<00:04, 67.78it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  96% 6220/6492 [01:31<00:04, 67.90it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  96% 6240/6492 [01:31<00:03, 68.00it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  96% 6260/6492 [01:31<00:03, 68.11it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  97% 6280/6492 [01:32<00:03, 68.22it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  97% 6300/6492 [01:32<00:02, 68.34it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  97% 6320/6492 [01:32<00:02, 68.45it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  98% 6340/6492 [01:32<00:02, 68.56it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  98% 6360/6492 [01:32<00:01, 68.67it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  98% 6380/6492 [01:32<00:01, 68.77it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  99% 6400/6492 [01:32<00:01, 68.88it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  99% 6420/6492 [01:33<00:01, 68.99it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5:  99% 6440/6492 [01:33<00:00, 69.10it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5: 100% 6460/6492 [01:33<00:00, 69.21it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5: 100% 6480/6492 [01:33<00:00, 69.32it/s, loss=4.47, v_num=35, val_loss=4.490, avg_val_loss=4.490, train_loss=4.520]\n",
            "Epoch 5: 100% 6492/6492 [01:33<00:00, 69.39it/s, loss=4.46, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.520]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.027 >= min_delta = 0.001. New best score: 4.468\n",
            "Epoch 6:  80% 5180/6492 [01:19<00:20, 65.17it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 5200/6492 [01:24<00:20, 61.86it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  80% 5220/6492 [01:24<00:20, 62.00it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  81% 5240/6492 [01:24<00:20, 62.13it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  81% 5260/6492 [01:24<00:19, 62.26it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  81% 5280/6492 [01:24<00:19, 62.38it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  82% 5300/6492 [01:24<00:19, 62.50it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  82% 5320/6492 [01:24<00:18, 62.62it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  82% 5340/6492 [01:25<00:18, 62.75it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  83% 5360/6492 [01:25<00:18, 62.88it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  83% 5380/6492 [01:25<00:17, 63.00it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  83% 5400/6492 [01:25<00:17, 63.13it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  83% 5420/6492 [01:25<00:16, 63.26it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  84% 5440/6492 [01:25<00:16, 63.39it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  84% 5460/6492 [01:25<00:16, 63.52it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  84% 5480/6492 [01:26<00:15, 63.65it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  85% 5500/6492 [01:26<00:15, 63.77it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  85% 5520/6492 [01:26<00:15, 63.90it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  85% 5540/6492 [01:26<00:14, 64.02it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  86% 5560/6492 [01:26<00:14, 64.14it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  86% 5580/6492 [01:26<00:14, 64.27it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  86% 5600/6492 [01:26<00:13, 64.39it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  87% 5620/6492 [01:27<00:13, 64.51it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  87% 5640/6492 [01:27<00:13, 64.64it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  87% 5660/6492 [01:27<00:12, 64.76it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  87% 5680/6492 [01:27<00:12, 64.88it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  88% 5700/6492 [01:27<00:12, 65.01it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  88% 5720/6492 [01:27<00:11, 65.13it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  88% 5740/6492 [01:27<00:11, 65.25it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  89% 5760/6492 [01:28<00:11, 65.37it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  89% 5780/6492 [01:28<00:10, 65.50it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  89% 5800/6492 [01:28<00:10, 65.61it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  90% 5820/6492 [01:28<00:10, 65.73it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  90% 5840/6492 [01:28<00:09, 65.85it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  90% 5860/6492 [01:28<00:09, 65.97it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  91% 5880/6492 [01:29<00:09, 65.98it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  91% 5900/6492 [01:29<00:08, 66.10it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  91% 5920/6492 [01:29<00:08, 66.22it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  91% 5940/6492 [01:29<00:08, 66.34it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  92% 5960/6492 [01:29<00:08, 66.45it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  92% 5980/6492 [01:29<00:07, 66.57it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  92% 6000/6492 [01:29<00:07, 66.69it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  93% 6020/6492 [01:30<00:07, 66.80it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  93% 6040/6492 [01:30<00:06, 66.92it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  93% 6060/6492 [01:30<00:06, 67.04it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  94% 6080/6492 [01:30<00:06, 67.15it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  94% 6100/6492 [01:30<00:05, 67.27it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  94% 6120/6492 [01:30<00:05, 67.38it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  95% 6140/6492 [01:30<00:05, 67.50it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  95% 6160/6492 [01:31<00:04, 67.61it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  95% 6180/6492 [01:31<00:04, 67.73it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  96% 6200/6492 [01:31<00:04, 67.84it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  96% 6220/6492 [01:31<00:04, 67.95it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  96% 6240/6492 [01:31<00:03, 68.06it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  96% 6260/6492 [01:31<00:03, 68.18it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  97% 6280/6492 [01:31<00:03, 68.29it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  97% 6300/6492 [01:32<00:02, 68.40it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  97% 6320/6492 [01:32<00:02, 68.51it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  98% 6340/6492 [01:32<00:02, 68.62it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  98% 6360/6492 [01:32<00:01, 68.73it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  98% 6380/6492 [01:32<00:01, 68.84it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  99% 6400/6492 [01:32<00:01, 68.95it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  99% 6420/6492 [01:32<00:01, 69.06it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6:  99% 6440/6492 [01:33<00:00, 69.17it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6: 100% 6460/6492 [01:33<00:00, 69.27it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6: 100% 6480/6492 [01:33<00:00, 69.38it/s, loss=4.45, v_num=35, val_loss=4.470, avg_val_loss=4.470, train_loss=4.480]\n",
            "Epoch 6: 100% 6492/6492 [01:33<00:00, 69.44it/s, loss=4.45, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.480]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.014 >= min_delta = 0.001. New best score: 4.454\n",
            "Epoch 7:  80% 5180/6492 [01:18<00:19, 65.74it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 5200/6492 [01:23<00:20, 62.41it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  80% 5220/6492 [01:23<00:20, 62.54it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  81% 5240/6492 [01:23<00:19, 62.66it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  81% 5260/6492 [01:23<00:19, 62.78it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  81% 5280/6492 [01:23<00:19, 62.90it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  82% 5300/6492 [01:24<00:18, 63.02it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  82% 5320/6492 [01:24<00:18, 63.15it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  82% 5340/6492 [01:24<00:18, 63.27it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  83% 5360/6492 [01:24<00:17, 63.39it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  83% 5380/6492 [01:24<00:17, 63.51it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  83% 5400/6492 [01:24<00:17, 63.63it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  83% 5420/6492 [01:25<00:16, 63.75it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  84% 5440/6492 [01:25<00:16, 63.87it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  84% 5460/6492 [01:25<00:16, 63.99it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  84% 5480/6492 [01:25<00:15, 64.10it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  85% 5500/6492 [01:25<00:15, 64.22it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  85% 5520/6492 [01:25<00:15, 64.34it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  85% 5540/6492 [01:25<00:14, 64.46it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  86% 5560/6492 [01:26<00:14, 64.59it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  86% 5580/6492 [01:26<00:14, 64.71it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  86% 5600/6492 [01:26<00:13, 64.83it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  87% 5620/6492 [01:26<00:13, 64.95it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  87% 5640/6492 [01:26<00:13, 65.06it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  87% 5660/6492 [01:26<00:12, 65.17it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  87% 5680/6492 [01:26<00:12, 65.29it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  88% 5700/6492 [01:27<00:12, 65.40it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  88% 5720/6492 [01:27<00:11, 65.52it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  88% 5740/6492 [01:27<00:11, 65.63it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  89% 5760/6492 [01:27<00:11, 65.74it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  89% 5780/6492 [01:27<00:10, 65.86it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  89% 5800/6492 [01:27<00:10, 65.97it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  90% 5820/6492 [01:28<00:10, 66.08it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  90% 5840/6492 [01:28<00:09, 66.19it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  90% 5860/6492 [01:28<00:09, 66.30it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  91% 5880/6492 [01:28<00:09, 66.41it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  91% 5900/6492 [01:28<00:08, 66.52it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  91% 5920/6492 [01:28<00:08, 66.62it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  91% 5940/6492 [01:29<00:08, 66.73it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  92% 5960/6492 [01:29<00:07, 66.84it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  92% 5980/6492 [01:29<00:07, 66.95it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  92% 6000/6492 [01:29<00:07, 67.05it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  93% 6020/6492 [01:29<00:07, 67.17it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  93% 6040/6492 [01:29<00:06, 67.28it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  93% 6060/6492 [01:29<00:06, 67.38it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  94% 6080/6492 [01:30<00:06, 67.49it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  94% 6100/6492 [01:30<00:05, 67.60it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  94% 6120/6492 [01:30<00:05, 67.71it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  95% 6140/6492 [01:30<00:05, 67.82it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  95% 6160/6492 [01:30<00:04, 67.94it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  95% 6180/6492 [01:30<00:04, 68.05it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  96% 6200/6492 [01:30<00:04, 68.16it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  96% 6220/6492 [01:31<00:03, 68.27it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  96% 6240/6492 [01:31<00:03, 68.38it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  96% 6260/6492 [01:31<00:03, 68.49it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  97% 6280/6492 [01:31<00:03, 68.60it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  97% 6300/6492 [01:31<00:02, 68.71it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  97% 6320/6492 [01:31<00:02, 68.81it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  98% 6340/6492 [01:31<00:02, 68.92it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  98% 6360/6492 [01:32<00:01, 69.02it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  98% 6380/6492 [01:32<00:01, 69.13it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  99% 6400/6492 [01:32<00:01, 69.24it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  99% 6420/6492 [01:32<00:01, 69.34it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7:  99% 6440/6492 [01:32<00:00, 69.44it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7: 100% 6460/6492 [01:32<00:00, 69.55it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7: 100% 6480/6492 [01:33<00:00, 69.65it/s, loss=4.44, v_num=35, val_loss=4.450, avg_val_loss=4.450, train_loss=4.460]\n",
            "Epoch 7: 100% 6492/6492 [01:33<00:00, 69.71it/s, loss=4.43, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.460]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.015 >= min_delta = 0.001. New best score: 4.439\n",
            "Epoch 8:  80% 5180/6492 [01:19<00:20, 65.50it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 5200/6492 [01:23<00:20, 62.13it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  80% 5220/6492 [01:23<00:20, 62.27it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  81% 5240/6492 [01:23<00:20, 62.40it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  81% 5260/6492 [01:24<00:19, 62.53it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  81% 5280/6492 [01:24<00:19, 62.66it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  82% 5300/6492 [01:24<00:18, 62.78it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  82% 5320/6492 [01:24<00:18, 62.91it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  82% 5340/6492 [01:24<00:18, 63.04it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  83% 5360/6492 [01:24<00:17, 63.17it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  83% 5380/6492 [01:25<00:17, 63.29it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  83% 5400/6492 [01:25<00:17, 63.42it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  83% 5420/6492 [01:25<00:16, 63.54it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  84% 5440/6492 [01:25<00:16, 63.66it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  84% 5460/6492 [01:25<00:16, 63.79it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  84% 5480/6492 [01:25<00:15, 63.91it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  85% 5500/6492 [01:26<00:15, 63.94it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  85% 5520/6492 [01:26<00:15, 64.06it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  85% 5540/6492 [01:26<00:14, 64.18it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  86% 5560/6492 [01:26<00:14, 64.31it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  86% 5580/6492 [01:26<00:14, 64.43it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  86% 5600/6492 [01:26<00:13, 64.55it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  87% 5620/6492 [01:26<00:13, 64.68it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  87% 5640/6492 [01:27<00:13, 64.80it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  87% 5660/6492 [01:27<00:12, 64.92it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  87% 5680/6492 [01:27<00:12, 65.04it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  88% 5700/6492 [01:27<00:12, 65.16it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  88% 5720/6492 [01:27<00:11, 65.28it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  88% 5740/6492 [01:27<00:11, 65.40it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  89% 5760/6492 [01:27<00:11, 65.52it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  89% 5780/6492 [01:28<00:10, 65.64it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  89% 5800/6492 [01:28<00:10, 65.76it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  90% 5820/6492 [01:28<00:10, 65.88it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  90% 5840/6492 [01:28<00:09, 66.00it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  90% 5860/6492 [01:28<00:09, 66.11it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  91% 5880/6492 [01:28<00:09, 66.23it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  91% 5900/6492 [01:28<00:08, 66.34it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  91% 5920/6492 [01:29<00:08, 66.46it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  91% 5940/6492 [01:29<00:08, 66.58it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  92% 5960/6492 [01:29<00:07, 66.70it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  92% 5980/6492 [01:29<00:07, 66.82it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  92% 6000/6492 [01:29<00:07, 66.93it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  93% 6020/6492 [01:29<00:07, 67.05it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  93% 6040/6492 [01:29<00:06, 67.17it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  93% 6060/6492 [01:30<00:06, 67.28it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  94% 6080/6492 [01:30<00:06, 67.40it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  94% 6100/6492 [01:30<00:05, 67.51it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  94% 6120/6492 [01:30<00:05, 67.63it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  95% 6140/6492 [01:30<00:05, 67.74it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  95% 6160/6492 [01:30<00:04, 67.85it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  95% 6180/6492 [01:30<00:04, 67.96it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  96% 6200/6492 [01:31<00:04, 68.07it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  96% 6220/6492 [01:31<00:03, 68.18it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  96% 6240/6492 [01:31<00:03, 68.29it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  96% 6260/6492 [01:31<00:03, 68.40it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  97% 6280/6492 [01:31<00:03, 68.51it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  97% 6300/6492 [01:31<00:02, 68.62it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  97% 6320/6492 [01:31<00:02, 68.72it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  98% 6340/6492 [01:32<00:02, 68.84it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  98% 6360/6492 [01:32<00:01, 68.95it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  98% 6380/6492 [01:32<00:01, 69.06it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  99% 6400/6492 [01:32<00:01, 69.17it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  99% 6420/6492 [01:32<00:01, 69.27it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8:  99% 6440/6492 [01:32<00:00, 69.38it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8: 100% 6460/6492 [01:32<00:00, 69.49it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8: 100% 6480/6492 [01:33<00:00, 69.60it/s, loss=4.42, v_num=35, val_loss=4.440, avg_val_loss=4.440, train_loss=4.440]\n",
            "Epoch 8: 100% 6492/6492 [01:33<00:00, 69.66it/s, loss=4.42, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.440]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.016 >= min_delta = 0.001. New best score: 4.423\n",
            "Epoch 9:  80% 5180/6492 [01:18<00:19, 65.62it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 5200/6492 [01:23<00:20, 62.23it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  80% 5220/6492 [01:23<00:20, 62.36it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  81% 5240/6492 [01:23<00:20, 62.48it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  81% 5260/6492 [01:24<00:19, 62.61it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  81% 5280/6492 [01:24<00:19, 62.73it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  82% 5300/6492 [01:24<00:18, 62.86it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  82% 5320/6492 [01:24<00:18, 62.99it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  82% 5340/6492 [01:24<00:18, 63.11it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  83% 5360/6492 [01:24<00:17, 63.23it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  83% 5380/6492 [01:24<00:17, 63.35it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  83% 5400/6492 [01:25<00:17, 63.47it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  83% 5420/6492 [01:25<00:16, 63.59it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  84% 5440/6492 [01:25<00:16, 63.72it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  84% 5460/6492 [01:25<00:16, 63.84it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  84% 5480/6492 [01:25<00:15, 63.95it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  85% 5500/6492 [01:25<00:15, 64.08it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  85% 5520/6492 [01:25<00:15, 64.20it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  85% 5540/6492 [01:26<00:14, 64.32it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  86% 5560/6492 [01:26<00:14, 64.44it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  86% 5580/6492 [01:26<00:14, 64.57it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  86% 5600/6492 [01:26<00:13, 64.69it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  87% 5620/6492 [01:26<00:13, 64.82it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  87% 5640/6492 [01:26<00:13, 64.94it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  87% 5660/6492 [01:26<00:12, 65.06it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  87% 5680/6492 [01:27<00:12, 65.18it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  88% 5700/6492 [01:27<00:12, 65.30it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  88% 5720/6492 [01:27<00:11, 65.42it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  88% 5740/6492 [01:27<00:11, 65.54it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  89% 5760/6492 [01:27<00:11, 65.66it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  89% 5780/6492 [01:27<00:10, 65.77it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  89% 5800/6492 [01:28<00:10, 65.89it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  90% 5820/6492 [01:28<00:10, 66.00it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  90% 5840/6492 [01:28<00:09, 66.11it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  90% 5860/6492 [01:28<00:09, 66.22it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  91% 5880/6492 [01:28<00:09, 66.33it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  91% 5900/6492 [01:28<00:08, 66.44it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  91% 5920/6492 [01:28<00:08, 66.55it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  91% 5940/6492 [01:29<00:08, 66.67it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  92% 5960/6492 [01:29<00:07, 66.78it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  92% 5980/6492 [01:29<00:07, 66.89it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  92% 6000/6492 [01:29<00:07, 67.00it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  93% 6020/6492 [01:29<00:07, 67.11it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  93% 6040/6492 [01:29<00:06, 67.22it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  93% 6060/6492 [01:29<00:06, 67.33it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  94% 6080/6492 [01:30<00:06, 67.44it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  94% 6100/6492 [01:30<00:05, 67.55it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  94% 6120/6492 [01:30<00:05, 67.67it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  95% 6140/6492 [01:30<00:05, 67.78it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  95% 6160/6492 [01:30<00:04, 67.90it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  95% 6180/6492 [01:30<00:04, 68.01it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  96% 6200/6492 [01:31<00:04, 68.12it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  96% 6220/6492 [01:31<00:03, 68.23it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  96% 6240/6492 [01:31<00:03, 68.35it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  96% 6260/6492 [01:31<00:03, 68.46it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  97% 6280/6492 [01:31<00:03, 68.57it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  97% 6300/6492 [01:31<00:02, 68.67it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  97% 6320/6492 [01:31<00:02, 68.78it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  98% 6340/6492 [01:32<00:02, 68.89it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  98% 6360/6492 [01:32<00:01, 69.00it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  98% 6380/6492 [01:32<00:01, 69.10it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  99% 6400/6492 [01:32<00:01, 69.21it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  99% 6420/6492 [01:32<00:01, 69.31it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9:  99% 6440/6492 [01:32<00:00, 69.42it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9: 100% 6460/6492 [01:32<00:00, 69.52it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9: 100% 6480/6492 [01:33<00:00, 69.63it/s, loss=4.41, v_num=35, val_loss=4.420, avg_val_loss=4.420, train_loss=4.430]\n",
            "Epoch 9: 100% 6492/6492 [01:33<00:00, 69.69it/s, loss=4.41, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.430]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.011 >= min_delta = 0.001. New best score: 4.413\n",
            "Epoch 10:  80% 5180/6492 [01:19<00:20, 64.86it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 5200/6492 [01:24<00:21, 61.42it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  80% 5220/6492 [01:24<00:20, 61.56it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  81% 5240/6492 [01:24<00:20, 61.68it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  81% 5260/6492 [01:25<00:19, 61.80it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  81% 5280/6492 [01:25<00:19, 61.92it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  82% 5300/6492 [01:25<00:19, 62.05it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  82% 5320/6492 [01:25<00:18, 62.17it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  82% 5340/6492 [01:25<00:18, 62.29it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  83% 5360/6492 [01:25<00:18, 62.42it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  83% 5380/6492 [01:26<00:17, 62.54it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  83% 5400/6492 [01:26<00:17, 62.66it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  83% 5420/6492 [01:26<00:17, 62.78it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  84% 5440/6492 [01:26<00:16, 62.90it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  84% 5460/6492 [01:26<00:16, 63.02it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  84% 5480/6492 [01:26<00:16, 63.14it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  85% 5500/6492 [01:26<00:15, 63.26it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  85% 5520/6492 [01:27<00:15, 63.39it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  85% 5540/6492 [01:27<00:14, 63.51it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  86% 5560/6492 [01:27<00:14, 63.62it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  86% 5580/6492 [01:27<00:14, 63.74it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  86% 5600/6492 [01:27<00:13, 63.86it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  87% 5620/6492 [01:27<00:13, 63.97it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  87% 5640/6492 [01:28<00:13, 64.09it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  87% 5660/6492 [01:28<00:12, 64.20it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  87% 5680/6492 [01:28<00:12, 64.31it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  88% 5700/6492 [01:28<00:12, 64.43it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  88% 5720/6492 [01:28<00:11, 64.54it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  88% 5740/6492 [01:28<00:11, 64.65it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  89% 5760/6492 [01:28<00:11, 64.76it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  89% 5780/6492 [01:29<00:10, 64.87it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  89% 5800/6492 [01:29<00:10, 64.99it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  90% 5820/6492 [01:29<00:10, 65.10it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  90% 5840/6492 [01:29<00:09, 65.21it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  90% 5860/6492 [01:29<00:09, 65.32it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  91% 5880/6492 [01:29<00:09, 65.43it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  91% 5900/6492 [01:30<00:09, 65.54it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  91% 5920/6492 [01:30<00:08, 65.65it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  91% 5940/6492 [01:30<00:08, 65.77it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  92% 5960/6492 [01:30<00:08, 65.88it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  92% 5980/6492 [01:30<00:07, 65.99it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  92% 6000/6492 [01:30<00:07, 66.11it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  93% 6020/6492 [01:30<00:07, 66.22it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  93% 6040/6492 [01:31<00:06, 66.32it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  93% 6060/6492 [01:31<00:06, 66.43it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  94% 6080/6492 [01:31<00:06, 66.54it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  94% 6100/6492 [01:31<00:05, 66.65it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  94% 6120/6492 [01:31<00:05, 66.76it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  95% 6140/6492 [01:31<00:05, 66.87it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  95% 6160/6492 [01:31<00:04, 66.98it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  95% 6180/6492 [01:32<00:04, 67.09it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  96% 6200/6492 [01:32<00:04, 67.20it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  96% 6220/6492 [01:32<00:04, 67.31it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  96% 6240/6492 [01:32<00:03, 67.42it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  96% 6260/6492 [01:32<00:03, 67.53it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  97% 6280/6492 [01:32<00:03, 67.64it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  97% 6300/6492 [01:32<00:02, 67.75it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  97% 6320/6492 [01:33<00:02, 67.85it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  98% 6340/6492 [01:33<00:02, 67.96it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  98% 6360/6492 [01:33<00:01, 68.07it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  98% 6380/6492 [01:33<00:01, 68.17it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  99% 6400/6492 [01:33<00:01, 68.28it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  99% 6420/6492 [01:33<00:01, 68.38it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10:  99% 6440/6492 [01:34<00:00, 68.49it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10: 100% 6460/6492 [01:34<00:00, 68.59it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10: 100% 6480/6492 [01:34<00:00, 68.70it/s, loss=4.4, v_num=35, val_loss=4.410, avg_val_loss=4.410, train_loss=4.420]\n",
            "Epoch 10: 100% 6492/6492 [01:34<00:00, 68.76it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.420]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.008 >= min_delta = 0.001. New best score: 4.405\n",
            "Epoch 11:  80% 5180/6492 [01:20<00:20, 63.97it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 5200/6492 [01:25<00:21, 60.81it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  80% 5220/6492 [01:25<00:20, 60.95it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  81% 5240/6492 [01:25<00:20, 61.07it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  81% 5260/6492 [01:25<00:20, 61.20it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  81% 5280/6492 [01:26<00:19, 61.32it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  82% 5300/6492 [01:26<00:19, 61.44it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  82% 5320/6492 [01:26<00:19, 61.57it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  82% 5340/6492 [01:26<00:18, 61.69it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  83% 5360/6492 [01:26<00:18, 61.81it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  83% 5380/6492 [01:26<00:17, 61.93it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  83% 5400/6492 [01:27<00:17, 62.06it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  83% 5420/6492 [01:27<00:17, 62.18it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  84% 5440/6492 [01:27<00:16, 62.30it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  84% 5460/6492 [01:27<00:16, 62.42it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  84% 5480/6492 [01:27<00:16, 62.54it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  85% 5500/6492 [01:27<00:15, 62.67it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  85% 5520/6492 [01:27<00:15, 62.79it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  85% 5540/6492 [01:28<00:15, 62.91it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  86% 5560/6492 [01:28<00:14, 63.03it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  86% 5580/6492 [01:28<00:14, 63.14it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  86% 5600/6492 [01:28<00:14, 63.27it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  87% 5620/6492 [01:28<00:13, 63.39it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  87% 5640/6492 [01:28<00:13, 63.51it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  87% 5660/6492 [01:28<00:13, 63.63it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  87% 5680/6492 [01:29<00:12, 63.74it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  88% 5700/6492 [01:29<00:12, 63.86it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  88% 5720/6492 [01:29<00:12, 63.97it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  88% 5740/6492 [01:29<00:11, 64.08it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  89% 5760/6492 [01:29<00:11, 64.18it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  89% 5780/6492 [01:29<00:11, 64.29it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  89% 5800/6492 [01:30<00:10, 64.41it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  90% 5820/6492 [01:30<00:10, 64.52it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  90% 5840/6492 [01:30<00:10, 64.63it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  90% 5860/6492 [01:30<00:09, 64.75it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  91% 5880/6492 [01:30<00:09, 64.85it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  91% 5900/6492 [01:30<00:09, 64.96it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  91% 5920/6492 [01:30<00:08, 65.06it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  91% 5940/6492 [01:31<00:08, 65.16it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  92% 5960/6492 [01:31<00:08, 65.27it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  92% 5980/6492 [01:31<00:07, 65.38it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  92% 6000/6492 [01:31<00:07, 65.49it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  93% 6020/6492 [01:31<00:07, 65.60it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  93% 6040/6492 [01:31<00:06, 65.71it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  93% 6060/6492 [01:32<00:06, 65.81it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  94% 6080/6492 [01:32<00:06, 65.92it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  94% 6100/6492 [01:32<00:05, 66.02it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  94% 6120/6492 [01:32<00:05, 66.13it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  95% 6140/6492 [01:32<00:05, 66.23it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  95% 6160/6492 [01:32<00:05, 66.34it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  95% 6180/6492 [01:33<00:04, 66.44it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  96% 6200/6492 [01:33<00:04, 66.54it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  96% 6220/6492 [01:33<00:04, 66.65it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  96% 6240/6492 [01:33<00:03, 66.75it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  96% 6260/6492 [01:33<00:03, 66.85it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  97% 6280/6492 [01:33<00:03, 66.96it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  97% 6300/6492 [01:33<00:02, 67.07it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  97% 6320/6492 [01:34<00:02, 67.17it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  98% 6340/6492 [01:34<00:02, 67.28it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  98% 6360/6492 [01:34<00:01, 67.39it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  98% 6380/6492 [01:34<00:01, 67.49it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  99% 6400/6492 [01:34<00:01, 67.59it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  99% 6420/6492 [01:34<00:01, 67.69it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11:  99% 6440/6492 [01:35<00:00, 67.79it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11: 100% 6460/6492 [01:35<00:00, 67.89it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11: 100% 6480/6492 [01:35<00:00, 67.99it/s, loss=4.4, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 11: 100% 6492/6492 [01:35<00:00, 68.05it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.005 >= min_delta = 0.001. New best score: 4.400\n",
            "Epoch 12:  80% 5180/6492 [01:21<00:20, 63.85it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 5200/6492 [01:25<00:21, 60.64it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  80% 5220/6492 [01:25<00:20, 60.77it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  81% 5240/6492 [01:26<00:20, 60.89it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  81% 5260/6492 [01:26<00:20, 61.02it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  81% 5280/6492 [01:26<00:19, 61.14it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  82% 5300/6492 [01:26<00:19, 61.26it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  82% 5320/6492 [01:26<00:19, 61.38it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  82% 5340/6492 [01:26<00:18, 61.50it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  83% 5360/6492 [01:26<00:18, 61.62it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  83% 5380/6492 [01:27<00:18, 61.74it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  83% 5400/6492 [01:27<00:17, 61.87it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  83% 5420/6492 [01:27<00:17, 61.99it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  84% 5440/6492 [01:27<00:16, 62.11it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  84% 5460/6492 [01:27<00:16, 62.23it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  84% 5480/6492 [01:27<00:16, 62.35it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  85% 5500/6492 [01:28<00:15, 62.47it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  85% 5520/6492 [01:28<00:15, 62.59it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  85% 5540/6492 [01:28<00:15, 62.71it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  86% 5560/6492 [01:28<00:14, 62.83it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  86% 5580/6492 [01:28<00:14, 62.95it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  86% 5600/6492 [01:28<00:14, 63.07it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  87% 5620/6492 [01:28<00:13, 63.18it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  87% 5640/6492 [01:29<00:13, 63.29it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  87% 5660/6492 [01:29<00:13, 63.40it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  87% 5680/6492 [01:29<00:12, 63.51it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  88% 5700/6492 [01:29<00:12, 63.63it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  88% 5720/6492 [01:29<00:12, 63.75it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  88% 5740/6492 [01:29<00:11, 63.87it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  89% 5760/6492 [01:30<00:11, 63.98it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  89% 5780/6492 [01:30<00:11, 64.10it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  89% 5800/6492 [01:30<00:10, 64.22it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  90% 5820/6492 [01:30<00:10, 64.34it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  90% 5840/6492 [01:30<00:10, 64.45it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  90% 5860/6492 [01:30<00:09, 64.57it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  91% 5880/6492 [01:30<00:09, 64.69it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  91% 5900/6492 [01:31<00:09, 64.80it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  91% 5920/6492 [01:31<00:08, 64.92it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  91% 5940/6492 [01:31<00:08, 65.04it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  92% 5960/6492 [01:31<00:08, 65.15it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  92% 5980/6492 [01:31<00:07, 65.26it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  92% 6000/6492 [01:31<00:07, 65.37it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  93% 6020/6492 [01:31<00:07, 65.48it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  93% 6040/6492 [01:32<00:06, 65.60it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  93% 6060/6492 [01:32<00:06, 65.71it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  94% 6080/6492 [01:32<00:06, 65.82it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  94% 6100/6492 [01:32<00:05, 65.92it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  94% 6120/6492 [01:32<00:05, 66.03it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  95% 6140/6492 [01:32<00:05, 66.14it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  95% 6160/6492 [01:32<00:05, 66.25it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  95% 6180/6492 [01:33<00:04, 66.35it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  96% 6200/6492 [01:33<00:04, 66.46it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  96% 6220/6492 [01:33<00:04, 66.56it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  96% 6240/6492 [01:33<00:03, 66.67it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  96% 6260/6492 [01:33<00:03, 66.77it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  97% 6280/6492 [01:33<00:03, 66.88it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  97% 6300/6492 [01:34<00:02, 66.99it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  97% 6320/6492 [01:34<00:02, 67.09it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  98% 6340/6492 [01:34<00:02, 67.19it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  98% 6360/6492 [01:34<00:01, 67.30it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  98% 6380/6492 [01:34<00:01, 67.40it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  99% 6400/6492 [01:34<00:01, 67.50it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  99% 6420/6492 [01:34<00:01, 67.61it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12:  99% 6440/6492 [01:35<00:00, 67.71it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12: 100% 6460/6492 [01:35<00:00, 67.81it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12: 100% 6480/6492 [01:35<00:00, 67.92it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "Epoch 12: 100% 6492/6492 [01:35<00:00, 67.98it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.400]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.397\n",
            "Epoch 13:  80% 5180/6492 [01:20<00:20, 64.69it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  80% 5200/6492 [01:24<00:21, 61.36it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  80% 5220/6492 [01:24<00:20, 61.48it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  81% 5240/6492 [01:25<00:20, 61.60it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  81% 5260/6492 [01:25<00:19, 61.70it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  81% 5280/6492 [01:25<00:19, 61.81it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  82% 5300/6492 [01:25<00:19, 61.93it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  82% 5320/6492 [01:25<00:18, 62.05it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  82% 5340/6492 [01:25<00:18, 62.17it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  83% 5360/6492 [01:26<00:18, 62.29it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  83% 5380/6492 [01:26<00:17, 62.41it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  83% 5400/6492 [01:26<00:17, 62.53it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  83% 5420/6492 [01:26<00:17, 62.65it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  84% 5440/6492 [01:26<00:16, 62.76it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  84% 5460/6492 [01:26<00:16, 62.87it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  84% 5480/6492 [01:27<00:16, 62.99it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  85% 5500/6492 [01:27<00:15, 63.11it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  85% 5520/6492 [01:27<00:15, 63.23it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  85% 5540/6492 [01:27<00:15, 63.35it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  86% 5560/6492 [01:27<00:14, 63.46it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  86% 5580/6492 [01:27<00:14, 63.57it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  86% 5600/6492 [01:27<00:14, 63.68it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  87% 5620/6492 [01:28<00:13, 63.79it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  87% 5640/6492 [01:28<00:13, 63.91it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  87% 5660/6492 [01:28<00:12, 64.04it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  87% 5680/6492 [01:28<00:12, 64.16it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  88% 5700/6492 [01:28<00:12, 64.28it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  88% 5720/6492 [01:28<00:11, 64.39it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  88% 5740/6492 [01:28<00:11, 64.50it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  89% 5760/6492 [01:29<00:11, 64.62it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  89% 5780/6492 [01:29<00:10, 64.74it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  89% 5800/6492 [01:29<00:10, 64.85it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  90% 5820/6492 [01:29<00:10, 64.97it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  90% 5840/6492 [01:29<00:10, 65.09it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  90% 5860/6492 [01:29<00:09, 65.21it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  91% 5880/6492 [01:30<00:09, 65.32it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  91% 5900/6492 [01:30<00:09, 65.44it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  91% 5920/6492 [01:30<00:08, 65.56it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  91% 5940/6492 [01:30<00:08, 65.67it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  92% 5960/6492 [01:30<00:08, 65.79it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  92% 5980/6492 [01:30<00:07, 65.90it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  92% 6000/6492 [01:30<00:07, 66.01it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  93% 6020/6492 [01:31<00:07, 66.12it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  93% 6040/6492 [01:31<00:06, 66.23it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  93% 6060/6492 [01:31<00:06, 66.34it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  94% 6080/6492 [01:31<00:06, 66.45it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  94% 6100/6492 [01:31<00:05, 66.55it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  94% 6120/6492 [01:31<00:05, 66.64it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  95% 6140/6492 [01:31<00:05, 66.75it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  95% 6160/6492 [01:32<00:04, 66.86it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  95% 6180/6492 [01:32<00:04, 66.97it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  96% 6200/6492 [01:32<00:04, 67.08it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  96% 6220/6492 [01:32<00:04, 67.18it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  96% 6240/6492 [01:32<00:03, 67.29it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  96% 6260/6492 [01:32<00:03, 67.40it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  97% 6280/6492 [01:33<00:03, 67.51it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  97% 6300/6492 [01:33<00:02, 67.61it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  97% 6320/6492 [01:33<00:02, 67.72it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  98% 6340/6492 [01:33<00:02, 67.83it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  98% 6360/6492 [01:33<00:01, 67.94it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  98% 6380/6492 [01:33<00:01, 68.05it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  99% 6400/6492 [01:33<00:01, 68.16it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  99% 6420/6492 [01:34<00:01, 68.27it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13:  99% 6440/6492 [01:34<00:00, 68.38it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13: 100% 6460/6492 [01:34<00:00, 68.49it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13: 100% 6480/6492 [01:34<00:00, 68.60it/s, loss=4.39, v_num=35, val_loss=4.400, avg_val_loss=4.400, train_loss=4.390]\n",
            "Epoch 13: 100% 6492/6492 [01:34<00:00, 68.66it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.393\n",
            "Epoch 14:  80% 5180/6492 [01:19<00:20, 64.99it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  80% 5200/6492 [01:24<00:20, 61.63it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  80% 5220/6492 [01:24<00:20, 61.77it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  81% 5240/6492 [01:24<00:20, 61.90it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  81% 5260/6492 [01:24<00:19, 62.02it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  81% 5280/6492 [01:24<00:19, 62.15it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  82% 5300/6492 [01:25<00:19, 62.28it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  82% 5320/6492 [01:25<00:18, 62.40it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  82% 5340/6492 [01:25<00:18, 62.53it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  83% 5360/6492 [01:25<00:18, 62.65it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  83% 5380/6492 [01:25<00:17, 62.77it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  83% 5400/6492 [01:25<00:17, 62.90it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  83% 5420/6492 [01:26<00:17, 63.02it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  84% 5440/6492 [01:26<00:16, 63.15it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  84% 5460/6492 [01:26<00:16, 63.27it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  84% 5480/6492 [01:26<00:15, 63.39it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  85% 5500/6492 [01:26<00:15, 63.51it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  85% 5520/6492 [01:26<00:15, 63.64it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  85% 5540/6492 [01:26<00:14, 63.76it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  86% 5560/6492 [01:27<00:14, 63.88it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  86% 5580/6492 [01:27<00:14, 64.00it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  86% 5600/6492 [01:27<00:13, 64.12it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  87% 5620/6492 [01:27<00:13, 64.24it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  87% 5640/6492 [01:27<00:13, 64.36it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  87% 5660/6492 [01:27<00:12, 64.48it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  87% 5680/6492 [01:27<00:12, 64.60it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  88% 5700/6492 [01:28<00:12, 64.72it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  88% 5720/6492 [01:28<00:11, 64.83it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  88% 5740/6492 [01:28<00:11, 64.95it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  89% 5760/6492 [01:28<00:11, 65.06it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  89% 5780/6492 [01:28<00:10, 65.18it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  89% 5800/6492 [01:28<00:10, 65.29it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  90% 5820/6492 [01:28<00:10, 65.41it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  90% 5840/6492 [01:29<00:09, 65.54it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  90% 5860/6492 [01:29<00:09, 65.65it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  91% 5880/6492 [01:29<00:09, 65.77it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  91% 5900/6492 [01:29<00:08, 65.89it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  91% 5920/6492 [01:29<00:08, 66.01it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  91% 5940/6492 [01:29<00:08, 66.12it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  92% 5960/6492 [01:29<00:08, 66.23it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  92% 5980/6492 [01:30<00:07, 66.35it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  92% 6000/6492 [01:30<00:07, 66.46it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  93% 6020/6492 [01:30<00:07, 66.57it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  93% 6040/6492 [01:30<00:06, 66.68it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  93% 6060/6492 [01:30<00:06, 66.79it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  94% 6080/6492 [01:30<00:06, 66.91it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  94% 6100/6492 [01:31<00:05, 67.02it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  94% 6120/6492 [01:31<00:05, 67.13it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  95% 6140/6492 [01:31<00:05, 67.24it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  95% 6160/6492 [01:31<00:04, 67.36it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  95% 6180/6492 [01:31<00:04, 67.47it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  96% 6200/6492 [01:31<00:04, 67.58it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  96% 6220/6492 [01:31<00:04, 67.69it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  96% 6240/6492 [01:32<00:03, 67.80it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  96% 6260/6492 [01:32<00:03, 67.91it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  97% 6280/6492 [01:32<00:03, 68.02it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  97% 6300/6492 [01:32<00:02, 68.13it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  97% 6320/6492 [01:32<00:02, 68.24it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  98% 6340/6492 [01:32<00:02, 68.34it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  98% 6360/6492 [01:32<00:01, 68.45it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  98% 6380/6492 [01:33<00:01, 68.55it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  99% 6400/6492 [01:33<00:01, 68.66it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  99% 6420/6492 [01:33<00:01, 68.76it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14:  99% 6440/6492 [01:33<00:00, 68.87it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14: 100% 6460/6492 [01:33<00:00, 68.97it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14: 100% 6480/6492 [01:33<00:00, 69.08it/s, loss=4.39, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 14: 100% 6492/6492 [01:33<00:00, 69.13it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.391\n",
            "Epoch 15:  80% 5180/6492 [01:20<00:20, 64.11it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  80% 5200/6492 [01:25<00:21, 60.80it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  80% 5220/6492 [01:25<00:20, 60.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  81% 5240/6492 [01:25<00:20, 61.06it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  81% 5260/6492 [01:25<00:20, 61.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  81% 5280/6492 [01:26<00:19, 61.31it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  82% 5300/6492 [01:26<00:19, 61.43it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  82% 5320/6492 [01:26<00:19, 61.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  82% 5340/6492 [01:26<00:18, 61.69it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  83% 5360/6492 [01:26<00:18, 61.81it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  83% 5380/6492 [01:26<00:17, 61.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  83% 5400/6492 [01:27<00:17, 62.06it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  83% 5420/6492 [01:27<00:17, 62.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  84% 5440/6492 [01:27<00:16, 62.31it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  84% 5460/6492 [01:27<00:16, 62.44it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  84% 5480/6492 [01:27<00:16, 62.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  85% 5500/6492 [01:27<00:15, 62.69it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  85% 5520/6492 [01:27<00:15, 62.81it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  85% 5540/6492 [01:28<00:15, 62.94it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  86% 5560/6492 [01:28<00:14, 63.06it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  86% 5580/6492 [01:28<00:14, 63.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  86% 5600/6492 [01:28<00:14, 63.30it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  87% 5620/6492 [01:28<00:13, 63.43it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  87% 5640/6492 [01:28<00:13, 63.55it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  87% 5660/6492 [01:28<00:13, 63.67it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  87% 5680/6492 [01:29<00:12, 63.78it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  88% 5700/6492 [01:29<00:12, 63.90it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  88% 5720/6492 [01:29<00:12, 64.02it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  88% 5740/6492 [01:29<00:11, 64.14it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  89% 5760/6492 [01:29<00:11, 64.25it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  89% 5780/6492 [01:29<00:11, 64.37it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  89% 5800/6492 [01:29<00:10, 64.49it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  90% 5820/6492 [01:30<00:10, 64.60it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  90% 5840/6492 [01:30<00:10, 64.71it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  90% 5860/6492 [01:30<00:09, 64.82it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  91% 5880/6492 [01:30<00:09, 64.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  91% 5900/6492 [01:30<00:09, 65.04it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  91% 5920/6492 [01:30<00:08, 65.15it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  91% 5940/6492 [01:31<00:08, 65.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  92% 5960/6492 [01:31<00:08, 65.38it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  92% 5980/6492 [01:31<00:07, 65.48it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  92% 6000/6492 [01:31<00:07, 65.59it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  93% 6020/6492 [01:31<00:07, 65.70it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  93% 6040/6492 [01:31<00:06, 65.80it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  93% 6060/6492 [01:31<00:06, 65.91it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  94% 6080/6492 [01:32<00:06, 66.02it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  94% 6100/6492 [01:32<00:05, 66.13it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  94% 6120/6492 [01:32<00:05, 66.24it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  95% 6140/6492 [01:32<00:05, 66.35it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  95% 6160/6492 [01:32<00:04, 66.45it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  95% 6180/6492 [01:32<00:04, 66.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  96% 6200/6492 [01:32<00:04, 66.67it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  96% 6220/6492 [01:33<00:04, 66.78it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  96% 6240/6492 [01:33<00:03, 66.89it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  96% 6260/6492 [01:33<00:03, 67.00it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  97% 6280/6492 [01:33<00:03, 67.11it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  97% 6300/6492 [01:33<00:02, 67.21it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  97% 6320/6492 [01:33<00:02, 67.32it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  98% 6340/6492 [01:34<00:02, 67.43it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  98% 6360/6492 [01:34<00:01, 67.54it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  98% 6380/6492 [01:34<00:01, 67.64it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  99% 6400/6492 [01:34<00:01, 67.75it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  99% 6420/6492 [01:34<00:01, 67.85it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15:  99% 6440/6492 [01:34<00:00, 67.95it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15: 100% 6460/6492 [01:34<00:00, 68.05it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15: 100% 6480/6492 [01:35<00:00, 68.16it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "Epoch 15: 100% 6492/6492 [01:35<00:00, 68.21it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.390]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.388\n",
            "Epoch 16:  80% 5180/6492 [01:20<00:20, 64.17it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  80% 5200/6492 [01:25<00:21, 60.90it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  80% 5220/6492 [01:25<00:20, 61.03it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  81% 5240/6492 [01:25<00:20, 61.16it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  81% 5260/6492 [01:25<00:20, 61.28it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  81% 5280/6492 [01:25<00:19, 61.41it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  82% 5300/6492 [01:26<00:19, 61.53it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  82% 5320/6492 [01:26<00:19, 61.65it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  82% 5340/6492 [01:26<00:18, 61.78it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  83% 5360/6492 [01:26<00:18, 61.90it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  83% 5380/6492 [01:26<00:17, 62.03it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  83% 5400/6492 [01:26<00:17, 62.16it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  83% 5420/6492 [01:27<00:17, 62.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  84% 5440/6492 [01:27<00:16, 62.40it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  84% 5460/6492 [01:27<00:16, 62.51it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  84% 5480/6492 [01:27<00:16, 62.64it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  85% 5500/6492 [01:27<00:15, 62.76it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  85% 5520/6492 [01:27<00:15, 62.88it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  85% 5540/6492 [01:27<00:15, 63.00it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  86% 5560/6492 [01:28<00:14, 63.11it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  86% 5580/6492 [01:28<00:14, 63.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  86% 5600/6492 [01:28<00:14, 63.34it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  87% 5620/6492 [01:28<00:13, 63.46it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  87% 5640/6492 [01:28<00:13, 63.59it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  87% 5660/6492 [01:28<00:13, 63.70it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  87% 5680/6492 [01:28<00:12, 63.82it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  88% 5700/6492 [01:29<00:12, 63.94it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  88% 5720/6492 [01:29<00:12, 64.06it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  88% 5740/6492 [01:29<00:11, 64.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  89% 5760/6492 [01:29<00:11, 64.30it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  89% 5780/6492 [01:29<00:11, 64.41it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  89% 5800/6492 [01:29<00:10, 64.53it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  90% 5820/6492 [01:30<00:10, 64.64it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  90% 5840/6492 [01:30<00:10, 64.75it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  90% 5860/6492 [01:30<00:09, 64.87it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  91% 5880/6492 [01:30<00:09, 64.98it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  91% 5900/6492 [01:30<00:09, 65.09it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  91% 5920/6492 [01:30<00:08, 65.19it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  91% 5940/6492 [01:30<00:08, 65.30it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  92% 5960/6492 [01:31<00:08, 65.41it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  92% 5980/6492 [01:31<00:07, 65.53it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  92% 6000/6492 [01:31<00:07, 65.63it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  93% 6020/6492 [01:31<00:07, 65.74it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  93% 6040/6492 [01:31<00:06, 65.84it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  93% 6060/6492 [01:31<00:06, 65.94it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  94% 6080/6492 [01:32<00:06, 66.05it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  94% 6100/6492 [01:32<00:05, 66.17it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  94% 6120/6492 [01:32<00:05, 66.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  95% 6140/6492 [01:32<00:05, 66.38it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  95% 6160/6492 [01:32<00:04, 66.49it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  95% 6180/6492 [01:32<00:04, 66.60it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  96% 6200/6492 [01:32<00:04, 66.70it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  96% 6220/6492 [01:33<00:04, 66.81it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  96% 6240/6492 [01:33<00:03, 66.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  96% 6260/6492 [01:33<00:03, 67.03it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  97% 6280/6492 [01:33<00:03, 67.13it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  97% 6300/6492 [01:33<00:02, 67.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  97% 6320/6492 [01:33<00:02, 67.34it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  98% 6340/6492 [01:34<00:02, 67.44it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  98% 6360/6492 [01:34<00:01, 67.55it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  98% 6380/6492 [01:34<00:01, 67.65it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  99% 6400/6492 [01:34<00:01, 67.76it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  99% 6420/6492 [01:34<00:01, 67.87it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16:  99% 6440/6492 [01:34<00:00, 67.98it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16: 100% 6460/6492 [01:34<00:00, 68.08it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16: 100% 6480/6492 [01:35<00:00, 68.19it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 16: 100% 6492/6492 [01:35<00:00, 68.24it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.387\n",
            "Epoch 17:  80% 5180/6492 [01:20<00:20, 64.68it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  80% 5200/6492 [01:24<00:21, 61.37it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  80% 5220/6492 [01:24<00:20, 61.50it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  81% 5240/6492 [01:25<00:20, 61.63it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  81% 5260/6492 [01:25<00:19, 61.75it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  81% 5280/6492 [01:25<00:19, 61.87it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  82% 5300/6492 [01:25<00:19, 61.99it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  82% 5320/6492 [01:25<00:18, 62.12it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  82% 5340/6492 [01:25<00:18, 62.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  83% 5360/6492 [01:25<00:18, 62.36it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  83% 5380/6492 [01:26<00:17, 62.48it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  83% 5400/6492 [01:26<00:17, 62.59it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  83% 5420/6492 [01:26<00:17, 62.71it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  84% 5440/6492 [01:26<00:16, 62.83it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  84% 5460/6492 [01:26<00:16, 62.95it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  84% 5480/6492 [01:26<00:16, 63.07it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  85% 5500/6492 [01:27<00:15, 63.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  85% 5520/6492 [01:27<00:15, 63.30it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  85% 5540/6492 [01:27<00:15, 63.42it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  86% 5560/6492 [01:27<00:14, 63.54it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  86% 5580/6492 [01:27<00:14, 63.66it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  86% 5600/6492 [01:27<00:13, 63.78it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  87% 5620/6492 [01:27<00:13, 63.89it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  87% 5640/6492 [01:28<00:13, 64.00it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  87% 5660/6492 [01:28<00:12, 64.11it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  87% 5680/6492 [01:28<00:12, 64.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  88% 5700/6492 [01:28<00:12, 64.35it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  88% 5720/6492 [01:28<00:11, 64.46it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  88% 5740/6492 [01:28<00:11, 64.57it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  89% 5760/6492 [01:29<00:11, 64.68it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  89% 5780/6492 [01:29<00:10, 64.79it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  89% 5800/6492 [01:29<00:10, 64.90it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  90% 5820/6492 [01:29<00:10, 65.01it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  90% 5840/6492 [01:29<00:10, 65.12it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  90% 5860/6492 [01:29<00:09, 65.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  91% 5880/6492 [01:29<00:09, 65.33it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  91% 5900/6492 [01:30<00:09, 65.44it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  91% 5920/6492 [01:30<00:08, 65.55it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  91% 5940/6492 [01:30<00:08, 65.66it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  92% 5960/6492 [01:30<00:08, 65.77it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  92% 5980/6492 [01:30<00:07, 65.88it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  92% 6000/6492 [01:30<00:07, 65.99it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  93% 6020/6492 [01:31<00:07, 66.09it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  93% 6040/6492 [01:31<00:06, 66.20it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  93% 6060/6492 [01:31<00:06, 66.32it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  94% 6080/6492 [01:31<00:06, 66.43it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  94% 6100/6492 [01:31<00:05, 66.54it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  94% 6120/6492 [01:31<00:05, 66.65it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  95% 6140/6492 [01:31<00:05, 66.77it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  95% 6160/6492 [01:32<00:04, 66.88it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  95% 6180/6492 [01:32<00:04, 66.99it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  96% 6200/6492 [01:32<00:04, 66.99it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  96% 6220/6492 [01:32<00:04, 67.10it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  96% 6240/6492 [01:32<00:03, 67.21it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  96% 6260/6492 [01:32<00:03, 67.32it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  97% 6280/6492 [01:33<00:03, 67.43it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  97% 6300/6492 [01:33<00:02, 67.54it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  97% 6320/6492 [01:33<00:02, 67.64it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  98% 6340/6492 [01:33<00:02, 67.75it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  98% 6360/6492 [01:33<00:01, 67.86it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  98% 6380/6492 [01:33<00:01, 67.97it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  99% 6400/6492 [01:34<00:01, 68.08it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  99% 6420/6492 [01:34<00:01, 68.18it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17:  99% 6440/6492 [01:34<00:00, 68.29it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17: 100% 6460/6492 [01:34<00:00, 68.39it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17: 100% 6480/6492 [01:34<00:00, 68.50it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 17: 100% 6492/6492 [01:34<00:00, 68.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.385\n",
            "Epoch 18:  80% 5180/6492 [01:21<00:20, 63.41it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  80% 5200/6492 [01:26<00:21, 60.14it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  80% 5220/6492 [01:26<00:21, 60.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  81% 5240/6492 [01:26<00:20, 60.39it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  81% 5260/6492 [01:26<00:20, 60.51it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  81% 5280/6492 [01:27<00:19, 60.64it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  82% 5300/6492 [01:27<00:19, 60.75it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  82% 5320/6492 [01:27<00:19, 60.86it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  82% 5340/6492 [01:27<00:18, 60.98it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  83% 5360/6492 [01:27<00:18, 61.10it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  83% 5380/6492 [01:27<00:18, 61.22it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  83% 5400/6492 [01:28<00:17, 61.35it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  83% 5420/6492 [01:28<00:17, 61.47it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  84% 5440/6492 [01:28<00:17, 61.59it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  84% 5460/6492 [01:28<00:16, 61.71it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  84% 5480/6492 [01:28<00:16, 61.84it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  85% 5500/6492 [01:28<00:16, 61.96it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  85% 5520/6492 [01:28<00:15, 62.08it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  85% 5540/6492 [01:29<00:15, 62.20it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  86% 5560/6492 [01:29<00:14, 62.32it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  86% 5580/6492 [01:29<00:14, 62.44it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  86% 5600/6492 [01:29<00:14, 62.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  87% 5620/6492 [01:29<00:13, 62.68it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  87% 5640/6492 [01:29<00:13, 62.81it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  87% 5660/6492 [01:29<00:13, 62.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  87% 5680/6492 [01:30<00:12, 63.05it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  88% 5700/6492 [01:30<00:12, 63.17it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  88% 5720/6492 [01:30<00:12, 63.29it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  88% 5740/6492 [01:30<00:11, 63.41it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  89% 5760/6492 [01:30<00:11, 63.53it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  89% 5780/6492 [01:30<00:11, 63.65it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  89% 5800/6492 [01:30<00:10, 63.77it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  90% 5820/6492 [01:31<00:10, 63.89it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  90% 5840/6492 [01:31<00:10, 64.00it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  90% 5860/6492 [01:31<00:09, 64.12it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  91% 5880/6492 [01:31<00:09, 64.24it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  91% 5900/6492 [01:31<00:09, 64.36it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  91% 5920/6492 [01:31<00:08, 64.48it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  91% 5940/6492 [01:31<00:08, 64.59it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  92% 5960/6492 [01:32<00:08, 64.71it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  92% 5980/6492 [01:32<00:07, 64.82it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  92% 6000/6492 [01:32<00:07, 64.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  93% 6020/6492 [01:32<00:07, 65.04it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  93% 6040/6492 [01:32<00:06, 65.16it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  93% 6060/6492 [01:32<00:06, 65.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  94% 6080/6492 [01:32<00:06, 65.38it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  94% 6100/6492 [01:33<00:05, 65.49it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  94% 6120/6492 [01:33<00:05, 65.60it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  95% 6140/6492 [01:33<00:05, 65.71it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  95% 6160/6492 [01:33<00:05, 65.82it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  95% 6180/6492 [01:33<00:04, 65.93it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  96% 6200/6492 [01:33<00:04, 66.05it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  96% 6220/6492 [01:34<00:04, 66.16it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  96% 6240/6492 [01:34<00:03, 66.27it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  96% 6260/6492 [01:34<00:03, 66.38it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  97% 6280/6492 [01:34<00:03, 66.49it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  97% 6300/6492 [01:34<00:02, 66.60it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  97% 6320/6492 [01:34<00:02, 66.70it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  98% 6340/6492 [01:34<00:02, 66.81it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  98% 6360/6492 [01:35<00:01, 66.91it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  98% 6380/6492 [01:35<00:01, 67.02it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  99% 6400/6492 [01:35<00:01, 67.12it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  99% 6420/6492 [01:35<00:01, 67.23it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18:  99% 6440/6492 [01:35<00:00, 67.34it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18: 100% 6460/6492 [01:35<00:00, 67.45it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18: 100% 6480/6492 [01:35<00:00, 67.56it/s, loss=4.38, v_num=35, val_loss=4.390, avg_val_loss=4.390, train_loss=4.380]\n",
            "Epoch 18: 100% 6492/6492 [01:36<00:00, 67.62it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.384\n",
            "Epoch 19:  80% 5180/6492 [01:20<00:20, 64.04it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  80% 5200/6492 [01:25<00:21, 60.59it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  80% 5220/6492 [01:25<00:20, 60.72it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  81% 5240/6492 [01:26<00:20, 60.85it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  81% 5260/6492 [01:26<00:20, 60.97it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  81% 5280/6492 [01:26<00:19, 61.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  82% 5300/6492 [01:26<00:19, 61.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  82% 5320/6492 [01:26<00:19, 61.34it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  82% 5340/6492 [01:26<00:18, 61.46it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  83% 5360/6492 [01:27<00:18, 61.58it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  83% 5380/6492 [01:27<00:18, 61.71it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  83% 5400/6492 [01:27<00:17, 61.83it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  83% 5420/6492 [01:27<00:17, 61.95it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  84% 5440/6492 [01:27<00:16, 62.07it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  84% 5460/6492 [01:27<00:16, 62.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  84% 5480/6492 [01:27<00:16, 62.31it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  85% 5500/6492 [01:28<00:15, 62.42it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  85% 5520/6492 [01:28<00:15, 62.54it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  85% 5540/6492 [01:28<00:15, 62.66it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  86% 5560/6492 [01:28<00:14, 62.77it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  86% 5580/6492 [01:28<00:14, 62.89it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  86% 5600/6492 [01:28<00:14, 63.01it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  87% 5620/6492 [01:29<00:13, 63.12it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  87% 5640/6492 [01:29<00:13, 63.24it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  87% 5660/6492 [01:29<00:13, 63.37it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  87% 5680/6492 [01:29<00:12, 63.49it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  88% 5700/6492 [01:29<00:12, 63.61it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  88% 5720/6492 [01:29<00:12, 63.72it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  88% 5740/6492 [01:29<00:11, 63.84it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  89% 5760/6492 [01:30<00:11, 63.96it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  89% 5780/6492 [01:30<00:11, 64.07it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  89% 5800/6492 [01:30<00:10, 64.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  90% 5820/6492 [01:30<00:10, 64.29it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  90% 5840/6492 [01:30<00:10, 64.40it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  90% 5860/6492 [01:30<00:09, 64.52it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  91% 5880/6492 [01:30<00:09, 64.64it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  91% 5900/6492 [01:31<00:09, 64.75it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  91% 5920/6492 [01:31<00:08, 64.86it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  91% 5940/6492 [01:31<00:08, 64.98it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  92% 5960/6492 [01:31<00:08, 65.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  92% 5980/6492 [01:31<00:07, 65.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  92% 6000/6492 [01:31<00:07, 65.32it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  93% 6020/6492 [01:31<00:07, 65.44it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  93% 6040/6492 [01:32<00:06, 65.55it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  93% 6060/6492 [01:32<00:06, 65.66it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  94% 6080/6492 [01:32<00:06, 65.76it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  94% 6100/6492 [01:32<00:05, 65.87it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  94% 6120/6492 [01:32<00:05, 65.98it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  95% 6140/6492 [01:32<00:05, 66.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  95% 6160/6492 [01:33<00:05, 66.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  95% 6180/6492 [01:33<00:04, 66.31it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  96% 6200/6492 [01:33<00:04, 66.42it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  96% 6220/6492 [01:33<00:04, 66.53it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  96% 6240/6492 [01:33<00:03, 66.64it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  96% 6260/6492 [01:33<00:03, 66.74it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  97% 6280/6492 [01:33<00:03, 66.85it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  97% 6300/6492 [01:34<00:02, 66.96it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  97% 6320/6492 [01:34<00:02, 67.07it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  98% 6340/6492 [01:34<00:02, 67.17it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  98% 6360/6492 [01:34<00:01, 67.28it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  98% 6380/6492 [01:34<00:01, 67.39it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  99% 6400/6492 [01:34<00:01, 67.49it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  99% 6420/6492 [01:34<00:01, 67.59it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19:  99% 6440/6492 [01:35<00:00, 67.69it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19: 100% 6460/6492 [01:35<00:00, 67.78it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19: 100% 6480/6492 [01:35<00:00, 67.88it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 19: 100% 6492/6492 [01:35<00:00, 67.94it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.383\n",
            "Epoch 20:  80% 5180/6492 [01:21<00:20, 63.89it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  80% 5200/6492 [01:25<00:21, 60.64it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  80% 5220/6492 [01:25<00:20, 60.77it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  81% 5240/6492 [01:26<00:20, 60.89it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  81% 5260/6492 [01:26<00:20, 61.02it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  81% 5280/6492 [01:26<00:19, 61.14it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  82% 5300/6492 [01:26<00:19, 61.27it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  82% 5320/6492 [01:26<00:19, 61.39it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  82% 5340/6492 [01:26<00:18, 61.52it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  83% 5360/6492 [01:26<00:18, 61.64it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  83% 5380/6492 [01:27<00:18, 61.76it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  83% 5400/6492 [01:27<00:17, 61.88it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  83% 5420/6492 [01:27<00:17, 62.00it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  84% 5440/6492 [01:27<00:16, 62.12it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  84% 5460/6492 [01:27<00:16, 62.24it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  84% 5480/6492 [01:27<00:16, 62.36it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  85% 5500/6492 [01:28<00:15, 62.47it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  85% 5520/6492 [01:28<00:15, 62.59it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  85% 5540/6492 [01:28<00:15, 62.71it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  86% 5560/6492 [01:28<00:14, 62.82it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  86% 5580/6492 [01:28<00:14, 62.94it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  86% 5600/6492 [01:28<00:14, 63.06it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  87% 5620/6492 [01:28<00:13, 63.18it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  87% 5640/6492 [01:29<00:13, 63.30it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  87% 5660/6492 [01:29<00:13, 63.41it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  87% 5680/6492 [01:29<00:12, 63.52it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  88% 5700/6492 [01:29<00:12, 63.63it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  88% 5720/6492 [01:29<00:12, 63.74it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  88% 5740/6492 [01:29<00:11, 63.85it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  89% 5760/6492 [01:30<00:11, 63.97it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  89% 5780/6492 [01:30<00:11, 64.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  89% 5800/6492 [01:30<00:10, 64.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  90% 5820/6492 [01:30<00:10, 64.31it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  90% 5840/6492 [01:30<00:10, 64.42it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  90% 5860/6492 [01:30<00:09, 64.53it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  91% 5880/6492 [01:30<00:09, 64.64it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  91% 5900/6492 [01:31<00:09, 64.76it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  91% 5920/6492 [01:31<00:08, 64.87it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  91% 5940/6492 [01:31<00:08, 64.99it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  92% 5960/6492 [01:31<00:08, 65.10it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  92% 5980/6492 [01:31<00:07, 65.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  92% 6000/6492 [01:31<00:07, 65.30it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  93% 6020/6492 [01:32<00:07, 65.42it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  93% 6040/6492 [01:32<00:06, 65.53it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  93% 6060/6492 [01:32<00:06, 65.63it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  94% 6080/6492 [01:32<00:06, 65.73it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  94% 6100/6492 [01:32<00:05, 65.84it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  94% 6120/6492 [01:32<00:05, 65.95it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  95% 6140/6492 [01:32<00:05, 66.06it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  95% 6160/6492 [01:33<00:05, 66.17it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  95% 6180/6492 [01:33<00:04, 66.28it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  96% 6200/6492 [01:33<00:04, 66.37it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  96% 6220/6492 [01:33<00:04, 66.47it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  96% 6240/6492 [01:33<00:03, 66.57it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  96% 6260/6492 [01:33<00:03, 66.67it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  97% 6280/6492 [01:34<00:03, 66.78it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  97% 6300/6492 [01:34<00:02, 66.89it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  97% 6320/6492 [01:34<00:02, 66.99it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  98% 6340/6492 [01:34<00:02, 67.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  98% 6360/6492 [01:34<00:01, 67.20it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  98% 6380/6492 [01:34<00:01, 67.30it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  99% 6400/6492 [01:34<00:01, 67.41it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  99% 6420/6492 [01:35<00:01, 67.52it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20:  99% 6440/6492 [01:35<00:00, 67.63it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20: 100% 6460/6492 [01:35<00:00, 67.73it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20: 100% 6480/6492 [01:35<00:00, 67.84it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 20: 100% 6492/6492 [01:35<00:00, 67.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.381\n",
            "Epoch 21:  80% 5180/6492 [01:20<00:20, 64.06it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  80% 5200/6492 [01:25<00:21, 60.73it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  80% 5220/6492 [01:25<00:20, 60.86it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  81% 5240/6492 [01:25<00:20, 60.97it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  81% 5260/6492 [01:26<00:20, 61.09it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  81% 5280/6492 [01:26<00:19, 61.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  82% 5300/6492 [01:26<00:19, 61.33it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  82% 5320/6492 [01:26<00:19, 61.45it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  82% 5340/6492 [01:26<00:18, 61.54it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  83% 5360/6492 [01:26<00:18, 61.66it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  83% 5380/6492 [01:27<00:17, 61.79it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  83% 5400/6492 [01:27<00:17, 61.91it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  83% 5420/6492 [01:27<00:17, 62.03it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  84% 5440/6492 [01:27<00:16, 62.15it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  84% 5460/6492 [01:27<00:16, 62.27it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  84% 5480/6492 [01:27<00:16, 62.38it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  85% 5500/6492 [01:28<00:15, 62.50it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  85% 5520/6492 [01:28<00:15, 62.62it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  85% 5540/6492 [01:28<00:15, 62.74it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  86% 5560/6492 [01:28<00:14, 62.86it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  86% 5580/6492 [01:28<00:14, 62.98it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  86% 5600/6492 [01:28<00:14, 63.10it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  87% 5620/6492 [01:28<00:13, 63.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  87% 5640/6492 [01:29<00:13, 63.32it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  87% 5660/6492 [01:29<00:13, 63.44it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  87% 5680/6492 [01:29<00:12, 63.55it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  88% 5700/6492 [01:29<00:12, 63.65it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  88% 5720/6492 [01:29<00:12, 63.76it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  88% 5740/6492 [01:29<00:11, 63.87it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  89% 5760/6492 [01:30<00:11, 63.98it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  89% 5780/6492 [01:30<00:11, 64.10it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  89% 5800/6492 [01:30<00:10, 64.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  90% 5820/6492 [01:30<00:10, 64.32it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  90% 5840/6492 [01:30<00:10, 64.43it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  90% 5860/6492 [01:30<00:09, 64.54it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  91% 5880/6492 [01:30<00:09, 64.66it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  91% 5900/6492 [01:31<00:09, 64.77it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  91% 5920/6492 [01:31<00:08, 64.87it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  91% 5940/6492 [01:31<00:08, 64.99it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  92% 5960/6492 [01:31<00:08, 65.10it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  92% 5980/6492 [01:31<00:07, 65.21it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  92% 6000/6492 [01:31<00:07, 65.32it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  93% 6020/6492 [01:32<00:07, 65.43it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  93% 6040/6492 [01:32<00:06, 65.54it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  93% 6060/6492 [01:32<00:06, 65.65it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  94% 6080/6492 [01:32<00:06, 65.75it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  94% 6100/6492 [01:32<00:05, 65.86it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  94% 6120/6492 [01:32<00:05, 65.97it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  95% 6140/6492 [01:32<00:05, 66.08it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  95% 6160/6492 [01:33<00:05, 66.19it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  95% 6180/6492 [01:33<00:04, 66.30it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  96% 6200/6492 [01:33<00:04, 66.41it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  96% 6220/6492 [01:33<00:04, 66.52it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  96% 6240/6492 [01:33<00:03, 66.63it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  96% 6260/6492 [01:33<00:03, 66.74it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  97% 6280/6492 [01:33<00:03, 66.85it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  97% 6300/6492 [01:34<00:02, 66.96it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  97% 6320/6492 [01:34<00:02, 67.07it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  98% 6340/6492 [01:34<00:02, 67.18it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  98% 6360/6492 [01:34<00:01, 67.29it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  98% 6380/6492 [01:34<00:01, 67.39it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  99% 6400/6492 [01:34<00:01, 67.50it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  99% 6420/6492 [01:34<00:01, 67.61it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21:  99% 6440/6492 [01:35<00:00, 67.71it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21: 100% 6460/6492 [01:35<00:00, 67.82it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21: 100% 6480/6492 [01:35<00:00, 67.93it/s, loss=4.38, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 21: 100% 6492/6492 [01:35<00:00, 67.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  80% 5180/6492 [01:21<00:20, 63.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  80% 5200/6492 [01:26<00:21, 59.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  80% 5220/6492 [01:26<00:21, 60.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  81% 5240/6492 [01:26<00:20, 60.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  81% 5260/6492 [01:27<00:20, 60.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  81% 5280/6492 [01:27<00:20, 60.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  82% 5300/6492 [01:27<00:19, 60.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  82% 5320/6492 [01:27<00:19, 60.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  82% 5340/6492 [01:27<00:18, 60.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  83% 5360/6492 [01:27<00:18, 60.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  83% 5380/6492 [01:28<00:18, 61.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  83% 5400/6492 [01:28<00:17, 61.23it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  83% 5420/6492 [01:28<00:17, 61.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  84% 5440/6492 [01:28<00:17, 61.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  84% 5460/6492 [01:28<00:16, 61.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  84% 5480/6492 [01:28<00:16, 61.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  85% 5500/6492 [01:28<00:16, 61.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  85% 5520/6492 [01:29<00:15, 61.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  85% 5540/6492 [01:29<00:15, 62.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  86% 5560/6492 [01:29<00:14, 62.21it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  86% 5580/6492 [01:29<00:14, 62.33it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  86% 5600/6492 [01:29<00:14, 62.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  87% 5620/6492 [01:29<00:13, 62.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  87% 5640/6492 [01:29<00:13, 62.68it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  87% 5660/6492 [01:30<00:13, 62.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  87% 5680/6492 [01:30<00:12, 62.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  88% 5700/6492 [01:30<00:12, 63.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  88% 5720/6492 [01:30<00:12, 63.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  88% 5740/6492 [01:30<00:11, 63.27it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  89% 5760/6492 [01:30<00:11, 63.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  89% 5780/6492 [01:31<00:11, 63.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  89% 5800/6492 [01:31<00:10, 63.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  90% 5820/6492 [01:31<00:10, 63.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  90% 5840/6492 [01:31<00:10, 63.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  90% 5860/6492 [01:31<00:09, 63.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  91% 5880/6492 [01:31<00:09, 64.07it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  91% 5900/6492 [01:31<00:09, 64.18it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  91% 5920/6492 [01:32<00:08, 64.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  91% 5940/6492 [01:32<00:08, 64.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  92% 5960/6492 [01:32<00:08, 64.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  92% 5980/6492 [01:32<00:07, 64.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  92% 6000/6492 [01:32<00:07, 64.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  93% 6020/6492 [01:32<00:07, 64.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  93% 6040/6492 [01:33<00:06, 64.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  93% 6060/6492 [01:33<00:06, 65.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  94% 6080/6492 [01:33<00:06, 65.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  94% 6100/6492 [01:33<00:06, 65.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  94% 6120/6492 [01:33<00:05, 65.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  95% 6140/6492 [01:33<00:05, 65.47it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  95% 6160/6492 [01:33<00:05, 65.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  95% 6180/6492 [01:34<00:04, 65.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  96% 6200/6492 [01:34<00:04, 65.78it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  96% 6220/6492 [01:34<00:04, 65.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  96% 6240/6492 [01:34<00:03, 65.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  96% 6260/6492 [01:34<00:03, 66.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  97% 6280/6492 [01:34<00:03, 66.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  97% 6300/6492 [01:35<00:02, 66.30it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  97% 6320/6492 [01:35<00:02, 66.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  98% 6340/6492 [01:35<00:02, 66.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  98% 6360/6492 [01:35<00:01, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  98% 6380/6492 [01:35<00:01, 66.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  99% 6400/6492 [01:35<00:01, 66.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  99% 6420/6492 [01:35<00:01, 66.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22:  99% 6440/6492 [01:36<00:00, 67.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22: 100% 6460/6492 [01:36<00:00, 67.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22: 100% 6480/6492 [01:36<00:00, 67.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "Epoch 22: 100% 6492/6492 [01:36<00:00, 67.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.380]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.380\n",
            "Epoch 23:  80% 5180/6492 [01:22<00:20, 62.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  80% 5200/6492 [01:27<00:21, 59.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  80% 5220/6492 [01:27<00:21, 59.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  81% 5240/6492 [01:27<00:20, 59.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  81% 5260/6492 [01:27<00:20, 59.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  81% 5280/6492 [01:28<00:20, 59.97it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  82% 5300/6492 [01:28<00:19, 60.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  82% 5320/6492 [01:28<00:19, 60.21it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  82% 5340/6492 [01:28<00:19, 60.33it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  83% 5360/6492 [01:28<00:18, 60.44it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  83% 5380/6492 [01:28<00:18, 60.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  83% 5400/6492 [01:28<00:17, 60.68it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  83% 5420/6492 [01:29<00:17, 60.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  84% 5440/6492 [01:29<00:17, 60.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  84% 5460/6492 [01:29<00:16, 61.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  84% 5480/6492 [01:29<00:16, 61.16it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  85% 5500/6492 [01:29<00:16, 61.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  85% 5520/6492 [01:29<00:15, 61.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  85% 5540/6492 [01:30<00:15, 61.52it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  86% 5560/6492 [01:30<00:15, 61.64it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  86% 5580/6492 [01:30<00:14, 61.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  86% 5600/6492 [01:30<00:14, 61.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  87% 5620/6492 [01:30<00:14, 62.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  87% 5640/6492 [01:30<00:13, 62.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  87% 5660/6492 [01:30<00:13, 62.23it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  87% 5680/6492 [01:31<00:13, 62.34it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  88% 5700/6492 [01:31<00:12, 62.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  88% 5720/6492 [01:31<00:12, 62.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  88% 5740/6492 [01:31<00:12, 62.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  89% 5760/6492 [01:31<00:11, 62.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  89% 5780/6492 [01:31<00:11, 62.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  89% 5800/6492 [01:32<00:10, 63.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  90% 5820/6492 [01:32<00:10, 63.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  90% 5840/6492 [01:32<00:10, 63.23it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  90% 5860/6492 [01:32<00:09, 63.34it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  91% 5880/6492 [01:32<00:09, 63.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  91% 5900/6492 [01:32<00:09, 63.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  91% 5920/6492 [01:32<00:08, 63.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  91% 5940/6492 [01:33<00:08, 63.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  92% 5960/6492 [01:33<00:08, 63.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  92% 5980/6492 [01:33<00:08, 63.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  92% 6000/6492 [01:33<00:07, 64.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  93% 6020/6492 [01:33<00:07, 64.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  93% 6040/6492 [01:33<00:07, 64.31it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  93% 6060/6492 [01:34<00:06, 64.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  94% 6080/6492 [01:34<00:06, 64.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  94% 6100/6492 [01:34<00:06, 64.64it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  94% 6120/6492 [01:34<00:05, 64.75it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  95% 6140/6492 [01:34<00:05, 64.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  95% 6160/6492 [01:34<00:05, 64.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  95% 6180/6492 [01:34<00:04, 65.07it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  96% 6200/6492 [01:35<00:04, 65.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  96% 6220/6492 [01:35<00:04, 65.27it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  96% 6240/6492 [01:35<00:03, 65.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  96% 6260/6492 [01:35<00:03, 65.47it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  97% 6280/6492 [01:35<00:03, 65.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  97% 6300/6492 [01:35<00:02, 65.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  97% 6320/6492 [01:36<00:02, 65.75it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  98% 6340/6492 [01:36<00:02, 65.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  98% 6360/6492 [01:36<00:02, 65.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  98% 6380/6492 [01:36<00:01, 66.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  99% 6400/6492 [01:36<00:01, 66.16it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  99% 6420/6492 [01:36<00:01, 66.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23:  99% 6440/6492 [01:37<00:00, 66.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23: 100% 6460/6492 [01:37<00:00, 66.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23: 100% 6480/6492 [01:37<00:00, 66.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 23: 100% 6492/6492 [01:37<00:00, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  80% 5180/6492 [01:22<00:20, 63.01it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  80% 5200/6492 [01:26<00:21, 59.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  80% 5220/6492 [01:27<00:21, 59.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  81% 5240/6492 [01:27<00:20, 60.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  81% 5260/6492 [01:27<00:20, 60.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  81% 5280/6492 [01:27<00:20, 60.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  82% 5300/6492 [01:27<00:19, 60.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  82% 5320/6492 [01:27<00:19, 60.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  82% 5340/6492 [01:28<00:18, 60.64it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  83% 5360/6492 [01:28<00:18, 60.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  83% 5380/6492 [01:28<00:18, 60.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  83% 5400/6492 [01:28<00:17, 60.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  83% 5420/6492 [01:28<00:17, 61.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  84% 5440/6492 [01:28<00:17, 61.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  84% 5460/6492 [01:29<00:16, 61.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  84% 5480/6492 [01:29<00:16, 61.43it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  85% 5500/6492 [01:29<00:16, 61.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  85% 5520/6492 [01:29<00:15, 61.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  85% 5540/6492 [01:29<00:15, 61.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  86% 5560/6492 [01:29<00:15, 61.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  86% 5580/6492 [01:30<00:14, 61.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  86% 5600/6492 [01:30<00:14, 62.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  87% 5620/6492 [01:30<00:14, 62.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  87% 5640/6492 [01:30<00:13, 62.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  87% 5660/6492 [01:30<00:13, 62.44it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  87% 5680/6492 [01:30<00:12, 62.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  88% 5700/6492 [01:30<00:12, 62.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  88% 5720/6492 [01:31<00:12, 62.78it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  88% 5740/6492 [01:31<00:11, 62.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  89% 5760/6492 [01:31<00:11, 63.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  89% 5780/6492 [01:31<00:11, 63.13it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  89% 5800/6492 [01:31<00:10, 63.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  90% 5820/6492 [01:31<00:10, 63.35it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  90% 5840/6492 [01:32<00:10, 63.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  90% 5860/6492 [01:32<00:09, 63.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  91% 5880/6492 [01:32<00:09, 63.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  91% 5900/6492 [01:32<00:09, 63.78it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  91% 5920/6492 [01:32<00:08, 63.89it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  91% 5940/6492 [01:32<00:08, 64.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  92% 5960/6492 [01:32<00:08, 64.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  92% 5980/6492 [01:33<00:07, 64.21it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  92% 6000/6492 [01:33<00:07, 64.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  93% 6020/6492 [01:33<00:07, 64.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  93% 6040/6492 [01:33<00:07, 64.52it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  93% 6060/6492 [01:33<00:06, 64.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  94% 6080/6492 [01:33<00:06, 64.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  94% 6100/6492 [01:34<00:06, 64.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  94% 6120/6492 [01:34<00:05, 64.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  95% 6140/6492 [01:34<00:05, 64.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  95% 6160/6492 [01:34<00:05, 65.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  95% 6180/6492 [01:34<00:04, 65.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  96% 6200/6492 [01:35<00:04, 65.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  96% 6220/6492 [01:35<00:04, 65.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  96% 6240/6492 [01:35<00:03, 65.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  96% 6260/6492 [01:35<00:03, 65.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  97% 6280/6492 [01:35<00:03, 65.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  97% 6300/6492 [01:35<00:02, 65.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  97% 6320/6492 [01:35<00:02, 65.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  98% 6340/6492 [01:36<00:02, 65.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  98% 6360/6492 [01:36<00:01, 66.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  98% 6380/6492 [01:36<00:01, 66.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  99% 6400/6492 [01:36<00:01, 66.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  99% 6420/6492 [01:36<00:01, 66.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24:  99% 6440/6492 [01:36<00:00, 66.46it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24: 100% 6460/6492 [01:37<00:00, 66.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24: 100% 6480/6492 [01:37<00:00, 66.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 24: 100% 6492/6492 [01:37<00:00, 66.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.378\n",
            "Epoch 25:  80% 5180/6492 [01:22<00:20, 63.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  80% 5200/6492 [01:26<00:21, 59.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  80% 5220/6492 [01:27<00:21, 59.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  81% 5240/6492 [01:27<00:20, 60.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  81% 5260/6492 [01:27<00:20, 60.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  81% 5280/6492 [01:27<00:20, 60.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  82% 5300/6492 [01:27<00:19, 60.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  82% 5320/6492 [01:27<00:19, 60.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  82% 5340/6492 [01:28<00:19, 60.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  83% 5360/6492 [01:28<00:18, 60.75it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  83% 5380/6492 [01:28<00:18, 60.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  83% 5400/6492 [01:28<00:17, 60.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  83% 5420/6492 [01:28<00:17, 61.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  84% 5440/6492 [01:28<00:17, 61.23it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  84% 5460/6492 [01:28<00:16, 61.35it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  84% 5480/6492 [01:29<00:16, 61.47it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  85% 5500/6492 [01:29<00:16, 61.59it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  85% 5520/6492 [01:29<00:15, 61.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  85% 5540/6492 [01:29<00:15, 61.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  86% 5560/6492 [01:29<00:15, 61.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  86% 5580/6492 [01:29<00:14, 62.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  86% 5600/6492 [01:30<00:14, 62.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  87% 5620/6492 [01:30<00:13, 62.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  87% 5640/6492 [01:30<00:13, 62.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  87% 5660/6492 [01:30<00:13, 62.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  87% 5680/6492 [01:30<00:12, 62.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  88% 5700/6492 [01:30<00:12, 62.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  88% 5720/6492 [01:30<00:12, 62.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  88% 5740/6492 [01:31<00:11, 63.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  89% 5760/6492 [01:31<00:11, 63.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  89% 5780/6492 [01:31<00:11, 63.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  89% 5800/6492 [01:31<00:10, 63.35it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  90% 5820/6492 [01:31<00:10, 63.46it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  90% 5840/6492 [01:31<00:10, 63.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  90% 5860/6492 [01:32<00:09, 63.69it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  91% 5880/6492 [01:32<00:09, 63.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  91% 5900/6492 [01:32<00:09, 63.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  91% 5920/6492 [01:32<00:08, 64.03it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  91% 5940/6492 [01:32<00:08, 64.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  92% 5960/6492 [01:32<00:08, 64.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  92% 5980/6492 [01:32<00:07, 64.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  92% 6000/6492 [01:33<00:07, 64.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  93% 6020/6492 [01:33<00:07, 64.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  93% 6040/6492 [01:33<00:06, 64.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  93% 6060/6492 [01:33<00:06, 64.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  94% 6080/6492 [01:33<00:06, 64.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  94% 6100/6492 [01:33<00:06, 65.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  94% 6120/6492 [01:33<00:05, 65.13it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  95% 6140/6492 [01:34<00:05, 65.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  95% 6160/6492 [01:34<00:05, 65.34it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  95% 6180/6492 [01:34<00:04, 65.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  96% 6200/6492 [01:34<00:04, 65.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  96% 6220/6492 [01:34<00:04, 65.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  96% 6240/6492 [01:34<00:03, 65.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  96% 6260/6492 [01:35<00:03, 65.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  97% 6280/6492 [01:35<00:03, 65.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  97% 6300/6492 [01:35<00:02, 66.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  97% 6320/6492 [01:35<00:02, 66.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  98% 6340/6492 [01:35<00:02, 66.30it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  98% 6360/6492 [01:35<00:01, 66.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  98% 6380/6492 [01:35<00:01, 66.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  99% 6400/6492 [01:36<00:01, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  99% 6420/6492 [01:36<00:01, 66.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25:  99% 6440/6492 [01:36<00:00, 66.82it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25: 100% 6460/6492 [01:36<00:00, 66.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25: 100% 6480/6492 [01:36<00:00, 67.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 25: 100% 6492/6492 [01:36<00:00, 67.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  80% 5180/6492 [01:20<00:20, 64.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  80% 5200/6492 [01:25<00:21, 60.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  80% 5220/6492 [01:26<00:20, 60.68it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  81% 5240/6492 [01:26<00:20, 60.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  81% 5260/6492 [01:26<00:20, 60.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  81% 5280/6492 [01:26<00:19, 61.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  82% 5300/6492 [01:26<00:19, 61.19it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  82% 5320/6492 [01:26<00:19, 61.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  82% 5340/6492 [01:26<00:18, 61.44it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  83% 5360/6492 [01:27<00:18, 61.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  83% 5380/6492 [01:27<00:18, 61.70it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  83% 5400/6492 [01:27<00:17, 61.82it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  83% 5420/6492 [01:27<00:17, 61.95it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  84% 5440/6492 [01:27<00:16, 62.07it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  84% 5460/6492 [01:27<00:16, 62.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  84% 5480/6492 [01:27<00:16, 62.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  85% 5500/6492 [01:28<00:15, 62.44it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  85% 5520/6492 [01:28<00:15, 62.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  85% 5540/6492 [01:28<00:15, 62.69it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  86% 5560/6492 [01:28<00:14, 62.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  86% 5580/6492 [01:28<00:14, 62.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  86% 5600/6492 [01:28<00:14, 63.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  87% 5620/6492 [01:28<00:13, 63.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  87% 5640/6492 [01:29<00:13, 63.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  87% 5660/6492 [01:29<00:13, 63.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  87% 5680/6492 [01:29<00:12, 63.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  88% 5700/6492 [01:29<00:12, 63.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  88% 5720/6492 [01:29<00:12, 63.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  88% 5740/6492 [01:29<00:11, 63.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  89% 5760/6492 [01:29<00:11, 64.01it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  89% 5780/6492 [01:30<00:11, 64.13it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  89% 5800/6492 [01:30<00:10, 64.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  90% 5820/6492 [01:30<00:10, 64.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  90% 5840/6492 [01:30<00:10, 64.47it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  90% 5860/6492 [01:30<00:09, 64.58it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  91% 5880/6492 [01:30<00:09, 64.70it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  91% 5900/6492 [01:31<00:09, 64.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  91% 5920/6492 [01:31<00:08, 64.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  91% 5940/6492 [01:31<00:08, 65.03it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  92% 5960/6492 [01:31<00:08, 65.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  92% 5980/6492 [01:31<00:07, 65.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  92% 6000/6492 [01:31<00:07, 65.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  93% 6020/6492 [01:31<00:07, 65.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  93% 6040/6492 [01:32<00:06, 65.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  93% 6060/6492 [01:32<00:06, 65.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  94% 6080/6492 [01:32<00:06, 65.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  94% 6100/6492 [01:32<00:05, 65.95it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  94% 6120/6492 [01:32<00:05, 66.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  95% 6140/6492 [01:32<00:05, 66.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  95% 6160/6492 [01:32<00:05, 66.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  95% 6180/6492 [01:33<00:04, 66.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  96% 6200/6492 [01:33<00:04, 66.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  96% 6220/6492 [01:33<00:04, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  96% 6240/6492 [01:33<00:03, 66.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  96% 6260/6492 [01:33<00:03, 66.82it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  97% 6280/6492 [01:33<00:03, 66.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  97% 6300/6492 [01:33<00:02, 67.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  97% 6320/6492 [01:34<00:02, 67.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  98% 6340/6492 [01:34<00:02, 67.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  98% 6360/6492 [01:34<00:01, 67.35it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  98% 6380/6492 [01:34<00:01, 67.46it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  99% 6400/6492 [01:34<00:01, 67.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  99% 6420/6492 [01:34<00:01, 67.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26:  99% 6440/6492 [01:35<00:00, 67.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26: 100% 6460/6492 [01:35<00:00, 67.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26: 100% 6480/6492 [01:35<00:00, 67.97it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 26: 100% 6492/6492 [01:35<00:00, 68.03it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.377\n",
            "Epoch 27:  80% 5180/6492 [01:20<00:20, 64.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  80% 5200/6492 [01:25<00:21, 60.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  80% 5220/6492 [01:25<00:20, 60.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  81% 5240/6492 [01:25<00:20, 61.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  81% 5260/6492 [01:25<00:20, 61.23it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  81% 5280/6492 [01:26<00:19, 61.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  82% 5300/6492 [01:26<00:19, 61.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  82% 5320/6492 [01:26<00:19, 61.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  82% 5340/6492 [01:26<00:18, 61.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  83% 5360/6492 [01:26<00:18, 61.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  83% 5380/6492 [01:26<00:17, 61.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  83% 5400/6492 [01:26<00:17, 62.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  83% 5420/6492 [01:27<00:17, 62.19it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  84% 5440/6492 [01:27<00:16, 62.31it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  84% 5460/6492 [01:27<00:16, 62.43it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  84% 5480/6492 [01:27<00:16, 62.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  85% 5500/6492 [01:27<00:15, 62.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  85% 5520/6492 [01:27<00:15, 62.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  85% 5540/6492 [01:28<00:15, 62.88it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  86% 5560/6492 [01:28<00:14, 63.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  86% 5580/6492 [01:28<00:14, 63.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  86% 5600/6492 [01:28<00:14, 63.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  87% 5620/6492 [01:28<00:13, 63.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  87% 5640/6492 [01:28<00:13, 63.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  87% 5660/6492 [01:29<00:13, 63.59it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  87% 5680/6492 [01:29<00:12, 63.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  88% 5700/6492 [01:29<00:12, 63.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  88% 5720/6492 [01:29<00:12, 63.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  88% 5740/6492 [01:29<00:11, 64.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  89% 5760/6492 [01:29<00:11, 64.16it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  89% 5780/6492 [01:29<00:11, 64.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  89% 5800/6492 [01:30<00:10, 64.37it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  90% 5820/6492 [01:30<00:10, 64.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  90% 5840/6492 [01:30<00:10, 64.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  90% 5860/6492 [01:30<00:09, 64.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  91% 5880/6492 [01:30<00:09, 64.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  91% 5900/6492 [01:30<00:09, 64.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  91% 5920/6492 [01:31<00:08, 65.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  91% 5940/6492 [01:31<00:08, 65.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  92% 5960/6492 [01:31<00:08, 65.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  92% 5980/6492 [01:31<00:07, 65.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  92% 6000/6492 [01:31<00:07, 65.46it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  93% 6020/6492 [01:31<00:07, 65.56it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  93% 6040/6492 [01:31<00:06, 65.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  93% 6060/6492 [01:32<00:06, 65.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  94% 6080/6492 [01:32<00:06, 65.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  94% 6100/6492 [01:32<00:05, 65.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  94% 6120/6492 [01:32<00:05, 66.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  95% 6140/6492 [01:32<00:05, 66.19it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  95% 6160/6492 [01:32<00:05, 66.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  95% 6180/6492 [01:33<00:04, 66.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  96% 6200/6492 [01:33<00:04, 66.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  96% 6220/6492 [01:33<00:04, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  96% 6240/6492 [01:33<00:03, 66.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  96% 6260/6492 [01:33<00:03, 66.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  97% 6280/6492 [01:33<00:03, 66.91it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  97% 6300/6492 [01:34<00:02, 67.01it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  97% 6320/6492 [01:34<00:02, 67.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  98% 6340/6492 [01:34<00:02, 67.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  98% 6360/6492 [01:34<00:01, 67.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  98% 6380/6492 [01:34<00:01, 67.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  99% 6400/6492 [01:34<00:01, 67.52it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  99% 6420/6492 [01:34<00:01, 67.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27:  99% 6440/6492 [01:35<00:00, 67.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27: 100% 6460/6492 [01:35<00:00, 67.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27: 100% 6480/6492 [01:35<00:00, 67.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 27: 100% 6492/6492 [01:35<00:00, 68.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  80% 5180/6492 [01:20<00:20, 64.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  80% 5200/6492 [01:25<00:21, 60.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  80% 5220/6492 [01:25<00:20, 60.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  81% 5240/6492 [01:25<00:20, 60.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  81% 5260/6492 [01:26<00:20, 61.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  81% 5280/6492 [01:26<00:19, 61.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  82% 5300/6492 [01:26<00:19, 61.34it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  82% 5320/6492 [01:26<00:19, 61.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  82% 5340/6492 [01:26<00:18, 61.58it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  83% 5360/6492 [01:26<00:18, 61.70it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  83% 5380/6492 [01:27<00:17, 61.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  83% 5400/6492 [01:27<00:17, 61.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  83% 5420/6492 [01:27<00:17, 62.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  84% 5440/6492 [01:27<00:16, 62.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  84% 5460/6492 [01:27<00:16, 62.30it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  84% 5480/6492 [01:27<00:16, 62.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  85% 5500/6492 [01:27<00:15, 62.54it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  85% 5520/6492 [01:28<00:15, 62.66it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  85% 5540/6492 [01:28<00:15, 62.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  86% 5560/6492 [01:28<00:14, 62.89it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  86% 5580/6492 [01:28<00:14, 63.01it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  86% 5600/6492 [01:28<00:14, 63.13it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  87% 5620/6492 [01:28<00:13, 63.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  87% 5640/6492 [01:28<00:13, 63.37it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  87% 5660/6492 [01:29<00:13, 63.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  87% 5680/6492 [01:29<00:12, 63.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  88% 5700/6492 [01:29<00:12, 63.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  88% 5720/6492 [01:29<00:12, 63.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  88% 5740/6492 [01:29<00:11, 63.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  89% 5760/6492 [01:29<00:11, 64.07it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  89% 5780/6492 [01:30<00:11, 64.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  89% 5800/6492 [01:30<00:10, 64.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  90% 5820/6492 [01:30<00:10, 64.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  90% 5840/6492 [01:30<00:10, 64.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  90% 5860/6492 [01:30<00:09, 64.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  91% 5880/6492 [01:30<00:09, 64.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  91% 5900/6492 [01:30<00:09, 64.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  91% 5920/6492 [01:31<00:08, 64.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  91% 5940/6492 [01:31<00:08, 65.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  92% 5960/6492 [01:31<00:08, 65.18it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  92% 5980/6492 [01:31<00:07, 65.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  92% 6000/6492 [01:31<00:07, 65.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  93% 6020/6492 [01:31<00:07, 65.52it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  93% 6040/6492 [01:32<00:06, 65.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  93% 6060/6492 [01:32<00:06, 65.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  94% 6080/6492 [01:32<00:06, 65.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  94% 6100/6492 [01:32<00:05, 65.95it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  94% 6120/6492 [01:32<00:05, 66.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  95% 6140/6492 [01:32<00:05, 66.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  95% 6160/6492 [01:32<00:05, 66.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  95% 6180/6492 [01:33<00:04, 66.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  96% 6200/6492 [01:33<00:04, 66.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  96% 6220/6492 [01:33<00:04, 66.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  96% 6240/6492 [01:33<00:03, 66.71it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  96% 6260/6492 [01:33<00:03, 66.81it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  97% 6280/6492 [01:33<00:03, 66.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  97% 6300/6492 [01:34<00:02, 67.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  97% 6320/6492 [01:34<00:02, 67.12it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  98% 6340/6492 [01:34<00:02, 67.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  98% 6360/6492 [01:34<00:01, 67.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  98% 6380/6492 [01:34<00:01, 67.43it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  99% 6400/6492 [01:34<00:01, 67.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  99% 6420/6492 [01:34<00:01, 67.63it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28:  99% 6440/6492 [01:35<00:00, 67.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28: 100% 6460/6492 [01:35<00:00, 67.82it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28: 100% 6480/6492 [01:35<00:00, 67.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 28: 100% 6492/6492 [01:35<00:00, 67.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  80% 5180/6492 [01:20<00:20, 64.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  80% 5200/6492 [01:24<00:21, 61.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  80% 5220/6492 [01:25<00:20, 61.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  81% 5240/6492 [01:25<00:20, 61.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  81% 5260/6492 [01:25<00:19, 61.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  81% 5280/6492 [01:25<00:19, 61.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  82% 5300/6492 [01:25<00:19, 61.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  82% 5320/6492 [01:25<00:18, 61.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  82% 5340/6492 [01:25<00:18, 62.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  83% 5360/6492 [01:26<00:18, 62.24it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  83% 5380/6492 [01:26<00:17, 62.36it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  83% 5400/6492 [01:26<00:17, 62.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  83% 5420/6492 [01:26<00:17, 62.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  84% 5440/6492 [01:26<00:16, 62.73it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  84% 5460/6492 [01:26<00:16, 62.84it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  84% 5480/6492 [01:27<00:16, 62.96it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  85% 5500/6492 [01:27<00:15, 63.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  85% 5520/6492 [01:27<00:15, 63.19it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  85% 5540/6492 [01:27<00:15, 63.31it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  86% 5560/6492 [01:27<00:14, 63.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  86% 5580/6492 [01:27<00:14, 63.54it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  86% 5600/6492 [01:27<00:14, 63.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  87% 5620/6492 [01:28<00:13, 63.79it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  87% 5640/6492 [01:28<00:13, 63.91it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  87% 5660/6492 [01:28<00:12, 64.03it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  87% 5680/6492 [01:28<00:12, 64.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  88% 5700/6492 [01:28<00:12, 64.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  88% 5720/6492 [01:28<00:11, 64.37it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  88% 5740/6492 [01:29<00:11, 64.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  89% 5760/6492 [01:29<00:11, 64.60it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  89% 5780/6492 [01:29<00:11, 64.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  89% 5800/6492 [01:29<00:10, 64.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  90% 5820/6492 [01:29<00:10, 64.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  90% 5840/6492 [01:29<00:10, 65.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  90% 5860/6492 [01:29<00:09, 65.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  91% 5880/6492 [01:30<00:09, 65.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  91% 5900/6492 [01:30<00:09, 65.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  91% 5920/6492 [01:30<00:08, 65.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  91% 5940/6492 [01:30<00:08, 65.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  92% 5960/6492 [01:30<00:08, 65.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  92% 5980/6492 [01:30<00:07, 65.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  92% 6000/6492 [01:30<00:07, 65.94it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  93% 6020/6492 [01:31<00:07, 66.05it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  93% 6040/6492 [01:31<00:06, 66.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  93% 6060/6492 [01:31<00:06, 66.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  94% 6080/6492 [01:31<00:06, 66.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  94% 6100/6492 [01:31<00:05, 66.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  94% 6120/6492 [01:31<00:05, 66.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  95% 6140/6492 [01:32<00:05, 66.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  95% 6160/6492 [01:32<00:04, 66.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  95% 6180/6492 [01:32<00:04, 66.93it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  96% 6200/6492 [01:32<00:04, 67.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  96% 6220/6492 [01:32<00:04, 67.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  96% 6240/6492 [01:32<00:03, 67.25it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  96% 6260/6492 [01:32<00:03, 67.35it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  97% 6280/6492 [01:33<00:03, 67.46it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  97% 6300/6492 [01:33<00:02, 67.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  97% 6320/6492 [01:33<00:02, 67.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  98% 6340/6492 [01:33<00:02, 67.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  98% 6360/6492 [01:33<00:01, 67.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  98% 6380/6492 [01:33<00:01, 67.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  99% 6400/6492 [01:34<00:01, 68.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  99% 6420/6492 [01:34<00:01, 68.18it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29:  99% 6440/6492 [01:34<00:00, 68.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29: 100% 6460/6492 [01:34<00:00, 68.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29: 100% 6480/6492 [01:34<00:00, 68.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 29: 100% 6492/6492 [01:34<00:00, 68.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.376\n",
            "Epoch 30:  80% 5180/6492 [01:21<00:20, 63.91it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  80% 5200/6492 [01:25<00:21, 60.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  80% 5220/6492 [01:26<00:20, 60.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  81% 5240/6492 [01:26<00:20, 60.79it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  81% 5260/6492 [01:26<00:20, 60.91it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  81% 5280/6492 [01:26<00:19, 61.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  82% 5300/6492 [01:26<00:19, 61.16it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  82% 5320/6492 [01:26<00:19, 61.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  82% 5340/6492 [01:26<00:18, 61.41it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  83% 5360/6492 [01:27<00:18, 61.53it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  83% 5380/6492 [01:27<00:18, 61.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  83% 5400/6492 [01:27<00:17, 61.77it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  83% 5420/6492 [01:27<00:17, 61.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  84% 5440/6492 [01:27<00:16, 62.02it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  84% 5460/6492 [01:27<00:16, 62.14it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  84% 5480/6492 [01:28<00:16, 62.26it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  85% 5500/6492 [01:28<00:15, 62.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  85% 5520/6492 [01:28<00:15, 62.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  85% 5540/6492 [01:28<00:15, 62.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  86% 5560/6492 [01:28<00:14, 62.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  86% 5580/6492 [01:28<00:14, 62.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  86% 5600/6492 [01:28<00:14, 62.98it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  87% 5620/6492 [01:29<00:13, 63.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  87% 5640/6492 [01:29<00:13, 63.21it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  87% 5660/6492 [01:29<00:13, 63.33it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  87% 5680/6492 [01:29<00:12, 63.45it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  88% 5700/6492 [01:29<00:12, 63.57it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  88% 5720/6492 [01:29<00:12, 63.68it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  88% 5740/6492 [01:29<00:11, 63.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  89% 5760/6492 [01:30<00:11, 63.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  89% 5780/6492 [01:30<00:11, 64.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  89% 5800/6492 [01:30<00:10, 64.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  90% 5820/6492 [01:30<00:10, 64.27it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  90% 5840/6492 [01:30<00:10, 64.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  90% 5860/6492 [01:30<00:09, 64.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  91% 5880/6492 [01:31<00:09, 64.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  91% 5900/6492 [01:31<00:09, 64.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  91% 5920/6492 [01:31<00:08, 64.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  91% 5940/6492 [01:31<00:08, 64.95it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  92% 5960/6492 [01:31<00:08, 65.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  92% 5980/6492 [01:31<00:07, 65.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  92% 6000/6492 [01:31<00:07, 65.28it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  93% 6020/6492 [01:32<00:07, 65.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  93% 6040/6492 [01:32<00:06, 65.50it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  93% 6060/6492 [01:32<00:06, 65.61it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  94% 6080/6492 [01:32<00:06, 65.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  94% 6100/6492 [01:32<00:05, 65.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  94% 6120/6492 [01:32<00:05, 65.95it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  95% 6140/6492 [01:32<00:05, 66.06it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  95% 6160/6492 [01:33<00:05, 66.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  95% 6180/6492 [01:33<00:04, 66.27it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  96% 6200/6492 [01:33<00:04, 66.37it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  96% 6220/6492 [01:33<00:04, 66.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  96% 6240/6492 [01:33<00:03, 66.59it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  96% 6260/6492 [01:33<00:03, 66.69it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  97% 6280/6492 [01:34<00:03, 66.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  97% 6300/6492 [01:34<00:02, 66.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  97% 6320/6492 [01:34<00:02, 67.01it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  98% 6340/6492 [01:34<00:02, 67.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  98% 6360/6492 [01:34<00:01, 67.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  98% 6380/6492 [01:34<00:01, 67.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  99% 6400/6492 [01:34<00:01, 67.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  99% 6420/6492 [01:35<00:01, 67.52it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30:  99% 6440/6492 [01:35<00:00, 67.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30: 100% 6460/6492 [01:35<00:00, 67.72it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30: 100% 6480/6492 [01:35<00:00, 67.83it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 30: 100% 6492/6492 [01:35<00:00, 67.89it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  80% 5180/6492 [01:20<00:20, 64.38it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  80% 5200/6492 [01:25<00:21, 61.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  80% 5220/6492 [01:25<00:20, 61.17it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  81% 5240/6492 [01:25<00:20, 61.29it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  81% 5260/6492 [01:25<00:20, 61.42it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  81% 5280/6492 [01:25<00:19, 61.54it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  82% 5300/6492 [01:25<00:19, 61.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  82% 5320/6492 [01:26<00:18, 61.79it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  82% 5340/6492 [01:26<00:18, 61.92it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  83% 5360/6492 [01:26<00:18, 62.04it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  83% 5380/6492 [01:26<00:17, 62.15it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  83% 5400/6492 [01:26<00:17, 62.27it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  83% 5420/6492 [01:26<00:17, 62.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  84% 5440/6492 [01:27<00:16, 62.51it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  84% 5460/6492 [01:27<00:16, 62.62it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  84% 5480/6492 [01:27<00:16, 62.74it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  85% 5500/6492 [01:27<00:15, 62.85it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  85% 5520/6492 [01:27<00:15, 62.97it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  85% 5540/6492 [01:27<00:15, 63.09it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  86% 5560/6492 [01:27<00:14, 63.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  86% 5580/6492 [01:28<00:14, 63.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  86% 5600/6492 [01:28<00:14, 63.43it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  87% 5620/6492 [01:28<00:13, 63.54it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  87% 5640/6492 [01:28<00:13, 63.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  87% 5660/6492 [01:28<00:13, 63.76it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  87% 5680/6492 [01:28<00:12, 63.87it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  88% 5700/6492 [01:29<00:12, 63.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  88% 5720/6492 [01:29<00:12, 64.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  88% 5740/6492 [01:29<00:11, 64.22it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  89% 5760/6492 [01:29<00:11, 64.33it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  89% 5780/6492 [01:29<00:11, 64.44it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  89% 5800/6492 [01:29<00:10, 64.55it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  90% 5820/6492 [01:30<00:10, 64.67it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  90% 5840/6492 [01:30<00:10, 64.78it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  90% 5860/6492 [01:30<00:09, 64.89it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  91% 5880/6492 [01:30<00:09, 65.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  91% 5900/6492 [01:30<00:09, 65.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  91% 5920/6492 [01:30<00:08, 65.21it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  91% 5940/6492 [01:30<00:08, 65.32it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  92% 5960/6492 [01:31<00:08, 65.43it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  92% 5980/6492 [01:31<00:07, 65.54it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  92% 6000/6492 [01:31<00:07, 65.65it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  93% 6020/6492 [01:31<00:07, 65.75it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  93% 6040/6492 [01:31<00:06, 65.86it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  93% 6060/6492 [01:31<00:06, 65.97it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  94% 6080/6492 [01:32<00:06, 66.08it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  94% 6100/6492 [01:32<00:05, 66.19it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  94% 6120/6492 [01:32<00:05, 66.30it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  95% 6140/6492 [01:32<00:05, 66.40it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  95% 6160/6492 [01:32<00:04, 66.48it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  95% 6180/6492 [01:32<00:04, 66.58it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  96% 6200/6492 [01:32<00:04, 66.68it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  96% 6220/6492 [01:33<00:04, 66.78it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  96% 6240/6492 [01:33<00:03, 66.89it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  96% 6260/6492 [01:33<00:03, 66.99it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  97% 6280/6492 [01:33<00:03, 67.10it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  97% 6300/6492 [01:33<00:02, 67.20it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  97% 6320/6492 [01:33<00:02, 67.30it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  98% 6340/6492 [01:34<00:02, 67.39it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  98% 6360/6492 [01:34<00:01, 67.49it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  98% 6380/6492 [01:34<00:01, 67.59it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  99% 6400/6492 [01:34<00:01, 67.69it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  99% 6420/6492 [01:34<00:01, 67.80it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31:  99% 6440/6492 [01:34<00:00, 67.90it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31: 100% 6460/6492 [01:35<00:00, 68.00it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31: 100% 6480/6492 [01:35<00:00, 68.11it/s, loss=4.37, v_num=35, val_loss=4.380, avg_val_loss=4.380, train_loss=4.370]\n",
            "Epoch 31: 100% 6492/6492 [01:35<00:00, 68.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.375\n",
            "Epoch 32:  80% 5180/6492 [01:20<00:20, 64.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  80% 5200/6492 [01:25<00:21, 61.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  80% 5220/6492 [01:25<00:20, 61.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  81% 5240/6492 [01:25<00:20, 61.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  81% 5260/6492 [01:25<00:20, 61.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  81% 5280/6492 [01:25<00:19, 61.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  82% 5300/6492 [01:25<00:19, 61.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  82% 5320/6492 [01:26<00:18, 61.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  82% 5340/6492 [01:26<00:18, 61.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  83% 5360/6492 [01:26<00:18, 62.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  83% 5380/6492 [01:26<00:17, 62.13it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  83% 5400/6492 [01:26<00:17, 62.24it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  83% 5420/6492 [01:26<00:17, 62.36it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  84% 5440/6492 [01:27<00:16, 62.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  84% 5460/6492 [01:27<00:16, 62.59it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  84% 5480/6492 [01:27<00:16, 62.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  85% 5500/6492 [01:27<00:15, 62.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  85% 5520/6492 [01:27<00:15, 62.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  85% 5540/6492 [01:27<00:15, 63.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  86% 5560/6492 [01:28<00:14, 63.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  86% 5580/6492 [01:28<00:14, 63.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  86% 5600/6492 [01:28<00:14, 63.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  87% 5620/6492 [01:28<00:13, 63.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  87% 5640/6492 [01:28<00:13, 63.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  87% 5660/6492 [01:28<00:13, 63.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  87% 5680/6492 [01:28<00:12, 63.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  88% 5700/6492 [01:29<00:12, 63.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  88% 5720/6492 [01:29<00:12, 64.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  88% 5740/6492 [01:29<00:11, 64.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  89% 5760/6492 [01:29<00:11, 64.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  89% 5780/6492 [01:29<00:11, 64.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  89% 5800/6492 [01:29<00:10, 64.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  90% 5820/6492 [01:30<00:10, 64.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  90% 5840/6492 [01:30<00:10, 64.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  90% 5860/6492 [01:30<00:09, 64.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  91% 5880/6492 [01:30<00:09, 64.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  91% 5900/6492 [01:30<00:09, 65.06it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  91% 5920/6492 [01:30<00:08, 65.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  91% 5940/6492 [01:30<00:08, 65.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  92% 5960/6492 [01:31<00:08, 65.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  92% 5980/6492 [01:31<00:07, 65.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  92% 6000/6492 [01:31<00:07, 65.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  93% 6020/6492 [01:31<00:07, 65.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  93% 6040/6492 [01:31<00:06, 65.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  93% 6060/6492 [01:31<00:06, 65.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  94% 6080/6492 [01:32<00:06, 66.06it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  94% 6100/6492 [01:32<00:05, 66.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  94% 6120/6492 [01:32<00:05, 66.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  95% 6140/6492 [01:32<00:05, 66.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  95% 6160/6492 [01:32<00:04, 66.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  95% 6180/6492 [01:32<00:04, 66.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  96% 6200/6492 [01:32<00:04, 66.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  96% 6220/6492 [01:33<00:04, 66.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  96% 6240/6492 [01:33<00:03, 66.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  96% 6260/6492 [01:33<00:03, 67.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  97% 6280/6492 [01:33<00:03, 67.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  97% 6300/6492 [01:33<00:02, 67.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  97% 6320/6492 [01:33<00:02, 67.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  98% 6340/6492 [01:33<00:02, 67.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  98% 6360/6492 [01:34<00:01, 67.59it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  98% 6380/6492 [01:34<00:01, 67.69it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  99% 6400/6492 [01:34<00:01, 67.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  99% 6420/6492 [01:34<00:01, 67.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32:  99% 6440/6492 [01:34<00:00, 68.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32: 100% 6460/6492 [01:34<00:00, 68.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32: 100% 6480/6492 [01:34<00:00, 68.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 32: 100% 6492/6492 [01:35<00:00, 68.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  80% 5180/6492 [01:20<00:20, 64.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  80% 5200/6492 [01:25<00:21, 60.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  80% 5220/6492 [01:25<00:20, 60.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  81% 5240/6492 [01:25<00:20, 61.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  81% 5260/6492 [01:25<00:20, 61.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  81% 5280/6492 [01:26<00:19, 61.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  82% 5300/6492 [01:26<00:19, 61.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  82% 5320/6492 [01:26<00:19, 61.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  82% 5340/6492 [01:26<00:18, 61.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  83% 5360/6492 [01:26<00:18, 61.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  83% 5380/6492 [01:26<00:17, 61.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  83% 5400/6492 [01:27<00:17, 62.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  83% 5420/6492 [01:27<00:17, 62.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  84% 5440/6492 [01:27<00:16, 62.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  84% 5460/6492 [01:27<00:16, 62.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  84% 5480/6492 [01:27<00:16, 62.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  85% 5500/6492 [01:27<00:15, 62.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  85% 5520/6492 [01:27<00:15, 62.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  85% 5540/6492 [01:28<00:15, 62.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  86% 5560/6492 [01:28<00:14, 62.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  86% 5580/6492 [01:28<00:14, 63.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  86% 5600/6492 [01:28<00:14, 63.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  87% 5620/6492 [01:28<00:13, 63.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  87% 5640/6492 [01:28<00:13, 63.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  87% 5660/6492 [01:29<00:13, 63.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  87% 5680/6492 [01:29<00:12, 63.70it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  88% 5700/6492 [01:29<00:12, 63.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  88% 5720/6492 [01:29<00:12, 63.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  88% 5740/6492 [01:29<00:11, 64.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  89% 5760/6492 [01:29<00:11, 64.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  89% 5780/6492 [01:29<00:11, 64.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  89% 5800/6492 [01:30<00:10, 64.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  90% 5820/6492 [01:30<00:10, 64.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  90% 5840/6492 [01:30<00:10, 64.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  90% 5860/6492 [01:30<00:09, 64.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  91% 5880/6492 [01:30<00:09, 64.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  91% 5900/6492 [01:30<00:09, 64.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  91% 5920/6492 [01:31<00:08, 65.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  91% 5940/6492 [01:31<00:08, 65.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  92% 5960/6492 [01:31<00:08, 65.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  92% 5980/6492 [01:31<00:07, 65.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  92% 6000/6492 [01:31<00:07, 65.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  93% 6020/6492 [01:31<00:07, 65.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  93% 6040/6492 [01:32<00:06, 65.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  93% 6060/6492 [01:32<00:06, 65.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  94% 6080/6492 [01:32<00:06, 65.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  94% 6100/6492 [01:32<00:05, 65.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  94% 6120/6492 [01:32<00:05, 66.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  95% 6140/6492 [01:32<00:05, 66.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  95% 6160/6492 [01:32<00:05, 66.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  95% 6180/6492 [01:33<00:04, 66.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  96% 6200/6492 [01:33<00:04, 66.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  96% 6220/6492 [01:33<00:04, 66.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  96% 6240/6492 [01:33<00:03, 66.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  96% 6260/6492 [01:33<00:03, 66.81it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  97% 6280/6492 [01:33<00:03, 66.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  97% 6300/6492 [01:34<00:02, 67.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  97% 6320/6492 [01:34<00:02, 67.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  98% 6340/6492 [01:34<00:02, 67.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  98% 6360/6492 [01:34<00:01, 67.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  98% 6380/6492 [01:34<00:01, 67.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  99% 6400/6492 [01:34<00:01, 67.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  99% 6420/6492 [01:34<00:01, 67.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33:  99% 6440/6492 [01:35<00:00, 67.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33: 100% 6460/6492 [01:35<00:00, 67.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33: 100% 6480/6492 [01:35<00:00, 67.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 33: 100% 6492/6492 [01:35<00:00, 68.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  80% 5180/6492 [01:20<00:20, 64.24it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  80% 5200/6492 [01:25<00:21, 60.81it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  80% 5220/6492 [01:25<00:20, 60.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  81% 5240/6492 [01:25<00:20, 61.06it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  81% 5260/6492 [01:25<00:20, 61.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  81% 5280/6492 [01:26<00:19, 61.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  82% 5300/6492 [01:26<00:19, 61.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  82% 5320/6492 [01:26<00:19, 61.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  82% 5340/6492 [01:26<00:18, 61.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  83% 5360/6492 [01:26<00:18, 61.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  83% 5380/6492 [01:26<00:17, 61.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  83% 5400/6492 [01:27<00:17, 62.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  83% 5420/6492 [01:27<00:17, 62.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  84% 5440/6492 [01:27<00:16, 62.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  84% 5460/6492 [01:27<00:16, 62.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  84% 5480/6492 [01:27<00:16, 62.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  85% 5500/6492 [01:27<00:15, 62.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  85% 5520/6492 [01:27<00:15, 62.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  85% 5540/6492 [01:28<00:15, 62.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  86% 5560/6492 [01:28<00:14, 63.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  86% 5580/6492 [01:28<00:14, 63.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  86% 5600/6492 [01:28<00:14, 63.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  87% 5620/6492 [01:28<00:13, 63.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  87% 5640/6492 [01:28<00:13, 63.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  87% 5660/6492 [01:29<00:13, 63.59it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  87% 5680/6492 [01:29<00:12, 63.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  88% 5700/6492 [01:29<00:12, 63.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  88% 5720/6492 [01:29<00:12, 63.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  88% 5740/6492 [01:29<00:11, 64.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  89% 5760/6492 [01:29<00:11, 64.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  89% 5780/6492 [01:29<00:11, 64.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  89% 5800/6492 [01:30<00:10, 64.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  90% 5820/6492 [01:30<00:10, 64.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  90% 5840/6492 [01:30<00:10, 64.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  90% 5860/6492 [01:30<00:09, 64.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  91% 5880/6492 [01:30<00:09, 64.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  91% 5900/6492 [01:30<00:09, 64.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  91% 5920/6492 [01:30<00:08, 65.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  91% 5940/6492 [01:31<00:08, 65.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  92% 5960/6492 [01:31<00:08, 65.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  92% 5980/6492 [01:31<00:07, 65.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  92% 6000/6492 [01:31<00:07, 65.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  93% 6020/6492 [01:31<00:07, 65.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  93% 6040/6492 [01:31<00:06, 65.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  93% 6060/6492 [01:32<00:06, 65.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  94% 6080/6492 [01:32<00:06, 65.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  94% 6100/6492 [01:32<00:05, 66.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  94% 6120/6492 [01:32<00:05, 66.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  95% 6140/6492 [01:32<00:05, 66.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  95% 6160/6492 [01:32<00:04, 66.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  95% 6180/6492 [01:32<00:04, 66.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  96% 6200/6492 [01:33<00:04, 66.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  96% 6220/6492 [01:33<00:04, 66.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  96% 6240/6492 [01:33<00:03, 66.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  96% 6260/6492 [01:33<00:03, 66.93it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  97% 6280/6492 [01:33<00:03, 67.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  97% 6300/6492 [01:33<00:02, 67.13it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  97% 6320/6492 [01:34<00:02, 67.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  98% 6340/6492 [01:34<00:02, 67.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  98% 6360/6492 [01:34<00:01, 67.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  98% 6380/6492 [01:34<00:01, 67.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  99% 6400/6492 [01:34<00:01, 67.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  99% 6420/6492 [01:34<00:01, 67.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34:  99% 6440/6492 [01:34<00:00, 67.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34: 100% 6460/6492 [01:35<00:00, 67.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34: 100% 6480/6492 [01:35<00:00, 68.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 34: 100% 6492/6492 [01:35<00:00, 68.13it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.374\n",
            "Epoch 35:  80% 5180/6492 [01:20<00:20, 64.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  80% 5200/6492 [01:25<00:21, 61.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  80% 5220/6492 [01:25<00:20, 61.25it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  81% 5240/6492 [01:25<00:20, 61.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  81% 5260/6492 [01:25<00:20, 61.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  81% 5280/6492 [01:25<00:19, 61.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  82% 5300/6492 [01:25<00:19, 61.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  82% 5320/6492 [01:26<00:18, 61.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  82% 5340/6492 [01:26<00:18, 61.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  83% 5360/6492 [01:26<00:18, 62.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  83% 5380/6492 [01:26<00:17, 62.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  83% 5400/6492 [01:26<00:17, 62.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  83% 5420/6492 [01:26<00:17, 62.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  84% 5440/6492 [01:26<00:16, 62.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  84% 5460/6492 [01:27<00:16, 62.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  84% 5480/6492 [01:27<00:16, 62.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  85% 5500/6492 [01:27<00:15, 62.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  85% 5520/6492 [01:27<00:15, 63.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  85% 5540/6492 [01:27<00:15, 63.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  86% 5560/6492 [01:27<00:14, 63.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  86% 5580/6492 [01:28<00:14, 63.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  86% 5600/6492 [01:28<00:14, 63.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  87% 5620/6492 [01:28<00:13, 63.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  87% 5640/6492 [01:28<00:13, 63.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  87% 5660/6492 [01:28<00:13, 63.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  87% 5680/6492 [01:28<00:12, 63.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  88% 5700/6492 [01:28<00:12, 64.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  88% 5720/6492 [01:29<00:12, 64.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  88% 5740/6492 [01:29<00:11, 64.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  89% 5760/6492 [01:29<00:11, 64.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  89% 5780/6492 [01:29<00:11, 64.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  89% 5800/6492 [01:29<00:10, 64.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  90% 5820/6492 [01:29<00:10, 64.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  90% 5840/6492 [01:30<00:10, 64.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  90% 5860/6492 [01:30<00:09, 65.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  91% 5880/6492 [01:30<00:09, 65.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  91% 5900/6492 [01:30<00:09, 65.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  91% 5920/6492 [01:30<00:08, 65.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  91% 5940/6492 [01:30<00:08, 65.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  92% 5960/6492 [01:30<00:08, 65.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  92% 5980/6492 [01:31<00:07, 65.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  92% 6000/6492 [01:31<00:07, 65.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  93% 6020/6492 [01:31<00:07, 65.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  93% 6040/6492 [01:31<00:06, 66.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  93% 6060/6492 [01:31<00:06, 66.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  94% 6080/6492 [01:31<00:06, 66.24it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  94% 6100/6492 [01:31<00:05, 66.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  94% 6120/6492 [01:32<00:05, 66.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  95% 6140/6492 [01:32<00:05, 66.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  95% 6160/6492 [01:32<00:04, 66.69it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  95% 6180/6492 [01:32<00:04, 66.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  96% 6200/6492 [01:32<00:04, 66.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  96% 6220/6492 [01:32<00:04, 67.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  96% 6240/6492 [01:32<00:03, 67.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  96% 6260/6492 [01:33<00:03, 67.24it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  97% 6280/6492 [01:33<00:03, 67.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  97% 6300/6492 [01:33<00:02, 67.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  97% 6320/6492 [01:33<00:02, 67.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  98% 6340/6492 [01:33<00:02, 67.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  98% 6360/6492 [01:33<00:01, 67.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  98% 6380/6492 [01:34<00:01, 67.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  99% 6400/6492 [01:34<00:01, 67.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  99% 6420/6492 [01:34<00:01, 68.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35:  99% 6440/6492 [01:34<00:00, 68.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35: 100% 6460/6492 [01:34<00:00, 68.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35: 100% 6480/6492 [01:34<00:00, 68.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 35: 100% 6492/6492 [01:34<00:00, 68.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  80% 5180/6492 [01:20<00:20, 64.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  80% 5200/6492 [01:24<00:21, 61.25it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  80% 5220/6492 [01:25<00:20, 61.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  81% 5240/6492 [01:25<00:20, 61.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  81% 5260/6492 [01:25<00:19, 61.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  81% 5280/6492 [01:25<00:19, 61.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  82% 5300/6492 [01:25<00:19, 61.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  82% 5320/6492 [01:25<00:18, 62.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  82% 5340/6492 [01:25<00:18, 62.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  83% 5360/6492 [01:26<00:18, 62.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  83% 5380/6492 [01:26<00:17, 62.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  83% 5400/6492 [01:26<00:17, 62.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  83% 5420/6492 [01:26<00:17, 62.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  84% 5440/6492 [01:26<00:16, 62.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  84% 5460/6492 [01:26<00:16, 62.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  84% 5480/6492 [01:26<00:16, 62.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  85% 5500/6492 [01:27<00:15, 63.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  85% 5520/6492 [01:27<00:15, 63.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  85% 5540/6492 [01:27<00:15, 63.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  86% 5560/6492 [01:27<00:14, 63.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  86% 5580/6492 [01:27<00:14, 63.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  86% 5600/6492 [01:27<00:14, 63.70it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  87% 5620/6492 [01:28<00:13, 63.81it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  87% 5640/6492 [01:28<00:13, 63.93it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  87% 5660/6492 [01:28<00:12, 64.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  87% 5680/6492 [01:28<00:12, 64.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  88% 5700/6492 [01:28<00:12, 64.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  88% 5720/6492 [01:28<00:11, 64.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  88% 5740/6492 [01:28<00:11, 64.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  89% 5760/6492 [01:29<00:11, 64.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  89% 5780/6492 [01:29<00:10, 64.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  89% 5800/6492 [01:29<00:10, 64.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  90% 5820/6492 [01:29<00:10, 64.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  90% 5840/6492 [01:29<00:10, 65.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  90% 5860/6492 [01:29<00:09, 65.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  91% 5880/6492 [01:30<00:09, 65.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  91% 5900/6492 [01:30<00:09, 65.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  91% 5920/6492 [01:30<00:08, 65.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  91% 5940/6492 [01:30<00:08, 65.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  92% 5960/6492 [01:30<00:08, 65.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  92% 5980/6492 [01:30<00:07, 65.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  92% 6000/6492 [01:30<00:07, 65.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  93% 6020/6492 [01:31<00:07, 66.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  93% 6040/6492 [01:31<00:06, 66.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  93% 6060/6492 [01:31<00:06, 66.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  94% 6080/6492 [01:31<00:06, 66.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  94% 6100/6492 [01:31<00:05, 66.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  94% 6120/6492 [01:31<00:05, 66.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  95% 6140/6492 [01:32<00:05, 66.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  95% 6160/6492 [01:32<00:04, 66.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  95% 6180/6492 [01:32<00:04, 66.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  96% 6200/6492 [01:32<00:04, 66.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  96% 6220/6492 [01:32<00:04, 67.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  96% 6240/6492 [01:32<00:03, 67.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  96% 6260/6492 [01:33<00:03, 67.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  97% 6280/6492 [01:33<00:03, 67.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  97% 6300/6492 [01:33<00:02, 67.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  97% 6320/6492 [01:33<00:02, 67.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  98% 6340/6492 [01:33<00:02, 67.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  98% 6360/6492 [01:33<00:01, 67.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  98% 6380/6492 [01:33<00:01, 67.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  99% 6400/6492 [01:34<00:01, 68.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  99% 6420/6492 [01:34<00:01, 68.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36:  99% 6440/6492 [01:34<00:00, 68.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36: 100% 6460/6492 [01:34<00:00, 68.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36: 100% 6480/6492 [01:34<00:00, 68.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 36: 100% 6492/6492 [01:34<00:00, 68.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  80% 5180/6492 [01:20<00:20, 64.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  80% 5200/6492 [01:25<00:21, 61.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  80% 5220/6492 [01:25<00:20, 61.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  81% 5240/6492 [01:25<00:20, 61.30it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  81% 5260/6492 [01:25<00:20, 61.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  81% 5280/6492 [01:25<00:19, 61.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  82% 5300/6492 [01:25<00:19, 61.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  82% 5320/6492 [01:26<00:18, 61.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  82% 5340/6492 [01:26<00:18, 61.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  83% 5360/6492 [01:26<00:18, 62.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  83% 5380/6492 [01:26<00:17, 62.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  83% 5400/6492 [01:26<00:17, 62.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  83% 5420/6492 [01:26<00:17, 62.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  84% 5440/6492 [01:27<00:16, 62.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  84% 5460/6492 [01:27<00:16, 62.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  84% 5480/6492 [01:27<00:16, 62.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  85% 5500/6492 [01:27<00:15, 62.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  85% 5520/6492 [01:27<00:15, 62.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  85% 5540/6492 [01:27<00:15, 63.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  86% 5560/6492 [01:27<00:14, 63.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  86% 5580/6492 [01:28<00:14, 63.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  86% 5600/6492 [01:28<00:14, 63.45it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  87% 5620/6492 [01:28<00:13, 63.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  87% 5640/6492 [01:28<00:13, 63.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  87% 5660/6492 [01:28<00:13, 63.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  87% 5680/6492 [01:28<00:12, 63.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  88% 5700/6492 [01:29<00:12, 64.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  88% 5720/6492 [01:29<00:12, 64.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  88% 5740/6492 [01:29<00:11, 64.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  89% 5760/6492 [01:29<00:11, 64.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  89% 5780/6492 [01:29<00:11, 64.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  89% 5800/6492 [01:29<00:10, 64.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  90% 5820/6492 [01:29<00:10, 64.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  90% 5840/6492 [01:30<00:10, 64.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  90% 5860/6492 [01:30<00:09, 64.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  91% 5880/6492 [01:30<00:09, 65.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  91% 5900/6492 [01:30<00:09, 65.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  91% 5920/6492 [01:30<00:08, 65.25it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  91% 5940/6492 [01:30<00:08, 65.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  92% 5960/6492 [01:31<00:08, 65.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  92% 5980/6492 [01:31<00:07, 65.59it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  92% 6000/6492 [01:31<00:07, 65.70it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  93% 6020/6492 [01:31<00:07, 65.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  93% 6040/6492 [01:31<00:06, 65.90it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  93% 6060/6492 [01:31<00:06, 66.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  94% 6080/6492 [01:31<00:06, 66.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  94% 6100/6492 [01:32<00:05, 66.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  94% 6120/6492 [01:32<00:05, 66.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  95% 6140/6492 [01:32<00:05, 66.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  95% 6160/6492 [01:32<00:04, 66.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  95% 6180/6492 [01:32<00:04, 66.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  96% 6200/6492 [01:32<00:04, 66.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  96% 6220/6492 [01:33<00:04, 66.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  96% 6240/6492 [01:33<00:03, 66.93it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  96% 6260/6492 [01:33<00:03, 67.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  97% 6280/6492 [01:33<00:03, 67.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  97% 6300/6492 [01:33<00:02, 67.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  97% 6320/6492 [01:33<00:02, 67.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  98% 6340/6492 [01:34<00:02, 67.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  98% 6360/6492 [01:34<00:01, 67.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  98% 6380/6492 [01:34<00:01, 67.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  99% 6400/6492 [01:34<00:01, 67.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  99% 6420/6492 [01:34<00:01, 67.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37:  99% 6440/6492 [01:34<00:00, 67.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37: 100% 6460/6492 [01:34<00:00, 68.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37: 100% 6480/6492 [01:35<00:00, 68.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 37: 100% 6492/6492 [01:35<00:00, 68.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  80% 5180/6492 [01:19<00:20, 64.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  80% 5200/6492 [01:24<00:21, 61.36it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  80% 5220/6492 [01:24<00:20, 61.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  81% 5240/6492 [01:25<00:20, 61.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  81% 5260/6492 [01:25<00:19, 61.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  81% 5280/6492 [01:25<00:19, 61.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  82% 5300/6492 [01:25<00:19, 61.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  82% 5320/6492 [01:25<00:18, 62.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  82% 5340/6492 [01:25<00:18, 62.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  83% 5360/6492 [01:26<00:18, 62.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  83% 5380/6492 [01:26<00:17, 62.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  83% 5400/6492 [01:26<00:17, 62.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  83% 5420/6492 [01:26<00:17, 62.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  84% 5440/6492 [01:26<00:16, 62.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  84% 5460/6492 [01:26<00:16, 62.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  84% 5480/6492 [01:26<00:16, 63.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  85% 5500/6492 [01:27<00:15, 63.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  85% 5520/6492 [01:27<00:15, 63.25it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  85% 5540/6492 [01:27<00:15, 63.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  86% 5560/6492 [01:27<00:14, 63.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  86% 5580/6492 [01:27<00:14, 63.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  86% 5600/6492 [01:27<00:13, 63.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  87% 5620/6492 [01:28<00:13, 63.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  87% 5640/6492 [01:28<00:13, 63.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  87% 5660/6492 [01:28<00:12, 64.06it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  87% 5680/6492 [01:28<00:12, 64.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  88% 5700/6492 [01:28<00:12, 64.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  88% 5720/6492 [01:28<00:11, 64.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  88% 5740/6492 [01:28<00:11, 64.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  89% 5760/6492 [01:29<00:11, 64.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  89% 5780/6492 [01:29<00:10, 64.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  89% 5800/6492 [01:29<00:10, 64.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  90% 5820/6492 [01:29<00:10, 64.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  90% 5840/6492 [01:29<00:10, 65.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  90% 5860/6492 [01:29<00:09, 65.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  91% 5880/6492 [01:30<00:09, 65.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  91% 5900/6492 [01:30<00:09, 65.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  91% 5920/6492 [01:30<00:08, 65.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  91% 5940/6492 [01:30<00:08, 65.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  92% 5960/6492 [01:30<00:08, 65.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  92% 5980/6492 [01:30<00:07, 65.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  92% 6000/6492 [01:30<00:07, 66.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  93% 6020/6492 [01:31<00:07, 66.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  93% 6040/6492 [01:31<00:06, 66.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  93% 6060/6492 [01:31<00:06, 66.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  94% 6080/6492 [01:31<00:06, 66.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  94% 6100/6492 [01:31<00:05, 66.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  94% 6120/6492 [01:31<00:05, 66.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  95% 6140/6492 [01:31<00:05, 66.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  95% 6160/6492 [01:32<00:04, 66.90it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  95% 6180/6492 [01:32<00:04, 67.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  96% 6200/6492 [01:32<00:04, 67.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  96% 6220/6492 [01:32<00:04, 67.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  96% 6240/6492 [01:32<00:03, 67.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  96% 6260/6492 [01:32<00:03, 67.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  97% 6280/6492 [01:32<00:03, 67.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  97% 6300/6492 [01:33<00:02, 67.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  97% 6320/6492 [01:33<00:02, 67.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  98% 6340/6492 [01:33<00:02, 67.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  98% 6360/6492 [01:33<00:01, 67.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  98% 6380/6492 [01:33<00:01, 68.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  99% 6400/6492 [01:33<00:01, 68.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  99% 6420/6492 [01:34<00:01, 68.28it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38:  99% 6440/6492 [01:34<00:00, 68.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38: 100% 6460/6492 [01:34<00:00, 68.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38: 100% 6480/6492 [01:34<00:00, 68.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 38: 100% 6492/6492 [01:34<00:00, 68.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  80% 5180/6492 [01:21<00:20, 63.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  80% 5200/6492 [01:26<00:21, 60.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  80% 5220/6492 [01:26<00:21, 60.24it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  81% 5240/6492 [01:26<00:20, 60.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  81% 5260/6492 [01:26<00:20, 60.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  81% 5280/6492 [01:27<00:19, 60.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  82% 5300/6492 [01:27<00:19, 60.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  82% 5320/6492 [01:27<00:19, 60.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  82% 5340/6492 [01:27<00:18, 60.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  83% 5360/6492 [01:27<00:18, 61.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  83% 5380/6492 [01:27<00:18, 61.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  83% 5400/6492 [01:28<00:17, 61.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  83% 5420/6492 [01:28<00:17, 61.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  84% 5440/6492 [01:28<00:17, 61.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  84% 5460/6492 [01:28<00:16, 61.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  84% 5480/6492 [01:28<00:16, 61.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  85% 5500/6492 [01:28<00:16, 61.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  85% 5520/6492 [01:28<00:15, 62.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  85% 5540/6492 [01:29<00:15, 62.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  86% 5560/6492 [01:29<00:14, 62.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  86% 5580/6492 [01:29<00:14, 62.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  86% 5600/6492 [01:29<00:14, 62.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  87% 5620/6492 [01:29<00:13, 62.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  87% 5640/6492 [01:29<00:13, 62.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  87% 5660/6492 [01:29<00:13, 62.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  87% 5680/6492 [01:30<00:12, 63.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  88% 5700/6492 [01:30<00:12, 63.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  88% 5720/6492 [01:30<00:12, 63.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  88% 5740/6492 [01:30<00:11, 63.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  89% 5760/6492 [01:30<00:11, 63.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  89% 5780/6492 [01:30<00:11, 63.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  89% 5800/6492 [01:31<00:10, 63.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  90% 5820/6492 [01:31<00:10, 63.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  90% 5840/6492 [01:31<00:10, 63.93it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  90% 5860/6492 [01:31<00:09, 64.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  91% 5880/6492 [01:31<00:09, 64.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  91% 5900/6492 [01:31<00:09, 64.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  91% 5920/6492 [01:31<00:08, 64.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  91% 5940/6492 [01:32<00:08, 64.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  92% 5960/6492 [01:32<00:08, 64.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  92% 5980/6492 [01:32<00:07, 64.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  92% 6000/6492 [01:32<00:07, 64.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  93% 6020/6492 [01:32<00:07, 64.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  93% 6040/6492 [01:32<00:06, 64.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  93% 6060/6492 [01:33<00:06, 65.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  94% 6080/6492 [01:33<00:06, 65.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  94% 6100/6492 [01:33<00:06, 65.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  94% 6120/6492 [01:33<00:05, 65.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  95% 6140/6492 [01:33<00:05, 65.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  95% 6160/6492 [01:33<00:05, 65.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  95% 6180/6492 [01:33<00:04, 65.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  96% 6200/6492 [01:34<00:04, 65.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  96% 6220/6492 [01:34<00:04, 65.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  96% 6240/6492 [01:34<00:03, 66.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  96% 6260/6492 [01:34<00:03, 66.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  97% 6280/6492 [01:34<00:03, 66.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  97% 6300/6492 [01:34<00:02, 66.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  97% 6320/6492 [01:35<00:02, 66.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  98% 6340/6492 [01:35<00:02, 66.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  98% 6360/6492 [01:35<00:01, 66.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  98% 6380/6492 [01:35<00:01, 66.81it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  99% 6400/6492 [01:35<00:01, 66.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  99% 6420/6492 [01:35<00:01, 67.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39:  99% 6440/6492 [01:35<00:00, 67.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39: 100% 6460/6492 [01:36<00:00, 67.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39: 100% 6480/6492 [01:36<00:00, 67.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 39: 100% 6492/6492 [01:36<00:00, 67.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  80% 5180/6492 [01:20<00:20, 63.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  80% 5200/6492 [01:25<00:21, 60.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  80% 5220/6492 [01:25<00:20, 60.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  81% 5240/6492 [01:26<00:20, 60.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  81% 5260/6492 [01:26<00:20, 60.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  81% 5280/6492 [01:26<00:19, 61.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  82% 5300/6492 [01:26<00:19, 61.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  82% 5320/6492 [01:26<00:19, 61.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  82% 5340/6492 [01:26<00:18, 61.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  83% 5360/6492 [01:27<00:18, 61.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  83% 5380/6492 [01:27<00:18, 61.70it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  83% 5400/6492 [01:27<00:17, 61.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  83% 5420/6492 [01:27<00:17, 61.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  84% 5440/6492 [01:27<00:16, 62.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  84% 5460/6492 [01:27<00:16, 62.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  84% 5480/6492 [01:27<00:16, 62.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  85% 5500/6492 [01:28<00:15, 62.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  85% 5520/6492 [01:28<00:15, 62.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  85% 5540/6492 [01:28<00:15, 62.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  86% 5560/6492 [01:28<00:14, 62.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  86% 5580/6492 [01:28<00:14, 62.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  86% 5600/6492 [01:28<00:14, 63.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  87% 5620/6492 [01:29<00:13, 63.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  87% 5640/6492 [01:29<00:13, 63.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  87% 5660/6492 [01:29<00:13, 63.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  87% 5680/6492 [01:29<00:12, 63.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  88% 5700/6492 [01:29<00:12, 63.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  88% 5720/6492 [01:29<00:12, 63.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  88% 5740/6492 [01:29<00:11, 63.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  89% 5760/6492 [01:30<00:11, 63.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  89% 5780/6492 [01:30<00:11, 64.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  89% 5800/6492 [01:30<00:10, 64.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  90% 5820/6492 [01:30<00:10, 64.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  90% 5840/6492 [01:30<00:10, 64.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  90% 5860/6492 [01:30<00:09, 64.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  91% 5880/6492 [01:30<00:09, 64.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  91% 5900/6492 [01:31<00:09, 64.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  91% 5920/6492 [01:31<00:08, 64.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  91% 5940/6492 [01:31<00:08, 64.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  92% 5960/6492 [01:31<00:08, 65.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  92% 5980/6492 [01:31<00:07, 65.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  92% 6000/6492 [01:31<00:07, 65.30it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  93% 6020/6492 [01:32<00:07, 65.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  93% 6040/6492 [01:32<00:06, 65.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  93% 6060/6492 [01:32<00:06, 65.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  94% 6080/6492 [01:32<00:06, 65.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  94% 6100/6492 [01:32<00:05, 65.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  94% 6120/6492 [01:32<00:05, 65.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  95% 6140/6492 [01:32<00:05, 66.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  95% 6160/6492 [01:33<00:05, 66.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  95% 6180/6492 [01:33<00:04, 66.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  96% 6200/6492 [01:33<00:04, 66.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  96% 6220/6492 [01:33<00:04, 66.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  96% 6240/6492 [01:33<00:03, 66.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  96% 6260/6492 [01:33<00:03, 66.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  97% 6280/6492 [01:33<00:03, 66.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  97% 6300/6492 [01:34<00:02, 66.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  97% 6320/6492 [01:34<00:02, 67.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  98% 6340/6492 [01:34<00:02, 67.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  98% 6360/6492 [01:34<00:01, 67.30it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  98% 6380/6492 [01:34<00:01, 67.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  99% 6400/6492 [01:34<00:01, 67.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  99% 6420/6492 [01:34<00:01, 67.61it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40:  99% 6440/6492 [01:35<00:00, 67.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40: 100% 6460/6492 [01:35<00:00, 67.81it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40: 100% 6480/6492 [01:35<00:00, 67.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 40: 100% 6492/6492 [01:35<00:00, 67.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.372\n",
            "Epoch 41:  80% 5180/6492 [01:21<00:20, 63.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  80% 5200/6492 [01:26<00:21, 60.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  80% 5220/6492 [01:26<00:21, 60.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  81% 5240/6492 [01:26<00:20, 60.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  81% 5260/6492 [01:26<00:20, 60.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  81% 5280/6492 [01:26<00:19, 60.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  82% 5300/6492 [01:26<00:19, 60.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  82% 5320/6492 [01:27<00:19, 61.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  82% 5340/6492 [01:27<00:18, 61.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  83% 5360/6492 [01:27<00:18, 61.36it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  83% 5380/6492 [01:27<00:18, 61.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  83% 5400/6492 [01:27<00:17, 61.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  83% 5420/6492 [01:27<00:17, 61.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  84% 5440/6492 [01:27<00:17, 61.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  84% 5460/6492 [01:28<00:16, 61.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  84% 5480/6492 [01:28<00:16, 62.06it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  85% 5500/6492 [01:28<00:15, 62.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  85% 5520/6492 [01:28<00:15, 62.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  85% 5540/6492 [01:28<00:15, 62.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  86% 5560/6492 [01:28<00:14, 62.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  86% 5580/6492 [01:29<00:14, 62.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  86% 5600/6492 [01:29<00:14, 62.74it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  87% 5620/6492 [01:29<00:13, 62.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  87% 5640/6492 [01:29<00:13, 62.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  87% 5660/6492 [01:29<00:13, 63.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  87% 5680/6492 [01:29<00:12, 63.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  88% 5700/6492 [01:30<00:12, 63.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  88% 5720/6492 [01:30<00:12, 63.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  88% 5740/6492 [01:30<00:11, 63.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  89% 5760/6492 [01:30<00:11, 63.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  89% 5780/6492 [01:30<00:11, 63.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  89% 5800/6492 [01:30<00:10, 63.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  90% 5820/6492 [01:30<00:10, 63.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  90% 5840/6492 [01:31<00:10, 64.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  90% 5860/6492 [01:31<00:09, 64.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  91% 5880/6492 [01:31<00:09, 64.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  91% 5900/6492 [01:31<00:09, 64.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  91% 5920/6492 [01:31<00:08, 64.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  91% 5940/6492 [01:31<00:08, 64.65it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  92% 5960/6492 [01:32<00:08, 64.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  92% 5980/6492 [01:32<00:07, 64.86it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  92% 6000/6492 [01:32<00:07, 64.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  93% 6020/6492 [01:32<00:07, 65.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  93% 6040/6492 [01:32<00:06, 65.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  93% 6060/6492 [01:32<00:06, 65.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  94% 6080/6492 [01:32<00:06, 65.40it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  94% 6100/6492 [01:33<00:05, 65.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  94% 6120/6492 [01:33<00:05, 65.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  95% 6140/6492 [01:33<00:05, 65.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  95% 6160/6492 [01:33<00:05, 65.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  95% 6180/6492 [01:33<00:04, 65.93it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  96% 6200/6492 [01:33<00:04, 66.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  96% 6220/6492 [01:34<00:04, 66.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  96% 6240/6492 [01:34<00:03, 66.25it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  96% 6260/6492 [01:34<00:03, 66.36it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  97% 6280/6492 [01:34<00:03, 66.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  97% 6300/6492 [01:34<00:02, 66.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  97% 6320/6492 [01:34<00:02, 66.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  98% 6340/6492 [01:34<00:02, 66.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  98% 6360/6492 [01:35<00:01, 66.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  98% 6380/6492 [01:35<00:01, 66.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  99% 6400/6492 [01:35<00:01, 67.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  99% 6420/6492 [01:35<00:01, 67.18it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41:  99% 6440/6492 [01:35<00:00, 67.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41: 100% 6460/6492 [01:35<00:00, 67.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41: 100% 6480/6492 [01:36<00:00, 67.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 41: 100% 6492/6492 [01:36<00:00, 67.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  80% 5180/6492 [01:20<00:20, 63.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  80% 5200/6492 [01:25<00:21, 60.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  80% 5220/6492 [01:25<00:20, 60.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  81% 5240/6492 [01:26<00:20, 60.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  81% 5260/6492 [01:26<00:20, 60.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  81% 5280/6492 [01:26<00:19, 61.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  82% 5300/6492 [01:26<00:19, 61.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  82% 5320/6492 [01:26<00:19, 61.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  82% 5340/6492 [01:26<00:18, 61.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  83% 5360/6492 [01:27<00:18, 61.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  83% 5380/6492 [01:27<00:18, 61.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  83% 5400/6492 [01:27<00:17, 61.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  83% 5420/6492 [01:27<00:17, 61.96it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  84% 5440/6492 [01:27<00:16, 62.08it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  84% 5460/6492 [01:27<00:16, 62.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  84% 5480/6492 [01:27<00:16, 62.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  85% 5500/6492 [01:28<00:15, 62.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  85% 5520/6492 [01:28<00:15, 62.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  85% 5540/6492 [01:28<00:15, 62.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  86% 5560/6492 [01:28<00:14, 62.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  86% 5580/6492 [01:28<00:14, 62.92it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  86% 5600/6492 [01:28<00:14, 63.04it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  87% 5620/6492 [01:29<00:13, 63.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  87% 5640/6492 [01:29<00:13, 63.17it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  87% 5660/6492 [01:29<00:13, 63.29it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  87% 5680/6492 [01:29<00:12, 63.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  88% 5700/6492 [01:29<00:12, 63.53it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  88% 5720/6492 [01:29<00:12, 63.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  88% 5740/6492 [01:30<00:11, 63.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  89% 5760/6492 [01:30<00:11, 63.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  89% 5780/6492 [01:30<00:11, 63.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  89% 5800/6492 [01:30<00:10, 64.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  90% 5820/6492 [01:30<00:10, 64.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  90% 5840/6492 [01:30<00:10, 64.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  90% 5860/6492 [01:30<00:09, 64.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  91% 5880/6492 [01:31<00:09, 64.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  91% 5900/6492 [01:31<00:09, 64.69it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  91% 5920/6492 [01:31<00:08, 64.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  91% 5940/6492 [01:31<00:08, 64.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  92% 5960/6492 [01:31<00:08, 65.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  92% 5980/6492 [01:31<00:07, 65.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  92% 6000/6492 [01:31<00:07, 65.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  93% 6020/6492 [01:32<00:07, 65.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  93% 6040/6492 [01:32<00:06, 65.45it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  93% 6060/6492 [01:32<00:06, 65.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  94% 6080/6492 [01:32<00:06, 65.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  94% 6100/6492 [01:32<00:05, 65.77it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  94% 6120/6492 [01:32<00:05, 65.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  95% 6140/6492 [01:33<00:05, 65.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  95% 6160/6492 [01:33<00:05, 66.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  95% 6180/6492 [01:33<00:04, 66.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  96% 6200/6492 [01:33<00:04, 66.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  96% 6220/6492 [01:33<00:04, 66.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  96% 6240/6492 [01:33<00:03, 66.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  96% 6260/6492 [01:33<00:03, 66.66it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  97% 6280/6492 [01:34<00:03, 66.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  97% 6300/6492 [01:34<00:02, 66.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  97% 6320/6492 [01:34<00:02, 66.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  98% 6340/6492 [01:34<00:02, 67.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  98% 6360/6492 [01:34<00:01, 67.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  98% 6380/6492 [01:34<00:01, 67.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  99% 6400/6492 [01:34<00:01, 67.41it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  99% 6420/6492 [01:35<00:01, 67.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42:  99% 6440/6492 [01:35<00:00, 67.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42: 100% 6460/6492 [01:35<00:00, 67.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42: 100% 6480/6492 [01:35<00:00, 67.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 42: 100% 6492/6492 [01:35<00:00, 67.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  80% 5180/6492 [01:21<00:20, 63.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  80% 5200/6492 [01:26<00:21, 60.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  80% 5220/6492 [01:26<00:20, 60.59it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  81% 5240/6492 [01:26<00:20, 60.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  81% 5260/6492 [01:26<00:20, 60.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  81% 5280/6492 [01:26<00:19, 60.95it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  82% 5300/6492 [01:26<00:19, 61.07it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  82% 5320/6492 [01:26<00:19, 61.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  82% 5340/6492 [01:27<00:18, 61.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  83% 5360/6492 [01:27<00:18, 61.43it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  83% 5380/6492 [01:27<00:18, 61.55it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  83% 5400/6492 [01:27<00:17, 61.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  83% 5420/6492 [01:27<00:17, 61.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  84% 5440/6492 [01:27<00:16, 61.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  84% 5460/6492 [01:28<00:16, 62.03it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  84% 5480/6492 [01:28<00:16, 62.15it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  85% 5500/6492 [01:28<00:15, 62.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  85% 5520/6492 [01:28<00:15, 62.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  85% 5540/6492 [01:28<00:15, 62.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  86% 5560/6492 [01:28<00:14, 62.62it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  86% 5580/6492 [01:28<00:14, 62.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  86% 5600/6492 [01:29<00:14, 62.85it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  87% 5620/6492 [01:29<00:13, 62.97it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  87% 5640/6492 [01:29<00:13, 63.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  87% 5660/6492 [01:29<00:13, 63.21it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  87% 5680/6492 [01:29<00:12, 63.32it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  88% 5700/6492 [01:29<00:12, 63.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  88% 5720/6492 [01:29<00:12, 63.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  88% 5740/6492 [01:30<00:11, 63.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  89% 5760/6492 [01:30<00:11, 63.79it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  89% 5780/6492 [01:30<00:11, 63.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  89% 5800/6492 [01:30<00:10, 64.02it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  90% 5820/6492 [01:30<00:10, 64.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  90% 5840/6492 [01:30<00:10, 64.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  90% 5860/6492 [01:31<00:09, 64.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  91% 5880/6492 [01:31<00:09, 64.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  91% 5900/6492 [01:31<00:09, 64.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  91% 5920/6492 [01:31<00:08, 64.71it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  91% 5940/6492 [01:31<00:08, 64.82it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  92% 5960/6492 [01:31<00:08, 64.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  92% 5980/6492 [01:31<00:07, 65.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  92% 6000/6492 [01:32<00:07, 65.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  93% 6020/6492 [01:32<00:07, 65.27it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  93% 6040/6492 [01:32<00:06, 65.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  93% 6060/6492 [01:32<00:06, 65.49it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  94% 6080/6492 [01:32<00:06, 65.60it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  94% 6100/6492 [01:32<00:05, 65.72it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  94% 6120/6492 [01:32<00:05, 65.83it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  95% 6140/6492 [01:33<00:05, 65.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  95% 6160/6492 [01:33<00:05, 66.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  95% 6180/6492 [01:33<00:04, 66.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  96% 6200/6492 [01:33<00:04, 66.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  96% 6220/6492 [01:33<00:04, 66.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  96% 6240/6492 [01:33<00:03, 66.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  96% 6260/6492 [01:34<00:03, 66.58it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  97% 6280/6492 [01:34<00:03, 66.69it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  97% 6300/6492 [01:34<00:02, 66.80it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  97% 6320/6492 [01:34<00:02, 66.91it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  98% 6340/6492 [01:34<00:02, 67.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  98% 6360/6492 [01:34<00:01, 67.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  98% 6380/6492 [01:34<00:01, 67.22it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  99% 6400/6492 [01:35<00:01, 67.33it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  99% 6420/6492 [01:35<00:01, 67.44it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43:  99% 6440/6492 [01:35<00:00, 67.54it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43: 100% 6460/6492 [01:35<00:00, 67.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43: 100% 6480/6492 [01:35<00:00, 67.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 43: 100% 6492/6492 [01:35<00:00, 67.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.370]\n",
            "Epoch 44:  80% 5180/6492 [01:20<00:20, 64.48it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  80% 5200/6492 [01:25<00:21, 60.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  80% 5220/6492 [01:25<00:20, 61.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  81% 5240/6492 [01:25<00:20, 61.13it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  81% 5260/6492 [01:25<00:20, 61.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  81% 5280/6492 [01:26<00:19, 61.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  82% 5300/6492 [01:26<00:19, 61.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  82% 5320/6492 [01:26<00:19, 61.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  82% 5340/6492 [01:26<00:18, 61.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  83% 5360/6492 [01:26<00:18, 61.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  83% 5380/6492 [01:26<00:17, 62.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  83% 5400/6492 [01:26<00:17, 62.14it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  83% 5420/6492 [01:27<00:17, 62.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  84% 5440/6492 [01:27<00:16, 62.39it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  84% 5460/6492 [01:27<00:16, 62.51it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  84% 5480/6492 [01:27<00:16, 62.64it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  85% 5500/6492 [01:27<00:15, 62.76it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  85% 5520/6492 [01:27<00:15, 62.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  85% 5540/6492 [01:27<00:15, 63.01it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  86% 5560/6492 [01:28<00:14, 63.13it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  86% 5580/6492 [01:28<00:14, 63.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  86% 5600/6492 [01:28<00:14, 63.38it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  87% 5620/6492 [01:28<00:13, 63.50it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  87% 5640/6492 [01:28<00:13, 63.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  87% 5660/6492 [01:28<00:13, 63.75it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  87% 5680/6492 [01:28<00:12, 63.87it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  88% 5700/6492 [01:29<00:12, 63.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  88% 5720/6492 [01:29<00:12, 64.11it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  88% 5740/6492 [01:29<00:11, 64.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  89% 5760/6492 [01:29<00:11, 64.34it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  89% 5780/6492 [01:29<00:11, 64.45it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  89% 5800/6492 [01:29<00:10, 64.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  90% 5820/6492 [01:29<00:10, 64.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  90% 5840/6492 [01:30<00:10, 64.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  90% 5860/6492 [01:30<00:09, 64.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  91% 5880/6492 [01:30<00:09, 65.00it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  91% 5900/6492 [01:30<00:09, 65.12it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  91% 5920/6492 [01:30<00:08, 65.23it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  91% 5940/6492 [01:30<00:08, 65.35it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  92% 5960/6492 [01:31<00:08, 65.46it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  92% 5980/6492 [01:31<00:07, 65.56it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  92% 6000/6492 [01:31<00:07, 65.67it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  93% 6020/6492 [01:31<00:07, 65.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  93% 6040/6492 [01:31<00:06, 65.89it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  93% 6060/6492 [01:31<00:06, 65.99it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  94% 6080/6492 [01:31<00:06, 66.10it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  94% 6100/6492 [01:32<00:05, 66.20it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  94% 6120/6492 [01:32<00:05, 66.31it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  95% 6140/6492 [01:32<00:05, 66.42it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  95% 6160/6492 [01:32<00:04, 66.52it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  95% 6180/6492 [01:32<00:04, 66.63it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  96% 6200/6492 [01:32<00:04, 66.73it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  96% 6220/6492 [01:33<00:04, 66.84it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  96% 6240/6492 [01:33<00:03, 66.94it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  96% 6260/6492 [01:33<00:03, 67.05it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  97% 6280/6492 [01:33<00:03, 67.16it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  97% 6300/6492 [01:33<00:02, 67.26it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  97% 6320/6492 [01:33<00:02, 67.37it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  98% 6340/6492 [01:33<00:02, 67.47it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  98% 6360/6492 [01:34<00:01, 67.57it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  98% 6380/6492 [01:34<00:01, 67.68it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  99% 6400/6492 [01:34<00:01, 67.78it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  99% 6420/6492 [01:34<00:01, 67.88it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44:  99% 6440/6492 [01:34<00:00, 67.98it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44: 100% 6460/6492 [01:34<00:00, 68.09it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44: 100% 6480/6492 [01:35<00:00, 68.19it/s, loss=4.37, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 44: 100% 6492/6492 [01:35<00:00, 68.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  80% 5180/6492 [01:19<00:20, 64.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  80% 5200/6492 [01:24<00:21, 61.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  80% 5220/6492 [01:24<00:20, 61.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  81% 5240/6492 [01:25<00:20, 61.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  81% 5260/6492 [01:25<00:19, 61.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  81% 5280/6492 [01:25<00:19, 61.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  82% 5300/6492 [01:25<00:19, 61.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  82% 5320/6492 [01:25<00:18, 62.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  82% 5340/6492 [01:25<00:18, 62.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  83% 5360/6492 [01:25<00:18, 62.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  83% 5380/6492 [01:26<00:17, 62.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  83% 5400/6492 [01:26<00:17, 62.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  83% 5420/6492 [01:26<00:17, 62.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  84% 5440/6492 [01:26<00:16, 62.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  84% 5460/6492 [01:26<00:16, 62.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  84% 5480/6492 [01:26<00:16, 63.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  85% 5500/6492 [01:27<00:15, 63.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  85% 5520/6492 [01:27<00:15, 63.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  85% 5540/6492 [01:27<00:15, 63.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  86% 5560/6492 [01:27<00:14, 63.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  86% 5580/6492 [01:27<00:14, 63.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  86% 5600/6492 [01:27<00:13, 63.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  87% 5620/6492 [01:28<00:13, 63.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  87% 5640/6492 [01:28<00:13, 63.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  87% 5660/6492 [01:28<00:12, 64.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  87% 5680/6492 [01:28<00:12, 64.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  88% 5700/6492 [01:28<00:12, 64.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  88% 5720/6492 [01:28<00:11, 64.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  88% 5740/6492 [01:28<00:11, 64.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  89% 5760/6492 [01:29<00:11, 64.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  89% 5780/6492 [01:29<00:11, 64.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  89% 5800/6492 [01:29<00:10, 64.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  90% 5820/6492 [01:29<00:10, 64.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  90% 5840/6492 [01:29<00:10, 65.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  90% 5860/6492 [01:29<00:09, 65.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  91% 5880/6492 [01:30<00:09, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  91% 5900/6492 [01:30<00:09, 65.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  91% 5920/6492 [01:30<00:08, 65.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  91% 5940/6492 [01:30<00:08, 65.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  92% 5960/6492 [01:30<00:08, 65.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  92% 5980/6492 [01:30<00:07, 65.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  92% 6000/6492 [01:31<00:07, 65.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  93% 6020/6492 [01:31<00:07, 66.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  93% 6040/6492 [01:31<00:06, 66.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  93% 6060/6492 [01:31<00:06, 66.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  94% 6080/6492 [01:31<00:06, 66.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  94% 6100/6492 [01:31<00:05, 66.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  94% 6120/6492 [01:31<00:05, 66.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  95% 6140/6492 [01:32<00:05, 66.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  95% 6160/6492 [01:32<00:04, 66.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  95% 6180/6492 [01:32<00:04, 66.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  96% 6200/6492 [01:32<00:04, 66.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  96% 6220/6492 [01:32<00:04, 67.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  96% 6240/6492 [01:32<00:03, 67.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  96% 6260/6492 [01:33<00:03, 67.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  97% 6280/6492 [01:33<00:03, 67.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  97% 6300/6492 [01:33<00:02, 67.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  97% 6320/6492 [01:33<00:02, 67.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  98% 6340/6492 [01:33<00:02, 67.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  98% 6360/6492 [01:33<00:01, 67.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  98% 6380/6492 [01:33<00:01, 67.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  99% 6400/6492 [01:34<00:01, 68.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  99% 6420/6492 [01:34<00:01, 68.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45:  99% 6440/6492 [01:34<00:00, 68.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45: 100% 6460/6492 [01:34<00:00, 68.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45: 100% 6480/6492 [01:34<00:00, 68.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 45: 100% 6492/6492 [01:34<00:00, 68.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  80% 5180/6492 [01:21<00:20, 63.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  80% 5200/6492 [01:26<00:21, 60.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  80% 5220/6492 [01:26<00:21, 60.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  81% 5240/6492 [01:26<00:20, 60.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  81% 5260/6492 [01:27<00:20, 60.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  81% 5280/6492 [01:27<00:20, 60.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  82% 5300/6492 [01:27<00:19, 60.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  82% 5320/6492 [01:27<00:19, 60.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  82% 5340/6492 [01:27<00:18, 60.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  83% 5360/6492 [01:27<00:18, 60.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  83% 5380/6492 [01:28<00:18, 61.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  83% 5400/6492 [01:28<00:17, 61.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  83% 5420/6492 [01:28<00:17, 61.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  84% 5440/6492 [01:28<00:17, 61.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  84% 5460/6492 [01:28<00:16, 61.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  84% 5480/6492 [01:28<00:16, 61.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  85% 5500/6492 [01:28<00:16, 61.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  85% 5520/6492 [01:29<00:15, 61.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  85% 5540/6492 [01:29<00:15, 62.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  86% 5560/6492 [01:29<00:14, 62.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  86% 5580/6492 [01:29<00:14, 62.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  86% 5600/6492 [01:29<00:14, 62.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  87% 5620/6492 [01:29<00:13, 62.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  87% 5640/6492 [01:30<00:13, 62.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  87% 5660/6492 [01:30<00:13, 62.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  87% 5680/6492 [01:30<00:12, 62.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  88% 5700/6492 [01:30<00:12, 62.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  88% 5720/6492 [01:30<00:12, 63.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  88% 5740/6492 [01:30<00:11, 63.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  89% 5760/6492 [01:31<00:11, 63.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  89% 5780/6492 [01:31<00:11, 63.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  89% 5800/6492 [01:31<00:10, 63.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  90% 5820/6492 [01:31<00:10, 63.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  90% 5840/6492 [01:31<00:10, 63.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  90% 5860/6492 [01:31<00:09, 63.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  91% 5880/6492 [01:32<00:09, 63.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  91% 5900/6492 [01:32<00:09, 63.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  91% 5920/6492 [01:32<00:08, 64.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  91% 5940/6492 [01:32<00:08, 64.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  92% 5960/6492 [01:32<00:08, 64.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  92% 5980/6492 [01:32<00:07, 64.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  92% 6000/6492 [01:32<00:07, 64.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  93% 6020/6492 [01:33<00:07, 64.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  93% 6040/6492 [01:33<00:06, 64.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  93% 6060/6492 [01:33<00:06, 64.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  94% 6080/6492 [01:33<00:06, 64.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  94% 6100/6492 [01:33<00:06, 65.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  94% 6120/6492 [01:33<00:05, 65.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  95% 6140/6492 [01:34<00:05, 65.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  95% 6160/6492 [01:34<00:05, 65.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  95% 6180/6492 [01:34<00:04, 65.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  96% 6200/6492 [01:34<00:04, 65.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  96% 6220/6492 [01:34<00:04, 65.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  96% 6240/6492 [01:34<00:03, 65.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  96% 6260/6492 [01:34<00:03, 65.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  97% 6280/6492 [01:35<00:03, 66.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  97% 6300/6492 [01:35<00:02, 66.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  97% 6320/6492 [01:35<00:02, 66.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  98% 6340/6492 [01:35<00:02, 66.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  98% 6360/6492 [01:35<00:01, 66.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  98% 6380/6492 [01:35<00:01, 66.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  99% 6400/6492 [01:36<00:01, 66.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  99% 6420/6492 [01:36<00:01, 66.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46:  99% 6440/6492 [01:36<00:00, 66.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46: 100% 6460/6492 [01:36<00:00, 66.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46: 100% 6480/6492 [01:36<00:00, 67.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 46: 100% 6492/6492 [01:36<00:00, 67.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  80% 5180/6492 [01:21<00:20, 63.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  80% 5200/6492 [01:26<00:21, 60.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  80% 5220/6492 [01:26<00:21, 60.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  81% 5240/6492 [01:26<00:20, 60.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  81% 5260/6492 [01:27<00:20, 60.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  81% 5280/6492 [01:27<00:20, 60.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  82% 5300/6492 [01:27<00:19, 60.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  82% 5320/6492 [01:27<00:19, 60.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  82% 5340/6492 [01:27<00:18, 60.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  83% 5360/6492 [01:27<00:18, 61.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  83% 5380/6492 [01:28<00:18, 61.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  83% 5400/6492 [01:28<00:17, 61.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  83% 5420/6492 [01:28<00:17, 61.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  84% 5440/6492 [01:28<00:17, 61.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  84% 5460/6492 [01:28<00:16, 61.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  84% 5480/6492 [01:28<00:16, 61.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  85% 5500/6492 [01:28<00:16, 61.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  85% 5520/6492 [01:29<00:15, 61.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  85% 5540/6492 [01:29<00:15, 62.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  86% 5560/6492 [01:29<00:14, 62.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  86% 5580/6492 [01:29<00:14, 62.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  86% 5600/6492 [01:29<00:14, 62.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  87% 5620/6492 [01:29<00:13, 62.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  87% 5640/6492 [01:29<00:13, 62.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  87% 5660/6492 [01:30<00:13, 62.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  87% 5680/6492 [01:30<00:12, 62.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  88% 5700/6492 [01:30<00:12, 63.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  88% 5720/6492 [01:30<00:12, 63.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  88% 5740/6492 [01:30<00:11, 63.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  89% 5760/6492 [01:30<00:11, 63.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  89% 5780/6492 [01:31<00:11, 63.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  89% 5800/6492 [01:31<00:10, 63.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  90% 5820/6492 [01:31<00:10, 63.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  90% 5840/6492 [01:31<00:10, 63.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  90% 5860/6492 [01:31<00:09, 63.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  91% 5880/6492 [01:31<00:09, 64.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  91% 5900/6492 [01:31<00:09, 64.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  91% 5920/6492 [01:32<00:08, 64.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  91% 5940/6492 [01:32<00:08, 64.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  92% 5960/6492 [01:32<00:08, 64.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  92% 5980/6492 [01:32<00:07, 64.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  92% 6000/6492 [01:32<00:07, 64.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  93% 6020/6492 [01:32<00:07, 64.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  93% 6040/6492 [01:33<00:06, 64.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  93% 6060/6492 [01:33<00:06, 65.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  94% 6080/6492 [01:33<00:06, 65.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  94% 6100/6492 [01:33<00:06, 65.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  94% 6120/6492 [01:33<00:05, 65.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  95% 6140/6492 [01:33<00:05, 65.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  95% 6160/6492 [01:33<00:05, 65.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  95% 6180/6492 [01:34<00:04, 65.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  96% 6200/6492 [01:34<00:04, 65.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  96% 6220/6492 [01:34<00:04, 65.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  96% 6240/6492 [01:34<00:03, 65.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  96% 6260/6492 [01:34<00:03, 66.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  97% 6280/6492 [01:34<00:03, 66.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  97% 6300/6492 [01:35<00:02, 66.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  97% 6320/6492 [01:35<00:02, 66.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  98% 6340/6492 [01:35<00:02, 66.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  98% 6360/6492 [01:35<00:01, 66.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  98% 6380/6492 [01:35<00:01, 66.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  99% 6400/6492 [01:35<00:01, 66.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  99% 6420/6492 [01:35<00:01, 66.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47:  99% 6440/6492 [01:36<00:00, 67.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47: 100% 6460/6492 [01:36<00:00, 67.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47: 100% 6480/6492 [01:36<00:00, 67.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 47: 100% 6492/6492 [01:36<00:00, 67.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.371\n",
            "Epoch 48:  80% 5180/6492 [01:21<00:20, 63.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  80% 5200/6492 [01:26<00:21, 60.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  80% 5220/6492 [01:26<00:20, 60.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  81% 5240/6492 [01:26<00:20, 60.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  81% 5260/6492 [01:26<00:20, 60.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  81% 5280/6492 [01:26<00:19, 60.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  82% 5300/6492 [01:26<00:19, 61.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  82% 5320/6492 [01:26<00:19, 61.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  82% 5340/6492 [01:27<00:18, 61.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  83% 5360/6492 [01:27<00:18, 61.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  83% 5380/6492 [01:27<00:18, 61.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  83% 5400/6492 [01:27<00:17, 61.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  83% 5420/6492 [01:27<00:17, 61.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  84% 5440/6492 [01:27<00:16, 61.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  84% 5460/6492 [01:27<00:16, 62.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  84% 5480/6492 [01:28<00:16, 62.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  85% 5500/6492 [01:28<00:15, 62.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  85% 5520/6492 [01:28<00:15, 62.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  85% 5540/6492 [01:28<00:15, 62.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  86% 5560/6492 [01:28<00:14, 62.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  86% 5580/6492 [01:28<00:14, 62.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  86% 5600/6492 [01:29<00:14, 62.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  87% 5620/6492 [01:29<00:13, 62.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  87% 5640/6492 [01:29<00:13, 63.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  87% 5660/6492 [01:29<00:13, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  87% 5680/6492 [01:29<00:12, 63.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  88% 5700/6492 [01:29<00:12, 63.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  88% 5720/6492 [01:29<00:12, 63.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  88% 5740/6492 [01:30<00:11, 63.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  89% 5760/6492 [01:30<00:11, 63.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  89% 5780/6492 [01:30<00:11, 63.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  89% 5800/6492 [01:30<00:10, 64.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  90% 5820/6492 [01:30<00:10, 64.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  90% 5840/6492 [01:30<00:10, 64.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  90% 5860/6492 [01:31<00:09, 64.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  91% 5880/6492 [01:31<00:09, 64.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  91% 5900/6492 [01:31<00:09, 64.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  91% 5920/6492 [01:31<00:08, 64.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  91% 5940/6492 [01:31<00:08, 64.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  92% 5960/6492 [01:31<00:08, 64.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  92% 5980/6492 [01:31<00:07, 65.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  92% 6000/6492 [01:32<00:07, 65.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  93% 6020/6492 [01:32<00:07, 65.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  93% 6040/6492 [01:32<00:06, 65.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  93% 6060/6492 [01:32<00:06, 65.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  94% 6080/6492 [01:32<00:06, 65.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  94% 6100/6492 [01:32<00:05, 65.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  94% 6120/6492 [01:33<00:05, 65.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  95% 6140/6492 [01:33<00:05, 65.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  95% 6160/6492 [01:33<00:05, 65.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  95% 6180/6492 [01:33<00:04, 66.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  96% 6200/6492 [01:33<00:04, 66.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  96% 6220/6492 [01:33<00:04, 66.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  96% 6240/6492 [01:34<00:03, 66.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  96% 6260/6492 [01:34<00:03, 66.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  97% 6280/6492 [01:34<00:03, 66.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  97% 6300/6492 [01:34<00:02, 66.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  97% 6320/6492 [01:34<00:02, 66.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  98% 6340/6492 [01:34<00:02, 66.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  98% 6360/6492 [01:34<00:01, 66.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  98% 6380/6492 [01:35<00:01, 67.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  99% 6400/6492 [01:35<00:01, 67.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  99% 6420/6492 [01:35<00:01, 67.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48:  99% 6440/6492 [01:35<00:00, 67.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48: 100% 6460/6492 [01:35<00:00, 67.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48: 100% 6480/6492 [01:35<00:00, 67.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 48: 100% 6492/6492 [01:36<00:00, 67.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  80% 5180/6492 [01:21<00:20, 63.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  80% 5200/6492 [01:26<00:21, 60.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  80% 5220/6492 [01:26<00:21, 60.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  81% 5240/6492 [01:26<00:20, 60.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  81% 5260/6492 [01:26<00:20, 60.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  81% 5280/6492 [01:26<00:19, 60.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  82% 5300/6492 [01:26<00:19, 60.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  82% 5320/6492 [01:27<00:19, 61.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  82% 5340/6492 [01:27<00:18, 61.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  83% 5360/6492 [01:27<00:18, 61.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  83% 5380/6492 [01:27<00:18, 61.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  83% 5400/6492 [01:27<00:17, 61.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  83% 5420/6492 [01:27<00:17, 61.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  84% 5440/6492 [01:28<00:17, 61.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  84% 5460/6492 [01:28<00:16, 61.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  84% 5480/6492 [01:28<00:16, 62.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  85% 5500/6492 [01:28<00:15, 62.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  85% 5520/6492 [01:28<00:15, 62.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  85% 5540/6492 [01:28<00:15, 62.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  86% 5560/6492 [01:28<00:14, 62.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  86% 5580/6492 [01:29<00:14, 62.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  86% 5600/6492 [01:29<00:14, 62.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  87% 5620/6492 [01:29<00:13, 62.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  87% 5640/6492 [01:29<00:13, 62.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  87% 5660/6492 [01:29<00:13, 63.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  87% 5680/6492 [01:29<00:12, 63.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  88% 5700/6492 [01:30<00:12, 63.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  88% 5720/6492 [01:30<00:12, 63.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  88% 5740/6492 [01:30<00:11, 63.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  89% 5760/6492 [01:30<00:11, 63.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  89% 5780/6492 [01:30<00:11, 63.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  89% 5800/6492 [01:30<00:10, 63.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  90% 5820/6492 [01:30<00:10, 64.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  90% 5840/6492 [01:31<00:10, 64.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  90% 5860/6492 [01:31<00:09, 64.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  91% 5880/6492 [01:31<00:09, 64.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  91% 5900/6492 [01:31<00:09, 64.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  91% 5920/6492 [01:31<00:08, 64.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  91% 5940/6492 [01:31<00:08, 64.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  92% 5960/6492 [01:32<00:08, 64.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  92% 5980/6492 [01:32<00:07, 64.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  92% 6000/6492 [01:32<00:07, 64.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  93% 6020/6492 [01:32<00:07, 65.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  93% 6040/6492 [01:32<00:06, 65.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  93% 6060/6492 [01:32<00:06, 65.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  94% 6080/6492 [01:33<00:06, 65.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  94% 6100/6492 [01:33<00:05, 65.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  94% 6120/6492 [01:33<00:05, 65.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  95% 6140/6492 [01:33<00:05, 65.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  95% 6160/6492 [01:33<00:05, 65.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  95% 6180/6492 [01:33<00:04, 65.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  96% 6200/6492 [01:34<00:04, 65.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  96% 6220/6492 [01:34<00:04, 66.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  96% 6240/6492 [01:34<00:03, 66.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  96% 6260/6492 [01:34<00:03, 66.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  97% 6280/6492 [01:34<00:03, 66.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  97% 6300/6492 [01:34<00:02, 66.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  97% 6320/6492 [01:34<00:02, 66.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  98% 6340/6492 [01:35<00:02, 66.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  98% 6360/6492 [01:35<00:01, 66.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  98% 6380/6492 [01:35<00:01, 66.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  99% 6400/6492 [01:35<00:01, 67.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  99% 6420/6492 [01:35<00:01, 67.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49:  99% 6440/6492 [01:35<00:00, 67.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49: 100% 6460/6492 [01:35<00:00, 67.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49: 100% 6480/6492 [01:36<00:00, 67.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 49: 100% 6492/6492 [01:36<00:00, 67.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  80% 5180/6492 [01:20<00:20, 64.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 50:  80% 5200/6492 [01:25<00:21, 60.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  80% 5220/6492 [01:25<00:20, 60.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  81% 5240/6492 [01:26<00:20, 60.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  81% 5260/6492 [01:26<00:20, 60.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  81% 5280/6492 [01:26<00:19, 61.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  82% 5300/6492 [01:26<00:19, 61.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  82% 5320/6492 [01:26<00:19, 61.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  82% 5340/6492 [01:26<00:18, 61.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  83% 5360/6492 [01:27<00:18, 61.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  83% 5380/6492 [01:27<00:18, 61.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  83% 5400/6492 [01:27<00:17, 61.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  83% 5420/6492 [01:27<00:17, 61.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  84% 5440/6492 [01:27<00:16, 62.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  84% 5460/6492 [01:27<00:16, 62.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  84% 5480/6492 [01:27<00:16, 62.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  85% 5500/6492 [01:28<00:15, 62.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  85% 5520/6492 [01:28<00:15, 62.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  85% 5540/6492 [01:28<00:15, 62.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  86% 5560/6492 [01:28<00:14, 62.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  86% 5580/6492 [01:28<00:14, 62.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  86% 5600/6492 [01:28<00:14, 63.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  87% 5620/6492 [01:29<00:13, 63.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  87% 5640/6492 [01:29<00:13, 63.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  87% 5660/6492 [01:29<00:13, 63.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  87% 5680/6492 [01:29<00:12, 63.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  88% 5700/6492 [01:29<00:12, 63.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  88% 5720/6492 [01:29<00:12, 63.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  88% 5740/6492 [01:29<00:11, 63.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  89% 5760/6492 [01:30<00:11, 63.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  89% 5780/6492 [01:30<00:11, 64.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  89% 5800/6492 [01:30<00:10, 64.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  90% 5820/6492 [01:30<00:10, 64.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  90% 5840/6492 [01:30<00:10, 64.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  90% 5860/6492 [01:30<00:09, 64.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  91% 5880/6492 [01:31<00:09, 64.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  91% 5900/6492 [01:31<00:09, 64.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  91% 5920/6492 [01:31<00:08, 64.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  91% 5940/6492 [01:31<00:08, 64.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  92% 5960/6492 [01:31<00:08, 65.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  92% 5980/6492 [01:31<00:07, 65.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  92% 6000/6492 [01:31<00:07, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  93% 6020/6492 [01:32<00:07, 65.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  93% 6040/6492 [01:32<00:06, 65.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  93% 6060/6492 [01:32<00:06, 65.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  94% 6080/6492 [01:32<00:06, 65.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  94% 6100/6492 [01:32<00:05, 65.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  94% 6120/6492 [01:32<00:05, 65.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  95% 6140/6492 [01:32<00:05, 66.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  95% 6160/6492 [01:33<00:05, 66.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  95% 6180/6492 [01:33<00:04, 66.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  96% 6200/6492 [01:33<00:04, 66.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  96% 6220/6492 [01:33<00:04, 66.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  96% 6240/6492 [01:33<00:03, 66.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  96% 6260/6492 [01:33<00:03, 66.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  97% 6280/6492 [01:34<00:03, 66.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  97% 6300/6492 [01:34<00:02, 66.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  97% 6320/6492 [01:34<00:02, 66.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  98% 6340/6492 [01:34<00:02, 67.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  98% 6360/6492 [01:34<00:01, 67.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  98% 6380/6492 [01:34<00:01, 67.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  99% 6400/6492 [01:34<00:01, 67.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  99% 6420/6492 [01:35<00:01, 67.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50:  99% 6440/6492 [01:35<00:00, 67.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50: 100% 6460/6492 [01:35<00:00, 67.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50: 100% 6480/6492 [01:35<00:00, 67.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 50: 100% 6492/6492 [01:35<00:00, 67.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  80% 5180/6492 [01:21<00:20, 63.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 51:  80% 5200/6492 [01:26<00:21, 60.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  80% 5220/6492 [01:26<00:21, 60.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  81% 5240/6492 [01:26<00:20, 60.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  81% 5260/6492 [01:26<00:20, 60.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  81% 5280/6492 [01:26<00:19, 60.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  82% 5300/6492 [01:27<00:19, 60.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  82% 5320/6492 [01:27<00:19, 61.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  82% 5340/6492 [01:27<00:18, 61.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  83% 5360/6492 [01:27<00:18, 61.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  83% 5380/6492 [01:27<00:18, 61.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  83% 5400/6492 [01:27<00:17, 61.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  83% 5420/6492 [01:27<00:17, 61.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  84% 5440/6492 [01:28<00:17, 61.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  84% 5460/6492 [01:28<00:16, 61.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  84% 5480/6492 [01:28<00:16, 62.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  85% 5500/6492 [01:28<00:15, 62.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  85% 5520/6492 [01:28<00:15, 62.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  85% 5540/6492 [01:28<00:15, 62.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  86% 5560/6492 [01:28<00:14, 62.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  86% 5580/6492 [01:29<00:14, 62.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  86% 5600/6492 [01:29<00:14, 62.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  87% 5620/6492 [01:29<00:13, 62.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  87% 5640/6492 [01:29<00:13, 62.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  87% 5660/6492 [01:29<00:13, 63.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  87% 5680/6492 [01:29<00:12, 63.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  88% 5700/6492 [01:29<00:12, 63.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  88% 5720/6492 [01:30<00:12, 63.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  88% 5740/6492 [01:30<00:11, 63.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  89% 5760/6492 [01:30<00:11, 63.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  89% 5780/6492 [01:30<00:11, 63.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  89% 5800/6492 [01:30<00:10, 63.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  90% 5820/6492 [01:30<00:10, 64.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  90% 5840/6492 [01:31<00:10, 64.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  90% 5860/6492 [01:31<00:09, 64.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  91% 5880/6492 [01:31<00:09, 64.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  91% 5900/6492 [01:31<00:09, 64.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  91% 5920/6492 [01:31<00:08, 64.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  91% 5940/6492 [01:31<00:08, 64.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  92% 5960/6492 [01:31<00:08, 64.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  92% 5980/6492 [01:32<00:07, 64.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  92% 6000/6492 [01:32<00:07, 65.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  93% 6020/6492 [01:32<00:07, 65.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  93% 6040/6492 [01:32<00:06, 65.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  93% 6060/6492 [01:32<00:06, 65.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  94% 6080/6492 [01:32<00:06, 65.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  94% 6100/6492 [01:32<00:05, 65.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  94% 6120/6492 [01:33<00:05, 65.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  95% 6140/6492 [01:33<00:05, 65.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  95% 6160/6492 [01:33<00:05, 65.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  95% 6180/6492 [01:33<00:04, 66.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  96% 6200/6492 [01:33<00:04, 66.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  96% 6220/6492 [01:33<00:04, 66.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  96% 6240/6492 [01:33<00:03, 66.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  96% 6260/6492 [01:34<00:03, 66.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  97% 6280/6492 [01:34<00:03, 66.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  97% 6300/6492 [01:34<00:02, 66.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  97% 6320/6492 [01:34<00:02, 66.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  98% 6340/6492 [01:34<00:02, 66.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  98% 6360/6492 [01:34<00:01, 66.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  98% 6380/6492 [01:35<00:01, 67.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  99% 6400/6492 [01:35<00:01, 67.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  99% 6420/6492 [01:35<00:01, 67.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51:  99% 6440/6492 [01:35<00:00, 67.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51: 100% 6460/6492 [01:35<00:00, 67.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51: 100% 6480/6492 [01:35<00:00, 67.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 51: 100% 6492/6492 [01:35<00:00, 67.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  80% 5180/6492 [01:21<00:20, 63.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 52:  80% 5200/6492 [01:26<00:21, 60.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  80% 5220/6492 [01:26<00:21, 60.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  81% 5240/6492 [01:26<00:20, 60.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  81% 5260/6492 [01:26<00:20, 60.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  81% 5280/6492 [01:26<00:19, 60.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  82% 5300/6492 [01:27<00:19, 60.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  82% 5320/6492 [01:27<00:19, 60.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  82% 5340/6492 [01:27<00:18, 61.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  83% 5360/6492 [01:27<00:18, 61.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  83% 5380/6492 [01:27<00:18, 61.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  83% 5400/6492 [01:27<00:17, 61.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  83% 5420/6492 [01:28<00:17, 61.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  84% 5440/6492 [01:28<00:17, 61.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  84% 5460/6492 [01:28<00:16, 61.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  84% 5480/6492 [01:28<00:16, 61.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  85% 5500/6492 [01:28<00:15, 62.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  85% 5520/6492 [01:28<00:15, 62.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  85% 5540/6492 [01:28<00:15, 62.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  86% 5560/6492 [01:29<00:14, 62.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  86% 5580/6492 [01:29<00:14, 62.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  86% 5600/6492 [01:29<00:14, 62.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  87% 5620/6492 [01:29<00:13, 62.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  87% 5640/6492 [01:29<00:13, 62.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  87% 5660/6492 [01:29<00:13, 62.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  87% 5680/6492 [01:30<00:12, 63.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  88% 5700/6492 [01:30<00:12, 63.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  88% 5720/6492 [01:30<00:12, 63.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  88% 5740/6492 [01:30<00:11, 63.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  89% 5760/6492 [01:30<00:11, 63.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  89% 5780/6492 [01:30<00:11, 63.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  89% 5800/6492 [01:30<00:10, 63.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  90% 5820/6492 [01:31<00:10, 63.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  90% 5840/6492 [01:31<00:10, 64.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  90% 5860/6492 [01:31<00:09, 64.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  91% 5880/6492 [01:31<00:09, 64.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  91% 5900/6492 [01:31<00:09, 64.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  91% 5920/6492 [01:31<00:08, 64.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  91% 5940/6492 [01:31<00:08, 64.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  92% 5960/6492 [01:32<00:08, 64.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  92% 5980/6492 [01:32<00:07, 64.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  92% 6000/6492 [01:32<00:07, 64.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  93% 6020/6492 [01:32<00:07, 64.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  93% 6040/6492 [01:32<00:06, 65.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  93% 6060/6492 [01:32<00:06, 65.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  94% 6080/6492 [01:33<00:06, 65.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  94% 6100/6492 [01:33<00:05, 65.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  94% 6120/6492 [01:33<00:05, 65.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  95% 6140/6492 [01:33<00:05, 65.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  95% 6160/6492 [01:33<00:05, 65.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  95% 6180/6492 [01:33<00:04, 65.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  96% 6200/6492 [01:34<00:04, 65.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  96% 6220/6492 [01:34<00:04, 65.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  96% 6240/6492 [01:34<00:03, 66.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  96% 6260/6492 [01:34<00:03, 66.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  97% 6280/6492 [01:34<00:03, 66.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  97% 6300/6492 [01:34<00:02, 66.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  97% 6320/6492 [01:35<00:02, 66.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  98% 6340/6492 [01:35<00:02, 66.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  98% 6360/6492 [01:35<00:01, 66.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  98% 6380/6492 [01:35<00:01, 66.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  99% 6400/6492 [01:35<00:01, 66.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  99% 6420/6492 [01:35<00:01, 67.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52:  99% 6440/6492 [01:35<00:00, 67.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52: 100% 6460/6492 [01:36<00:00, 67.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52: 100% 6480/6492 [01:36<00:00, 67.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 52: 100% 6492/6492 [01:36<00:00, 67.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  80% 5180/6492 [01:21<00:20, 63.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 53:  80% 5200/6492 [01:26<00:21, 60.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  80% 5220/6492 [01:26<00:21, 60.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  81% 5240/6492 [01:26<00:20, 60.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  81% 5260/6492 [01:26<00:20, 60.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  81% 5280/6492 [01:26<00:19, 60.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  82% 5300/6492 [01:26<00:19, 61.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  82% 5320/6492 [01:26<00:19, 61.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  82% 5340/6492 [01:27<00:18, 61.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  83% 5360/6492 [01:27<00:18, 61.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  83% 5380/6492 [01:27<00:18, 61.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  83% 5400/6492 [01:27<00:17, 61.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  83% 5420/6492 [01:27<00:17, 61.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  84% 5440/6492 [01:27<00:16, 61.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  84% 5460/6492 [01:28<00:16, 62.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  84% 5480/6492 [01:28<00:16, 62.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  85% 5500/6492 [01:28<00:15, 62.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  85% 5520/6492 [01:28<00:15, 62.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  85% 5540/6492 [01:28<00:15, 62.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  86% 5560/6492 [01:28<00:14, 62.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  86% 5580/6492 [01:28<00:14, 62.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  86% 5600/6492 [01:29<00:14, 62.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  87% 5620/6492 [01:29<00:13, 62.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  87% 5640/6492 [01:29<00:13, 63.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  87% 5660/6492 [01:29<00:13, 63.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  87% 5680/6492 [01:29<00:12, 63.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  88% 5700/6492 [01:29<00:12, 63.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  88% 5720/6492 [01:30<00:12, 63.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  88% 5740/6492 [01:30<00:11, 63.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  89% 5760/6492 [01:30<00:11, 63.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  89% 5780/6492 [01:30<00:11, 63.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  89% 5800/6492 [01:30<00:10, 63.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  90% 5820/6492 [01:30<00:10, 64.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  90% 5840/6492 [01:30<00:10, 64.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  90% 5860/6492 [01:31<00:09, 64.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  91% 5880/6492 [01:31<00:09, 64.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  91% 5900/6492 [01:31<00:09, 64.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  91% 5920/6492 [01:31<00:08, 64.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  91% 5940/6492 [01:31<00:08, 64.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  92% 5960/6492 [01:31<00:08, 64.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  92% 5980/6492 [01:32<00:07, 64.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  92% 6000/6492 [01:32<00:07, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  93% 6020/6492 [01:32<00:07, 65.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  93% 6040/6492 [01:32<00:06, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  93% 6060/6492 [01:32<00:06, 65.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  94% 6080/6492 [01:32<00:06, 65.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  94% 6100/6492 [01:32<00:05, 65.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  94% 6120/6492 [01:33<00:05, 65.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  95% 6140/6492 [01:33<00:05, 65.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  95% 6160/6492 [01:33<00:05, 65.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  95% 6180/6492 [01:33<00:04, 66.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  96% 6200/6492 [01:33<00:04, 66.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  96% 6220/6492 [01:33<00:04, 66.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  96% 6240/6492 [01:34<00:03, 66.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  96% 6260/6492 [01:34<00:03, 66.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  97% 6280/6492 [01:34<00:03, 66.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  97% 6300/6492 [01:34<00:02, 66.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  97% 6320/6492 [01:34<00:02, 66.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  98% 6340/6492 [01:34<00:02, 66.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  98% 6360/6492 [01:34<00:01, 66.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  98% 6380/6492 [01:35<00:01, 67.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  99% 6400/6492 [01:35<00:01, 67.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  99% 6420/6492 [01:35<00:01, 67.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53:  99% 6440/6492 [01:35<00:00, 67.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53: 100% 6460/6492 [01:35<00:00, 67.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53: 100% 6480/6492 [01:35<00:00, 67.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 53: 100% 6492/6492 [01:36<00:00, 67.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  80% 5180/6492 [01:21<00:20, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 54:  80% 5200/6492 [01:26<00:21, 59.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  80% 5220/6492 [01:26<00:21, 60.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  81% 5240/6492 [01:27<00:20, 60.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  81% 5260/6492 [01:27<00:20, 60.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  81% 5280/6492 [01:27<00:20, 60.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  82% 5300/6492 [01:27<00:19, 60.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  82% 5320/6492 [01:27<00:19, 60.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  82% 5340/6492 [01:27<00:18, 60.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  83% 5360/6492 [01:28<00:18, 60.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  83% 5380/6492 [01:28<00:18, 60.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  83% 5400/6492 [01:28<00:17, 61.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  83% 5420/6492 [01:28<00:17, 61.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  84% 5440/6492 [01:28<00:17, 61.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  84% 5460/6492 [01:28<00:16, 61.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  84% 5480/6492 [01:29<00:16, 61.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  85% 5500/6492 [01:29<00:16, 61.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  85% 5520/6492 [01:29<00:15, 61.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  85% 5540/6492 [01:29<00:15, 61.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  86% 5560/6492 [01:29<00:15, 61.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  86% 5580/6492 [01:29<00:14, 62.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  86% 5600/6492 [01:30<00:14, 62.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  87% 5620/6492 [01:30<00:13, 62.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  87% 5640/6492 [01:30<00:13, 62.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  87% 5660/6492 [01:30<00:13, 62.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  87% 5680/6492 [01:30<00:12, 62.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  88% 5700/6492 [01:30<00:12, 62.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  88% 5720/6492 [01:30<00:12, 62.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  88% 5740/6492 [01:31<00:11, 63.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  89% 5760/6492 [01:31<00:11, 63.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  89% 5780/6492 [01:31<00:11, 63.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  89% 5800/6492 [01:31<00:10, 63.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  90% 5820/6492 [01:31<00:10, 63.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  90% 5840/6492 [01:31<00:10, 63.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  90% 5860/6492 [01:31<00:09, 63.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  91% 5880/6492 [01:32<00:09, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  91% 5900/6492 [01:32<00:09, 63.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  91% 5920/6492 [01:32<00:08, 64.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  91% 5940/6492 [01:32<00:08, 64.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  92% 5960/6492 [01:32<00:08, 64.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  92% 5980/6492 [01:32<00:07, 64.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  92% 6000/6492 [01:33<00:07, 64.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  93% 6020/6492 [01:33<00:07, 64.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  93% 6040/6492 [01:33<00:06, 64.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  93% 6060/6492 [01:33<00:06, 64.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  94% 6080/6492 [01:33<00:06, 64.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  94% 6100/6492 [01:33<00:06, 65.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  94% 6120/6492 [01:33<00:05, 65.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  95% 6140/6492 [01:34<00:05, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  95% 6160/6492 [01:34<00:05, 65.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  95% 6180/6492 [01:34<00:04, 65.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  96% 6200/6492 [01:34<00:04, 65.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  96% 6220/6492 [01:34<00:04, 65.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  96% 6240/6492 [01:34<00:03, 65.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  96% 6260/6492 [01:35<00:03, 65.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  97% 6280/6492 [01:35<00:03, 65.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  97% 6300/6492 [01:35<00:02, 66.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  97% 6320/6492 [01:35<00:02, 66.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  98% 6340/6492 [01:35<00:02, 66.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  98% 6360/6492 [01:35<00:01, 66.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  98% 6380/6492 [01:35<00:01, 66.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  99% 6400/6492 [01:36<00:01, 66.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  99% 6420/6492 [01:36<00:01, 66.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54:  99% 6440/6492 [01:36<00:00, 66.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54: 100% 6460/6492 [01:36<00:00, 66.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54: 100% 6480/6492 [01:36<00:00, 66.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 54: 100% 6492/6492 [01:36<00:00, 67.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  80% 5180/6492 [01:21<00:20, 63.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 55:  80% 5200/6492 [01:26<00:21, 60.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  80% 5220/6492 [01:26<00:21, 60.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  81% 5240/6492 [01:26<00:20, 60.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  81% 5260/6492 [01:26<00:20, 60.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  81% 5280/6492 [01:26<00:19, 60.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  82% 5300/6492 [01:27<00:19, 60.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  82% 5320/6492 [01:27<00:19, 60.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  82% 5340/6492 [01:27<00:18, 61.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  83% 5360/6492 [01:27<00:18, 61.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  83% 5380/6492 [01:27<00:18, 61.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  83% 5400/6492 [01:27<00:17, 61.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  83% 5420/6492 [01:28<00:17, 61.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  84% 5440/6492 [01:28<00:17, 61.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  84% 5460/6492 [01:28<00:16, 61.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  84% 5480/6492 [01:28<00:16, 61.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  85% 5500/6492 [01:28<00:15, 62.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  85% 5520/6492 [01:28<00:15, 62.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  85% 5540/6492 [01:28<00:15, 62.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  86% 5560/6492 [01:29<00:14, 62.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  86% 5580/6492 [01:29<00:14, 62.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  86% 5600/6492 [01:29<00:14, 62.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  87% 5620/6492 [01:29<00:13, 62.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  87% 5640/6492 [01:29<00:13, 62.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  87% 5660/6492 [01:29<00:13, 63.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  87% 5680/6492 [01:29<00:12, 63.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  88% 5700/6492 [01:30<00:12, 63.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  88% 5720/6492 [01:30<00:12, 63.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  88% 5740/6492 [01:30<00:11, 63.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  89% 5760/6492 [01:30<00:11, 63.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  89% 5780/6492 [01:30<00:11, 63.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  89% 5800/6492 [01:30<00:10, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  90% 5820/6492 [01:31<00:10, 63.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  90% 5840/6492 [01:31<00:10, 64.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  90% 5860/6492 [01:31<00:09, 64.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  91% 5880/6492 [01:31<00:09, 64.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  91% 5900/6492 [01:31<00:09, 64.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  91% 5920/6492 [01:31<00:08, 64.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  91% 5940/6492 [01:31<00:08, 64.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  92% 5960/6492 [01:32<00:08, 64.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  92% 5980/6492 [01:32<00:07, 64.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  92% 6000/6492 [01:32<00:07, 64.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  93% 6020/6492 [01:32<00:07, 65.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  93% 6040/6492 [01:32<00:06, 65.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  93% 6060/6492 [01:32<00:06, 65.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  94% 6080/6492 [01:32<00:06, 65.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  94% 6100/6492 [01:33<00:05, 65.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  94% 6120/6492 [01:33<00:05, 65.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  95% 6140/6492 [01:33<00:05, 65.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  95% 6160/6492 [01:33<00:05, 65.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  95% 6180/6492 [01:33<00:04, 65.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  96% 6200/6492 [01:33<00:04, 66.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  96% 6220/6492 [01:33<00:04, 66.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  96% 6240/6492 [01:34<00:03, 66.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  96% 6260/6492 [01:34<00:03, 66.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  97% 6280/6492 [01:34<00:03, 66.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  97% 6300/6492 [01:34<00:02, 66.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  97% 6320/6492 [01:34<00:02, 66.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  98% 6340/6492 [01:34<00:02, 66.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  98% 6360/6492 [01:35<00:01, 66.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  98% 6380/6492 [01:35<00:01, 67.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  99% 6400/6492 [01:35<00:01, 67.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  99% 6420/6492 [01:35<00:01, 67.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55:  99% 6440/6492 [01:35<00:00, 67.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55: 100% 6460/6492 [01:35<00:00, 67.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55: 100% 6480/6492 [01:35<00:00, 67.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 55: 100% 6492/6492 [01:36<00:00, 67.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.370\n",
            "Epoch 56:  80% 5180/6492 [01:22<00:20, 63.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 56:  80% 5200/6492 [01:27<00:21, 59.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  80% 5220/6492 [01:27<00:21, 59.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  81% 5240/6492 [01:27<00:20, 59.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  81% 5260/6492 [01:27<00:20, 60.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  81% 5280/6492 [01:27<00:20, 60.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  82% 5300/6492 [01:27<00:19, 60.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  82% 5320/6492 [01:27<00:19, 60.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  82% 5340/6492 [01:28<00:19, 60.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  83% 5360/6492 [01:28<00:18, 60.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  83% 5380/6492 [01:28<00:18, 60.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  83% 5400/6492 [01:28<00:17, 60.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  83% 5420/6492 [01:28<00:17, 61.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  84% 5440/6492 [01:28<00:17, 61.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  84% 5460/6492 [01:29<00:16, 61.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  84% 5480/6492 [01:29<00:16, 61.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  85% 5500/6492 [01:29<00:16, 61.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  85% 5520/6492 [01:29<00:15, 61.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  85% 5540/6492 [01:29<00:15, 61.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  86% 5560/6492 [01:29<00:15, 61.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  86% 5580/6492 [01:29<00:14, 62.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  86% 5600/6492 [01:30<00:14, 62.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  87% 5620/6492 [01:30<00:14, 62.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  87% 5640/6492 [01:30<00:13, 62.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  87% 5660/6492 [01:30<00:13, 62.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  87% 5680/6492 [01:30<00:12, 62.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  88% 5700/6492 [01:30<00:12, 62.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  88% 5720/6492 [01:31<00:12, 62.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  88% 5740/6492 [01:31<00:11, 62.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  89% 5760/6492 [01:31<00:11, 63.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  89% 5780/6492 [01:31<00:11, 63.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  89% 5800/6492 [01:31<00:10, 63.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  90% 5820/6492 [01:31<00:10, 63.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  90% 5840/6492 [01:32<00:10, 63.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  90% 5860/6492 [01:32<00:09, 63.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  91% 5880/6492 [01:32<00:09, 63.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  91% 5900/6492 [01:32<00:09, 63.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  91% 5920/6492 [01:32<00:08, 63.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  91% 5940/6492 [01:32<00:08, 64.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  92% 5960/6492 [01:32<00:08, 64.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  92% 5980/6492 [01:33<00:07, 64.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  92% 6000/6492 [01:33<00:07, 64.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  93% 6020/6492 [01:33<00:07, 64.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  93% 6040/6492 [01:33<00:06, 64.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  93% 6060/6492 [01:33<00:06, 64.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  94% 6080/6492 [01:33<00:06, 64.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  94% 6100/6492 [01:33<00:06, 64.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  94% 6120/6492 [01:34<00:05, 65.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  95% 6140/6492 [01:34<00:05, 65.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  95% 6160/6492 [01:34<00:05, 65.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  95% 6180/6492 [01:34<00:04, 65.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  96% 6200/6492 [01:34<00:04, 65.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  96% 6220/6492 [01:34<00:04, 65.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  96% 6240/6492 [01:35<00:03, 65.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  96% 6260/6492 [01:35<00:03, 65.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  97% 6280/6492 [01:35<00:03, 65.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  97% 6300/6492 [01:35<00:02, 65.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  97% 6320/6492 [01:35<00:02, 66.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  98% 6340/6492 [01:35<00:02, 66.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  98% 6360/6492 [01:35<00:01, 66.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  98% 6380/6492 [01:36<00:01, 66.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  99% 6400/6492 [01:36<00:01, 66.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  99% 6420/6492 [01:36<00:01, 66.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56:  99% 6440/6492 [01:36<00:00, 66.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56: 100% 6460/6492 [01:36<00:00, 66.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56: 100% 6480/6492 [01:36<00:00, 66.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 56: 100% 6492/6492 [01:36<00:00, 66.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  80% 5180/6492 [01:21<00:20, 63.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 57:  80% 5200/6492 [01:26<00:21, 60.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  80% 5220/6492 [01:26<00:20, 60.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  81% 5240/6492 [01:26<00:20, 60.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  81% 5260/6492 [01:26<00:20, 60.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  81% 5280/6492 [01:26<00:19, 60.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  82% 5300/6492 [01:26<00:19, 61.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  82% 5320/6492 [01:26<00:19, 61.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  82% 5340/6492 [01:27<00:18, 61.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  83% 5360/6492 [01:27<00:18, 61.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  83% 5380/6492 [01:27<00:18, 61.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  83% 5400/6492 [01:27<00:17, 61.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  83% 5420/6492 [01:27<00:17, 61.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  84% 5440/6492 [01:27<00:16, 61.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  84% 5460/6492 [01:28<00:16, 62.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  84% 5480/6492 [01:28<00:16, 62.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  85% 5500/6492 [01:28<00:15, 62.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  85% 5520/6492 [01:28<00:15, 62.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  85% 5540/6492 [01:28<00:15, 62.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  86% 5560/6492 [01:28<00:14, 62.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  86% 5580/6492 [01:28<00:14, 62.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  86% 5600/6492 [01:29<00:14, 62.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  87% 5620/6492 [01:29<00:13, 62.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  87% 5640/6492 [01:29<00:13, 63.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  87% 5660/6492 [01:29<00:13, 63.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  87% 5680/6492 [01:29<00:12, 63.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  88% 5700/6492 [01:29<00:12, 63.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  88% 5720/6492 [01:30<00:12, 63.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  88% 5740/6492 [01:30<00:11, 63.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  89% 5760/6492 [01:30<00:11, 63.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  89% 5780/6492 [01:30<00:11, 63.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  89% 5800/6492 [01:30<00:10, 63.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  90% 5820/6492 [01:30<00:10, 64.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  90% 5840/6492 [01:30<00:10, 64.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  90% 5860/6492 [01:31<00:09, 64.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  91% 5880/6492 [01:31<00:09, 64.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  91% 5900/6492 [01:31<00:09, 64.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  91% 5920/6492 [01:31<00:08, 64.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  91% 5940/6492 [01:31<00:08, 64.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  92% 5960/6492 [01:31<00:08, 64.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  92% 5980/6492 [01:32<00:07, 64.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  92% 6000/6492 [01:32<00:07, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  93% 6020/6492 [01:32<00:07, 65.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  93% 6040/6492 [01:32<00:06, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  93% 6060/6492 [01:32<00:06, 65.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  94% 6080/6492 [01:32<00:06, 65.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  94% 6100/6492 [01:33<00:05, 65.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  94% 6120/6492 [01:33<00:05, 65.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  95% 6140/6492 [01:33<00:05, 65.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  95% 6160/6492 [01:33<00:05, 65.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  95% 6180/6492 [01:33<00:04, 66.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  96% 6200/6492 [01:33<00:04, 66.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  96% 6220/6492 [01:33<00:04, 66.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  96% 6240/6492 [01:34<00:03, 66.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  96% 6260/6492 [01:34<00:03, 66.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  97% 6280/6492 [01:34<00:03, 66.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  97% 6300/6492 [01:34<00:02, 66.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  97% 6320/6492 [01:34<00:02, 66.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  98% 6340/6492 [01:34<00:02, 66.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  98% 6360/6492 [01:34<00:01, 66.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  98% 6380/6492 [01:35<00:01, 67.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  99% 6400/6492 [01:35<00:01, 67.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  99% 6420/6492 [01:35<00:01, 67.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57:  99% 6440/6492 [01:35<00:00, 67.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57: 100% 6460/6492 [01:35<00:00, 67.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57: 100% 6480/6492 [01:35<00:00, 67.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 57: 100% 6492/6492 [01:36<00:00, 67.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  80% 5180/6492 [01:20<00:20, 64.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 58:  80% 5200/6492 [01:25<00:21, 60.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  80% 5220/6492 [01:25<00:20, 60.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  81% 5240/6492 [01:25<00:20, 61.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  81% 5260/6492 [01:26<00:20, 61.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  81% 5280/6492 [01:26<00:19, 61.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  82% 5300/6492 [01:26<00:19, 61.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  82% 5320/6492 [01:26<00:19, 61.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  82% 5340/6492 [01:26<00:18, 61.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  83% 5360/6492 [01:26<00:18, 61.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  83% 5380/6492 [01:26<00:17, 61.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  83% 5400/6492 [01:27<00:17, 61.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  83% 5420/6492 [01:27<00:17, 62.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  84% 5440/6492 [01:27<00:16, 62.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  84% 5460/6492 [01:27<00:16, 62.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  84% 5480/6492 [01:27<00:16, 62.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  85% 5500/6492 [01:27<00:15, 62.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  85% 5520/6492 [01:28<00:15, 62.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  85% 5540/6492 [01:28<00:15, 62.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  86% 5560/6492 [01:28<00:14, 62.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  86% 5580/6492 [01:28<00:14, 63.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  86% 5600/6492 [01:28<00:14, 63.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  87% 5620/6492 [01:28<00:13, 63.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  87% 5640/6492 [01:28<00:13, 63.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  87% 5660/6492 [01:29<00:13, 63.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  87% 5680/6492 [01:29<00:12, 63.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  88% 5700/6492 [01:29<00:12, 63.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  88% 5720/6492 [01:29<00:12, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  88% 5740/6492 [01:29<00:11, 63.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  89% 5760/6492 [01:29<00:11, 64.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  89% 5780/6492 [01:30<00:11, 64.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  89% 5800/6492 [01:30<00:10, 64.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  90% 5820/6492 [01:30<00:10, 64.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  90% 5840/6492 [01:30<00:10, 64.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  90% 5860/6492 [01:30<00:09, 64.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  91% 5880/6492 [01:30<00:09, 64.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  91% 5900/6492 [01:31<00:09, 64.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  91% 5920/6492 [01:31<00:08, 64.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  91% 5940/6492 [01:31<00:08, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  92% 5960/6492 [01:31<00:08, 65.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  92% 5980/6492 [01:31<00:07, 65.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  92% 6000/6492 [01:31<00:07, 65.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  93% 6020/6492 [01:31<00:07, 65.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  93% 6040/6492 [01:32<00:06, 65.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  93% 6060/6492 [01:32<00:06, 65.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  94% 6080/6492 [01:32<00:06, 65.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  94% 6100/6492 [01:32<00:05, 65.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  94% 6120/6492 [01:32<00:05, 66.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  95% 6140/6492 [01:32<00:05, 66.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  95% 6160/6492 [01:32<00:05, 66.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  95% 6180/6492 [01:33<00:04, 66.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  96% 6200/6492 [01:33<00:04, 66.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  96% 6220/6492 [01:33<00:04, 66.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  96% 6240/6492 [01:33<00:03, 66.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  96% 6260/6492 [01:33<00:03, 66.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  97% 6280/6492 [01:33<00:03, 66.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  97% 6300/6492 [01:34<00:02, 67.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  97% 6320/6492 [01:34<00:02, 67.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  98% 6340/6492 [01:34<00:02, 67.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  98% 6360/6492 [01:34<00:01, 67.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  98% 6380/6492 [01:34<00:01, 67.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  99% 6400/6492 [01:34<00:01, 67.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  99% 6420/6492 [01:34<00:01, 67.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58:  99% 6440/6492 [01:35<00:00, 67.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58: 100% 6460/6492 [01:35<00:00, 67.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58: 100% 6480/6492 [01:35<00:00, 67.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 58: 100% 6492/6492 [01:35<00:00, 67.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  80% 5180/6492 [01:20<00:20, 64.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 59:  80% 5200/6492 [01:25<00:21, 60.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  80% 5220/6492 [01:25<00:20, 60.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  81% 5240/6492 [01:26<00:20, 60.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  81% 5260/6492 [01:26<00:20, 60.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  81% 5280/6492 [01:26<00:19, 61.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  82% 5300/6492 [01:26<00:19, 61.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  82% 5320/6492 [01:26<00:19, 61.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  82% 5340/6492 [01:26<00:18, 61.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  83% 5360/6492 [01:27<00:18, 61.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  83% 5380/6492 [01:27<00:18, 61.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  83% 5400/6492 [01:27<00:17, 61.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  83% 5420/6492 [01:27<00:17, 61.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  84% 5440/6492 [01:27<00:16, 62.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  84% 5460/6492 [01:27<00:16, 62.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  84% 5480/6492 [01:28<00:16, 62.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  85% 5500/6492 [01:28<00:15, 62.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  85% 5520/6492 [01:28<00:15, 62.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  85% 5540/6492 [01:28<00:15, 62.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  86% 5560/6492 [01:28<00:14, 62.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  86% 5580/6492 [01:28<00:14, 62.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  86% 5600/6492 [01:28<00:14, 62.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  87% 5620/6492 [01:29<00:13, 63.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  87% 5640/6492 [01:29<00:13, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  87% 5660/6492 [01:29<00:13, 63.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  87% 5680/6492 [01:29<00:12, 63.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  88% 5700/6492 [01:29<00:12, 63.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  88% 5720/6492 [01:29<00:12, 63.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  88% 5740/6492 [01:29<00:11, 63.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  89% 5760/6492 [01:30<00:11, 63.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  89% 5780/6492 [01:30<00:11, 64.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  89% 5800/6492 [01:30<00:10, 64.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  90% 5820/6492 [01:30<00:10, 64.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  90% 5840/6492 [01:30<00:10, 64.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  90% 5860/6492 [01:30<00:09, 64.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  91% 5880/6492 [01:31<00:09, 64.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  91% 5900/6492 [01:31<00:09, 64.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  91% 5920/6492 [01:31<00:08, 64.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  91% 5940/6492 [01:31<00:08, 64.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  92% 5960/6492 [01:31<00:08, 65.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  92% 5980/6492 [01:31<00:07, 65.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  92% 6000/6492 [01:31<00:07, 65.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  93% 6020/6492 [01:32<00:07, 65.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  93% 6040/6492 [01:32<00:06, 65.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  93% 6060/6492 [01:32<00:06, 65.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  94% 6080/6492 [01:32<00:06, 65.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  94% 6100/6492 [01:32<00:05, 65.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  94% 6120/6492 [01:32<00:05, 65.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  95% 6140/6492 [01:33<00:05, 66.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  95% 6160/6492 [01:33<00:05, 66.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  95% 6180/6492 [01:33<00:04, 66.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  96% 6200/6492 [01:33<00:04, 66.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  96% 6220/6492 [01:33<00:04, 66.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  96% 6240/6492 [01:33<00:03, 66.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  96% 6260/6492 [01:33<00:03, 66.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  97% 6280/6492 [01:34<00:03, 66.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  97% 6300/6492 [01:34<00:02, 66.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  97% 6320/6492 [01:34<00:02, 66.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  98% 6340/6492 [01:34<00:02, 67.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  98% 6360/6492 [01:34<00:01, 67.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  98% 6380/6492 [01:34<00:01, 67.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  99% 6400/6492 [01:34<00:01, 67.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  99% 6420/6492 [01:35<00:01, 67.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59:  99% 6440/6492 [01:35<00:00, 67.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59: 100% 6460/6492 [01:35<00:00, 67.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59: 100% 6480/6492 [01:35<00:00, 67.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 59: 100% 6492/6492 [01:35<00:00, 67.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  80% 5180/6492 [01:20<00:20, 64.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 60:  80% 5200/6492 [01:25<00:21, 60.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  80% 5220/6492 [01:25<00:20, 60.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  81% 5240/6492 [01:26<00:20, 60.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  81% 5260/6492 [01:26<00:20, 61.02it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  81% 5280/6492 [01:26<00:19, 61.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  82% 5300/6492 [01:26<00:19, 61.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  82% 5320/6492 [01:26<00:19, 61.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  82% 5340/6492 [01:26<00:18, 61.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  83% 5360/6492 [01:26<00:18, 61.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  83% 5380/6492 [01:27<00:18, 61.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  83% 5400/6492 [01:27<00:17, 61.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  83% 5420/6492 [01:27<00:17, 62.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  84% 5440/6492 [01:27<00:16, 62.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  84% 5460/6492 [01:27<00:16, 62.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  84% 5480/6492 [01:27<00:16, 62.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  85% 5500/6492 [01:28<00:15, 62.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  85% 5520/6492 [01:28<00:15, 62.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  85% 5540/6492 [01:28<00:15, 62.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  86% 5560/6492 [01:28<00:14, 62.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  86% 5580/6492 [01:28<00:14, 62.97it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  86% 5600/6492 [01:28<00:14, 63.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  87% 5620/6492 [01:28<00:13, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  87% 5640/6492 [01:29<00:13, 63.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  87% 5660/6492 [01:29<00:13, 63.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  87% 5680/6492 [01:29<00:12, 63.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  88% 5700/6492 [01:29<00:12, 63.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  88% 5720/6492 [01:29<00:12, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  88% 5740/6492 [01:29<00:11, 63.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  89% 5760/6492 [01:29<00:11, 64.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  89% 5780/6492 [01:30<00:11, 64.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  89% 5800/6492 [01:30<00:10, 64.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  90% 5820/6492 [01:30<00:10, 64.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  90% 5840/6492 [01:30<00:10, 64.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  90% 5860/6492 [01:30<00:09, 64.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  91% 5880/6492 [01:30<00:09, 64.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  91% 5900/6492 [01:30<00:09, 64.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  91% 5920/6492 [01:31<00:08, 64.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  91% 5940/6492 [01:31<00:08, 65.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  92% 5960/6492 [01:31<00:08, 65.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  92% 5980/6492 [01:31<00:07, 65.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  92% 6000/6492 [01:31<00:07, 65.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  93% 6020/6492 [01:31<00:07, 65.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  93% 6040/6492 [01:31<00:06, 65.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  93% 6060/6492 [01:32<00:06, 65.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  94% 6080/6492 [01:32<00:06, 65.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  94% 6100/6492 [01:32<00:05, 65.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  94% 6120/6492 [01:32<00:05, 66.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  95% 6140/6492 [01:32<00:05, 66.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  95% 6160/6492 [01:32<00:05, 66.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  95% 6180/6492 [01:33<00:04, 66.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  96% 6200/6492 [01:33<00:04, 66.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  96% 6220/6492 [01:33<00:04, 66.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  96% 6240/6492 [01:33<00:03, 66.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  96% 6260/6492 [01:33<00:03, 66.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  97% 6280/6492 [01:33<00:03, 66.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  97% 6300/6492 [01:33<00:02, 67.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  97% 6320/6492 [01:34<00:02, 67.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  98% 6340/6492 [01:34<00:02, 67.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  98% 6360/6492 [01:34<00:01, 67.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  98% 6380/6492 [01:34<00:01, 67.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  99% 6400/6492 [01:34<00:01, 67.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  99% 6420/6492 [01:34<00:01, 67.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60:  99% 6440/6492 [01:34<00:00, 67.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60: 100% 6460/6492 [01:35<00:00, 67.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60: 100% 6480/6492 [01:35<00:00, 68.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 60: 100% 6492/6492 [01:35<00:00, 68.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  80% 5180/6492 [01:20<00:20, 63.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 61:  80% 5200/6492 [01:25<00:21, 60.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  80% 5220/6492 [01:26<00:20, 60.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  81% 5240/6492 [01:26<00:20, 60.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  81% 5260/6492 [01:26<00:20, 60.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  81% 5280/6492 [01:26<00:19, 61.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  82% 5300/6492 [01:26<00:19, 61.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  82% 5320/6492 [01:26<00:19, 61.30it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  82% 5340/6492 [01:26<00:18, 61.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  83% 5360/6492 [01:27<00:18, 61.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  83% 5380/6492 [01:27<00:18, 61.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  83% 5400/6492 [01:27<00:17, 61.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  83% 5420/6492 [01:27<00:17, 61.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  84% 5440/6492 [01:27<00:16, 62.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  84% 5460/6492 [01:27<00:16, 62.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  84% 5480/6492 [01:28<00:16, 62.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  85% 5500/6492 [01:28<00:15, 62.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  85% 5520/6492 [01:28<00:15, 62.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  85% 5540/6492 [01:28<00:15, 62.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  86% 5560/6492 [01:28<00:14, 62.69it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  86% 5580/6492 [01:28<00:14, 62.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  86% 5600/6492 [01:28<00:14, 62.92it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  87% 5620/6492 [01:29<00:13, 63.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  87% 5640/6492 [01:29<00:13, 63.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  87% 5660/6492 [01:29<00:13, 63.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  87% 5680/6492 [01:29<00:12, 63.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  88% 5700/6492 [01:29<00:12, 63.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  88% 5720/6492 [01:29<00:12, 63.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  88% 5740/6492 [01:30<00:11, 63.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  89% 5760/6492 [01:30<00:11, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  89% 5780/6492 [01:30<00:11, 63.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  89% 5800/6492 [01:30<00:10, 64.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  90% 5820/6492 [01:30<00:10, 64.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  90% 5840/6492 [01:30<00:10, 64.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  90% 5860/6492 [01:31<00:09, 64.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  91% 5880/6492 [01:31<00:09, 64.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  91% 5900/6492 [01:31<00:09, 64.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  91% 5920/6492 [01:31<00:08, 64.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  91% 5940/6492 [01:31<00:08, 64.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  92% 5960/6492 [01:31<00:08, 64.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  92% 5980/6492 [01:31<00:07, 65.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  92% 6000/6492 [01:32<00:07, 65.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  93% 6020/6492 [01:32<00:07, 65.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  93% 6040/6492 [01:32<00:06, 65.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  93% 6060/6492 [01:32<00:06, 65.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  94% 6080/6492 [01:32<00:06, 65.56it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  94% 6100/6492 [01:32<00:05, 65.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  94% 6120/6492 [01:33<00:05, 65.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  95% 6140/6492 [01:33<00:05, 65.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  95% 6160/6492 [01:33<00:05, 65.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  95% 6180/6492 [01:33<00:04, 66.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  96% 6200/6492 [01:33<00:04, 66.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  96% 6220/6492 [01:33<00:04, 66.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  96% 6240/6492 [01:34<00:03, 66.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  96% 6260/6492 [01:34<00:03, 66.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  97% 6280/6492 [01:34<00:03, 66.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  97% 6300/6492 [01:34<00:02, 66.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  97% 6320/6492 [01:34<00:02, 66.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  98% 6340/6492 [01:34<00:02, 66.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  98% 6360/6492 [01:34<00:01, 66.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  98% 6380/6492 [01:35<00:01, 67.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  99% 6400/6492 [01:35<00:01, 67.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  99% 6420/6492 [01:35<00:01, 67.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61:  99% 6440/6492 [01:35<00:00, 67.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61: 100% 6460/6492 [01:35<00:00, 67.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61: 100% 6480/6492 [01:35<00:00, 67.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 61: 100% 6492/6492 [01:36<00:00, 67.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  80% 5180/6492 [01:21<00:20, 63.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 62:  80% 5200/6492 [01:26<00:21, 60.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  80% 5220/6492 [01:26<00:21, 60.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  81% 5240/6492 [01:26<00:20, 60.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  81% 5260/6492 [01:26<00:20, 60.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  81% 5280/6492 [01:27<00:19, 60.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  82% 5300/6492 [01:27<00:19, 60.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  82% 5320/6492 [01:27<00:19, 60.90it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  82% 5340/6492 [01:27<00:18, 61.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  83% 5360/6492 [01:27<00:18, 61.13it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  83% 5380/6492 [01:27<00:18, 61.24it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  83% 5400/6492 [01:28<00:17, 61.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  83% 5420/6492 [01:28<00:17, 61.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  84% 5440/6492 [01:28<00:17, 61.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  84% 5460/6492 [01:28<00:16, 61.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  84% 5480/6492 [01:28<00:16, 61.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  85% 5500/6492 [01:28<00:16, 61.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  85% 5520/6492 [01:28<00:15, 62.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  85% 5540/6492 [01:29<00:15, 62.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  86% 5560/6492 [01:29<00:14, 62.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  86% 5580/6492 [01:29<00:14, 62.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  86% 5600/6492 [01:29<00:14, 62.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  87% 5620/6492 [01:29<00:13, 62.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  87% 5640/6492 [01:29<00:13, 62.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  87% 5660/6492 [01:30<00:13, 62.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  87% 5680/6492 [01:30<00:12, 62.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  88% 5700/6492 [01:30<00:12, 63.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  88% 5720/6492 [01:30<00:12, 63.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  88% 5740/6492 [01:30<00:11, 63.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  89% 5760/6492 [01:30<00:11, 63.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  89% 5780/6492 [01:30<00:11, 63.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  89% 5800/6492 [01:31<00:10, 63.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  90% 5820/6492 [01:31<00:10, 63.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  90% 5840/6492 [01:31<00:10, 63.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  90% 5860/6492 [01:31<00:09, 64.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  91% 5880/6492 [01:31<00:09, 64.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  91% 5900/6492 [01:31<00:09, 64.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  91% 5920/6492 [01:32<00:08, 64.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  91% 5940/6492 [01:32<00:08, 64.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  92% 5960/6492 [01:32<00:08, 64.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  92% 5980/6492 [01:32<00:07, 64.68it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  92% 6000/6492 [01:32<00:07, 64.80it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  93% 6020/6492 [01:32<00:07, 64.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  93% 6040/6492 [01:32<00:06, 65.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  93% 6060/6492 [01:33<00:06, 65.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  94% 6080/6492 [01:33<00:06, 65.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  94% 6100/6492 [01:33<00:05, 65.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  94% 6120/6492 [01:33<00:05, 65.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  95% 6140/6492 [01:33<00:05, 65.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  95% 6160/6492 [01:33<00:05, 65.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  95% 6180/6492 [01:33<00:04, 65.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  96% 6200/6492 [01:34<00:04, 65.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  96% 6220/6492 [01:34<00:04, 65.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  96% 6240/6492 [01:34<00:03, 66.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  96% 6260/6492 [01:34<00:03, 66.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  97% 6280/6492 [01:34<00:03, 66.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  97% 6300/6492 [01:34<00:02, 66.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  97% 6320/6492 [01:34<00:02, 66.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  98% 6340/6492 [01:35<00:02, 66.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  98% 6360/6492 [01:35<00:01, 66.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  98% 6380/6492 [01:35<00:01, 66.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  99% 6400/6492 [01:35<00:01, 66.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  99% 6420/6492 [01:35<00:01, 67.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62:  99% 6440/6492 [01:35<00:00, 67.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62: 100% 6460/6492 [01:36<00:00, 67.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62: 100% 6480/6492 [01:36<00:00, 67.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 62: 100% 6492/6492 [01:36<00:00, 67.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  80% 5180/6492 [01:21<00:20, 63.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 63:  80% 5200/6492 [01:26<00:21, 60.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  80% 5220/6492 [01:26<00:21, 60.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  81% 5240/6492 [01:26<00:20, 60.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  81% 5260/6492 [01:26<00:20, 60.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  81% 5280/6492 [01:26<00:19, 60.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  82% 5300/6492 [01:27<00:19, 60.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  82% 5320/6492 [01:27<00:19, 60.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  82% 5340/6492 [01:27<00:18, 61.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  83% 5360/6492 [01:27<00:18, 61.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  83% 5380/6492 [01:27<00:18, 61.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  83% 5400/6492 [01:27<00:17, 61.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  83% 5420/6492 [01:28<00:17, 61.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  84% 5440/6492 [01:28<00:17, 61.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  84% 5460/6492 [01:28<00:16, 61.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  84% 5480/6492 [01:28<00:16, 61.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  85% 5500/6492 [01:28<00:15, 62.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  85% 5520/6492 [01:28<00:15, 62.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  85% 5540/6492 [01:28<00:15, 62.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  86% 5560/6492 [01:29<00:14, 62.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  86% 5580/6492 [01:29<00:14, 62.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  86% 5600/6492 [01:29<00:14, 62.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  87% 5620/6492 [01:29<00:13, 62.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  87% 5640/6492 [01:29<00:13, 62.87it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  87% 5660/6492 [01:29<00:13, 62.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  87% 5680/6492 [01:30<00:12, 63.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  88% 5700/6492 [01:30<00:12, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  88% 5720/6492 [01:30<00:12, 63.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  88% 5740/6492 [01:30<00:11, 63.42it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  89% 5760/6492 [01:30<00:11, 63.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  89% 5780/6492 [01:30<00:11, 63.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  89% 5800/6492 [01:30<00:10, 63.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  90% 5820/6492 [01:31<00:10, 63.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  90% 5840/6492 [01:31<00:10, 63.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  90% 5860/6492 [01:31<00:09, 64.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  91% 5880/6492 [01:31<00:09, 64.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  91% 5900/6492 [01:31<00:09, 64.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  91% 5920/6492 [01:31<00:08, 64.38it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  91% 5940/6492 [01:32<00:08, 64.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  92% 5960/6492 [01:32<00:08, 64.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  92% 5980/6492 [01:32<00:07, 64.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  92% 6000/6492 [01:32<00:07, 64.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  93% 6020/6492 [01:32<00:07, 64.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  93% 6040/6492 [01:32<00:06, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  93% 6060/6492 [01:32<00:06, 65.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  94% 6080/6492 [01:33<00:06, 65.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  94% 6100/6492 [01:33<00:05, 65.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  94% 6120/6492 [01:33<00:05, 65.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  95% 6140/6492 [01:33<00:05, 65.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  95% 6160/6492 [01:33<00:05, 65.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  95% 6180/6492 [01:34<00:04, 65.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  96% 6200/6492 [01:34<00:04, 65.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  96% 6220/6492 [01:34<00:04, 65.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  96% 6240/6492 [01:34<00:03, 66.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  96% 6260/6492 [01:34<00:03, 66.18it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  97% 6280/6492 [01:34<00:03, 66.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  97% 6300/6492 [01:34<00:02, 66.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  97% 6320/6492 [01:35<00:02, 66.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  98% 6340/6492 [01:35<00:02, 66.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  98% 6360/6492 [01:35<00:01, 66.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  98% 6380/6492 [01:35<00:01, 66.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  99% 6400/6492 [01:35<00:01, 66.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  99% 6420/6492 [01:35<00:01, 67.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63:  99% 6440/6492 [01:35<00:00, 67.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63: 100% 6460/6492 [01:36<00:00, 67.25it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63: 100% 6480/6492 [01:36<00:00, 67.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 63: 100% 6492/6492 [01:36<00:00, 67.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  80% 5180/6492 [01:21<00:20, 63.76it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 64:  80% 5200/6492 [01:26<00:21, 60.36it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  80% 5220/6492 [01:26<00:21, 60.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  81% 5240/6492 [01:26<00:20, 60.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  81% 5260/6492 [01:26<00:20, 60.74it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  81% 5280/6492 [01:26<00:19, 60.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  82% 5300/6492 [01:26<00:19, 60.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  82% 5320/6492 [01:27<00:19, 61.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  82% 5340/6492 [01:27<00:18, 61.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  83% 5360/6492 [01:27<00:18, 61.35it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  83% 5380/6492 [01:27<00:18, 61.48it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  83% 5400/6492 [01:27<00:17, 61.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  83% 5420/6492 [01:27<00:17, 61.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  84% 5440/6492 [01:27<00:17, 61.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  84% 5460/6492 [01:28<00:16, 61.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  84% 5480/6492 [01:28<00:16, 62.07it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  85% 5500/6492 [01:28<00:15, 62.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  85% 5520/6492 [01:28<00:15, 62.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  85% 5540/6492 [01:28<00:15, 62.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  86% 5560/6492 [01:28<00:14, 62.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  86% 5580/6492 [01:29<00:14, 62.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  86% 5600/6492 [01:29<00:14, 62.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  87% 5620/6492 [01:29<00:13, 62.89it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  87% 5640/6492 [01:29<00:13, 63.00it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  87% 5660/6492 [01:29<00:13, 63.11it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  87% 5680/6492 [01:29<00:12, 63.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  88% 5700/6492 [01:29<00:12, 63.34it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  88% 5720/6492 [01:30<00:12, 63.46it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  88% 5740/6492 [01:30<00:11, 63.58it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  89% 5760/6492 [01:30<00:11, 63.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  89% 5780/6492 [01:30<00:11, 63.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  89% 5800/6492 [01:30<00:10, 63.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  90% 5820/6492 [01:30<00:10, 64.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  90% 5840/6492 [01:31<00:10, 64.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  90% 5860/6492 [01:31<00:09, 64.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  91% 5880/6492 [01:31<00:09, 64.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  91% 5900/6492 [01:31<00:09, 64.51it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  91% 5920/6492 [01:31<00:08, 64.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  91% 5940/6492 [01:31<00:08, 64.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  92% 5960/6492 [01:31<00:08, 64.85it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  92% 5980/6492 [01:32<00:07, 64.95it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  92% 6000/6492 [01:32<00:07, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  93% 6020/6492 [01:32<00:07, 65.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  93% 6040/6492 [01:32<00:06, 65.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  93% 6060/6492 [01:32<00:06, 65.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  94% 6080/6492 [01:32<00:06, 65.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  94% 6100/6492 [01:32<00:05, 65.60it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  94% 6120/6492 [01:33<00:05, 65.70it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  95% 6140/6492 [01:33<00:05, 65.81it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  95% 6160/6492 [01:33<00:05, 65.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  95% 6180/6492 [01:33<00:04, 66.01it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  96% 6200/6492 [01:33<00:04, 66.12it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  96% 6220/6492 [01:33<00:04, 66.22it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  96% 6240/6492 [01:34<00:03, 66.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  96% 6260/6492 [01:34<00:03, 66.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  97% 6280/6492 [01:34<00:03, 66.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  97% 6300/6492 [01:34<00:02, 66.63it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  97% 6320/6492 [01:34<00:02, 66.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  98% 6340/6492 [01:34<00:02, 66.82it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  98% 6360/6492 [01:35<00:01, 66.93it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  98% 6380/6492 [01:35<00:01, 67.04it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  99% 6400/6492 [01:35<00:01, 67.14it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  99% 6420/6492 [01:35<00:01, 67.23it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64:  99% 6440/6492 [01:35<00:00, 67.33it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64: 100% 6460/6492 [01:35<00:00, 67.43it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64: 100% 6480/6492 [01:35<00:00, 67.53it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 64: 100% 6492/6492 [01:36<00:00, 67.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  80% 5180/6492 [01:21<00:20, 63.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1299 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 65:  80% 5200/6492 [01:26<00:21, 60.45it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  80% 5220/6492 [01:26<00:20, 60.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  81% 5240/6492 [01:26<00:20, 60.71it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  81% 5260/6492 [01:26<00:20, 60.84it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  81% 5280/6492 [01:26<00:19, 60.96it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  82% 5300/6492 [01:26<00:19, 61.08it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  82% 5320/6492 [01:26<00:19, 61.20it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  82% 5340/6492 [01:27<00:18, 61.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  83% 5360/6492 [01:27<00:18, 61.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  83% 5380/6492 [01:27<00:18, 61.55it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  83% 5400/6492 [01:27<00:17, 61.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  83% 5420/6492 [01:27<00:17, 61.79it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  84% 5440/6492 [01:27<00:16, 61.91it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  84% 5460/6492 [01:28<00:16, 62.03it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  84% 5480/6492 [01:28<00:16, 62.15it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  85% 5500/6492 [01:28<00:15, 62.27it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  85% 5520/6492 [01:28<00:15, 62.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  85% 5540/6492 [01:28<00:15, 62.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  86% 5560/6492 [01:28<00:14, 62.64it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  86% 5580/6492 [01:28<00:14, 62.75it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  86% 5600/6492 [01:29<00:14, 62.86it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  87% 5620/6492 [01:29<00:13, 62.98it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  87% 5640/6492 [01:29<00:13, 63.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  87% 5660/6492 [01:29<00:13, 63.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  87% 5680/6492 [01:29<00:12, 63.32it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  88% 5700/6492 [01:29<00:12, 63.44it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  88% 5720/6492 [01:30<00:12, 63.54it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  88% 5740/6492 [01:30<00:11, 63.66it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  89% 5760/6492 [01:30<00:11, 63.78it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  89% 5780/6492 [01:30<00:11, 63.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  89% 5800/6492 [01:30<00:10, 63.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  90% 5820/6492 [01:30<00:10, 64.10it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  90% 5840/6492 [01:30<00:10, 64.21it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  90% 5860/6492 [01:31<00:09, 64.31it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  91% 5880/6492 [01:31<00:09, 64.41it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  91% 5900/6492 [01:31<00:09, 64.52it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  91% 5920/6492 [01:31<00:08, 64.62it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  91% 5940/6492 [01:31<00:08, 64.73it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  92% 5960/6492 [01:31<00:08, 64.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  92% 5980/6492 [01:32<00:07, 64.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  92% 6000/6492 [01:32<00:07, 65.06it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  93% 6020/6492 [01:32<00:07, 65.17it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  93% 6040/6492 [01:32<00:06, 65.28it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  93% 6060/6492 [01:32<00:06, 65.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  94% 6080/6492 [01:32<00:06, 65.50it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  94% 6100/6492 [01:32<00:05, 65.61it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  94% 6120/6492 [01:33<00:05, 65.72it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  95% 6140/6492 [01:33<00:05, 65.83it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  95% 6160/6492 [01:33<00:05, 65.94it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  95% 6180/6492 [01:33<00:04, 66.05it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  96% 6200/6492 [01:33<00:04, 66.16it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  96% 6220/6492 [01:33<00:04, 66.26it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  96% 6240/6492 [01:34<00:03, 66.37it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  96% 6260/6492 [01:34<00:03, 66.47it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  97% 6280/6492 [01:34<00:03, 66.57it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  97% 6300/6492 [01:34<00:02, 66.67it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  97% 6320/6492 [01:34<00:02, 66.77it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  98% 6340/6492 [01:34<00:02, 66.88it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  98% 6360/6492 [01:34<00:01, 66.99it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  98% 6380/6492 [01:35<00:01, 67.09it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  99% 6400/6492 [01:35<00:01, 67.19it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  99% 6420/6492 [01:35<00:01, 67.29it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65:  99% 6440/6492 [01:35<00:00, 67.39it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65: 100% 6460/6492 [01:35<00:00, 67.49it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65: 100% 6480/6492 [01:35<00:00, 67.59it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "Epoch 65: 100% 6492/6492 [01:35<00:00, 67.65it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "                                                                 \u001b[AMonitored metric avg_val_loss did not improve in the last 10 records. Best score: 4.370. Signaling Trainer to stop.\n",
            "Epoch 65: 100% 6492/6492 [01:36<00:00, 67.40it/s, loss=4.36, v_num=35, val_loss=4.370, avg_val_loss=4.370, train_loss=4.360]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 153\n",
            "Shape of the feature representation generated by the base encoder: (41093, 64)\n",
            "end time: 1728377781.305515\n",
            "Execution time: 1.78 hours\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_41093_2000_norm_log.h5ad\" \\\n",
        "                                              --batch_size 270 \\\n",
        "                                              --size_factor 0.4 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 100\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/feat_1006_bs260_sf04_larry_hotspot\" \\\n",
        "                                              --train_test 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
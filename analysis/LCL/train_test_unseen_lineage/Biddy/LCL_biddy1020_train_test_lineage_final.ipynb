{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "bb1b25e3-da71-4d14-f3e2-3693985461c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgFzoXTPCVub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14ace3a-2b31-4009-f632-51de807f070c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "dfbf0712-c734-4221-ed58-42eea1194bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.8 pytorch-lightning-2.4.0 torchmetrics-1.5.0\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
            "Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: array-api-compat, anndata\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.26.4)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.5.0)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.8)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.19.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.17.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (3.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.2.0)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.4.0\n",
            "    Uninstalling pytorch-lightning-2.4.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.4.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.10.9)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.1)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.5)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.9)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=04a4e7a236ab17d4734100649eb91be11890b561eda4a348ed6bf85601be60e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, session-info, pynndescent, umap-learn, scanpy\n",
            "Successfully installed legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "46329a8f-5d8e-496d-91c7-7d954c1a29c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1729470852.035727\n",
            "usage: scContrastiveLearning_Main_709_ckpt_epoch.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                                                    [--batch_size BATCH_SIZE]\n",
            "                                                    [--size_factor SIZE_FACTOR]\n",
            "                                                    [--temperature TEMPERATURE]\n",
            "                                                    [--patience PATIENCE] [--min_delta MIN_DELTA]\n",
            "                                                    [--max_epoch MAX_EPOCH] --output_dir\n",
            "                                                    OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                                                    [--hidden_dims HIDDEN_DIMS]\n",
            "                                                    [--embedding_size EMBEDDING_SIZE]\n",
            "                                                    [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V30WCiJDs9I",
        "outputId": "d2f79644-3991-4f82-cfa7-2e8792fff4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 2:  93% 31740/34255 [04:40<00:22, 112.99it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31760/34255 [04:40<00:22, 113.03it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31780/34255 [04:41<00:21, 113.07it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31800/34255 [04:41<00:21, 113.10it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31820/34255 [04:41<00:21, 113.14it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31840/34255 [04:41<00:21, 113.18it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31860/34255 [04:41<00:21, 113.21it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31880/34255 [04:41<00:20, 113.25it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31900/34255 [04:41<00:20, 113.28it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31920/34255 [04:41<00:20, 113.32it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31940/34255 [04:41<00:20, 113.35it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31960/34255 [04:41<00:20, 113.39it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 31980/34255 [04:41<00:20, 113.43it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 32000/34255 [04:42<00:19, 113.46it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  93% 32020/34255 [04:42<00:19, 113.50it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32040/34255 [04:42<00:19, 113.54it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32060/34255 [04:42<00:19, 113.57it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32080/34255 [04:42<00:19, 113.61it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32100/34255 [04:42<00:18, 113.64it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32120/34255 [04:42<00:18, 113.68it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32140/34255 [04:42<00:18, 113.71it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32160/34255 [04:42<00:18, 113.75it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32180/34255 [04:42<00:18, 113.78it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32200/34255 [04:42<00:18, 113.82it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32220/34255 [04:43<00:17, 113.85it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32240/34255 [04:43<00:17, 113.88it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32260/34255 [04:43<00:17, 113.92it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32280/34255 [04:43<00:17, 113.95it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32300/34255 [04:43<00:17, 113.98it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32320/34255 [04:43<00:16, 114.01it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32340/34255 [04:43<00:16, 114.05it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  94% 32360/34255 [04:43<00:16, 114.08it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32380/34255 [04:43<00:16, 114.11it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32400/34255 [04:43<00:16, 114.15it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32420/34255 [04:43<00:16, 114.18it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32440/34255 [04:44<00:15, 114.22it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32460/34255 [04:44<00:15, 114.25it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32480/34255 [04:44<00:15, 114.28it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32500/34255 [04:44<00:15, 114.32it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32520/34255 [04:44<00:15, 114.35it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32540/34255 [04:44<00:14, 114.39it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32560/34255 [04:44<00:14, 114.42it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32580/34255 [04:44<00:14, 114.46it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32600/34255 [04:44<00:14, 114.49it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32620/34255 [04:44<00:14, 114.52it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32640/34255 [04:44<00:14, 114.56it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32660/34255 [04:45<00:13, 114.59it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32680/34255 [04:45<00:13, 114.62it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  95% 32700/34255 [04:45<00:13, 114.66it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32720/34255 [04:45<00:13, 114.70it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32740/34255 [04:45<00:13, 114.73it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32760/34255 [04:45<00:13, 114.77it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32780/34255 [04:45<00:12, 114.81it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32800/34255 [04:45<00:12, 114.85it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32820/34255 [04:45<00:12, 114.88it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32840/34255 [04:45<00:12, 114.91it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32860/34255 [04:45<00:12, 114.95it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32880/34255 [04:45<00:11, 114.98it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32900/34255 [04:46<00:11, 115.02it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32920/34255 [04:46<00:11, 115.05it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32940/34255 [04:46<00:11, 115.09it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32960/34255 [04:46<00:11, 115.12it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 32980/34255 [04:46<00:11, 115.16it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 33000/34255 [04:46<00:10, 115.19it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 33020/34255 [04:46<00:10, 115.23it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  96% 33040/34255 [04:46<00:10, 115.26it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33060/34255 [04:46<00:10, 115.30it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33080/34255 [04:46<00:10, 115.33it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33100/34255 [04:46<00:10, 115.37it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33120/34255 [04:46<00:09, 115.40it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33140/34255 [04:47<00:09, 115.44it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33160/34255 [04:47<00:09, 115.47it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33180/34255 [04:47<00:09, 115.51it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33200/34255 [04:47<00:09, 115.55it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33220/34255 [04:47<00:08, 115.58it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33240/34255 [04:47<00:08, 115.62it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33260/34255 [04:47<00:08, 115.65it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33280/34255 [04:47<00:08, 115.69it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33300/34255 [04:47<00:08, 115.73it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33320/34255 [04:47<00:08, 115.76it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33340/34255 [04:47<00:07, 115.80it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33360/34255 [04:47<00:07, 115.84it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  97% 33380/34255 [04:48<00:07, 115.87it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33400/34255 [04:48<00:07, 115.90it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33420/34255 [04:48<00:07, 115.94it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33440/34255 [04:48<00:07, 115.97it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33460/34255 [04:48<00:06, 116.01it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33480/34255 [04:48<00:06, 116.04it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33500/34255 [04:48<00:06, 116.07it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33520/34255 [04:48<00:06, 116.11it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33540/34255 [04:48<00:06, 116.14it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33560/34255 [04:48<00:05, 116.17it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33580/34255 [04:48<00:05, 116.21it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33600/34255 [04:49<00:05, 116.24it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33620/34255 [04:49<00:05, 116.28it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33640/34255 [04:49<00:05, 116.31it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33660/34255 [04:49<00:05, 116.35it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33680/34255 [04:49<00:04, 116.38it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33700/34255 [04:49<00:04, 116.41it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33720/34255 [04:49<00:04, 116.45it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  98% 33740/34255 [04:49<00:04, 116.48it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33760/34255 [04:49<00:04, 116.52it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33780/34255 [04:49<00:04, 116.55it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33800/34255 [04:49<00:03, 116.59it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33820/34255 [04:49<00:03, 116.62it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33840/34255 [04:50<00:03, 116.66it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33860/34255 [04:50<00:03, 116.69it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33880/34255 [04:50<00:03, 116.72it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33900/34255 [04:50<00:03, 116.76it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33920/34255 [04:50<00:02, 116.79it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33940/34255 [04:50<00:02, 116.82it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33960/34255 [04:50<00:02, 116.86it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 33980/34255 [04:50<00:02, 116.89it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 34000/34255 [04:50<00:02, 116.92it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 34020/34255 [04:50<00:02, 116.96it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 34040/34255 [04:50<00:01, 116.99it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 34060/34255 [04:51<00:01, 117.03it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2:  99% 34080/34255 [04:51<00:01, 117.06it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34100/34255 [04:51<00:01, 117.10it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34120/34255 [04:51<00:01, 117.13it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34140/34255 [04:51<00:00, 117.16it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34160/34255 [04:51<00:00, 117.20it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34180/34255 [04:51<00:00, 117.23it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34200/34255 [04:51<00:00, 117.26it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34220/34255 [04:51<00:00, 117.29it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Epoch 2: 100% 34240/34255 [04:51<00:00, 117.33it/s, loss=2.66, v_num=53, val_loss=2.730, avg_val_loss=2.730, train_loss=2.820]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 233.31it/s]\u001b[A\n",
            "Epoch 2: 100% 34255/34255 [04:51<00:00, 117.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.820]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.033 >= min_delta = 0.001. New best score: 2.695\n",
            "Epoch 3:  80% 27400/34255 [04:18<01:04, 106.01it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 27420/34255 [04:22<01:05, 104.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27440/34255 [04:22<01:05, 104.36it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27460/34255 [04:23<01:05, 104.41it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27480/34255 [04:23<01:04, 104.45it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27500/34255 [04:23<01:04, 104.49it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27520/34255 [04:23<01:04, 104.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27540/34255 [04:23<01:04, 104.57it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  80% 27560/34255 [04:23<01:03, 104.61it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27580/34255 [04:23<01:03, 104.65it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27600/34255 [04:23<01:03, 104.70it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27620/34255 [04:23<01:03, 104.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27640/34255 [04:23<01:03, 104.78it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27660/34255 [04:23<01:02, 104.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27680/34255 [04:23<01:02, 104.86it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27700/34255 [04:24<01:02, 104.90it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27720/34255 [04:24<01:02, 104.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27740/34255 [04:24<01:02, 104.98it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27760/34255 [04:24<01:01, 105.02it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27780/34255 [04:24<01:01, 105.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27800/34255 [04:24<01:01, 105.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27820/34255 [04:24<01:01, 105.15it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27840/34255 [04:24<01:00, 105.19it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27860/34255 [04:24<01:00, 105.24it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27880/34255 [04:24<01:00, 105.28it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  81% 27900/34255 [04:24<01:00, 105.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 27920/34255 [04:24<01:00, 105.36it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 27940/34255 [04:25<00:59, 105.40it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 27960/34255 [04:25<00:59, 105.44it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 27980/34255 [04:25<00:59, 105.48it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28000/34255 [04:25<00:59, 105.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28020/34255 [04:25<00:59, 105.57it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28040/34255 [04:25<00:58, 105.61it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28060/34255 [04:25<00:58, 105.65it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28080/34255 [04:25<00:58, 105.69it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28100/34255 [04:25<00:58, 105.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28120/34255 [04:25<00:57, 105.78it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28140/34255 [04:25<00:57, 105.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28160/34255 [04:26<00:57, 105.86it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28180/34255 [04:26<00:57, 105.90it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28200/34255 [04:26<00:57, 105.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28220/34255 [04:26<00:56, 105.98it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28240/34255 [04:26<00:56, 106.02it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  82% 28260/34255 [04:26<00:56, 106.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28280/34255 [04:26<00:56, 106.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28300/34255 [04:26<00:56, 106.15it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28320/34255 [04:26<00:55, 106.19it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28340/34255 [04:26<00:55, 106.23it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28360/34255 [04:26<00:55, 106.27it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28380/34255 [04:26<00:55, 106.31it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28400/34255 [04:27<00:55, 106.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28420/34255 [04:27<00:54, 106.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28440/34255 [04:27<00:54, 106.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28460/34255 [04:27<00:54, 106.47it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28480/34255 [04:27<00:54, 106.51it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28500/34255 [04:27<00:54, 106.55it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28520/34255 [04:27<00:53, 106.59it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28540/34255 [04:27<00:53, 106.63it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28560/34255 [04:27<00:53, 106.67it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28580/34255 [04:27<00:53, 106.71it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  83% 28600/34255 [04:27<00:52, 106.75it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28620/34255 [04:28<00:52, 106.79it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28640/34255 [04:28<00:52, 106.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28660/34255 [04:28<00:52, 106.86it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28680/34255 [04:28<00:52, 106.90it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28700/34255 [04:28<00:51, 106.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28720/34255 [04:28<00:51, 106.98it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28740/34255 [04:28<00:51, 107.01it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28760/34255 [04:28<00:51, 107.05it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28780/34255 [04:28<00:51, 107.09it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28800/34255 [04:28<00:50, 107.13it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28820/34255 [04:28<00:50, 107.17it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28840/34255 [04:29<00:50, 107.21it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28860/34255 [04:29<00:50, 107.25it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28880/34255 [04:29<00:50, 107.29it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28900/34255 [04:29<00:49, 107.33it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28920/34255 [04:29<00:49, 107.37it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  84% 28940/34255 [04:29<00:49, 107.41it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 28960/34255 [04:29<00:49, 107.45it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 28980/34255 [04:29<00:49, 107.49it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29000/34255 [04:29<00:48, 107.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29020/34255 [04:29<00:48, 107.58it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29040/34255 [04:29<00:48, 107.62it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29060/34255 [04:29<00:48, 107.66it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29080/34255 [04:30<00:48, 107.70it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29100/34255 [04:30<00:47, 107.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29120/34255 [04:30<00:47, 107.78it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29140/34255 [04:30<00:47, 107.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29160/34255 [04:30<00:47, 107.86it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29180/34255 [04:30<00:47, 107.90it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29200/34255 [04:30<00:46, 107.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29220/34255 [04:30<00:46, 107.98it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29240/34255 [04:30<00:46, 108.02it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29260/34255 [04:30<00:46, 108.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  85% 29280/34255 [04:30<00:46, 108.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29300/34255 [04:30<00:45, 108.15it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29320/34255 [04:31<00:45, 108.19it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29340/34255 [04:31<00:45, 108.23it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29360/34255 [04:31<00:45, 108.27it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29380/34255 [04:31<00:45, 108.31it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29400/34255 [04:31<00:44, 108.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29420/34255 [04:31<00:44, 108.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29440/34255 [04:31<00:44, 108.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29460/34255 [04:31<00:44, 108.47it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29480/34255 [04:31<00:44, 108.51it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29500/34255 [04:31<00:43, 108.55it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29520/34255 [04:31<00:43, 108.59it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29540/34255 [04:31<00:43, 108.64it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29560/34255 [04:32<00:43, 108.68it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29580/34255 [04:32<00:43, 108.72it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29600/34255 [04:32<00:42, 108.76it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  86% 29620/34255 [04:32<00:42, 108.79it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29640/34255 [04:32<00:42, 108.83it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29660/34255 [04:32<00:42, 108.87it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29680/34255 [04:32<00:42, 108.91it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29700/34255 [04:32<00:41, 108.95it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29720/34255 [04:32<00:41, 108.99it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29740/34255 [04:32<00:41, 109.03it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29760/34255 [04:32<00:41, 109.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29780/34255 [04:32<00:41, 109.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29800/34255 [04:33<00:40, 109.15it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29820/34255 [04:33<00:40, 109.19it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29840/34255 [04:33<00:40, 109.23it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29860/34255 [04:33<00:40, 109.27it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29880/34255 [04:33<00:40, 109.31it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29900/34255 [04:33<00:39, 109.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29920/34255 [04:33<00:39, 109.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29940/34255 [04:33<00:39, 109.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  87% 29960/34255 [04:33<00:39, 109.46it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 29980/34255 [04:33<00:39, 109.50it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30000/34255 [04:33<00:38, 109.54it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30020/34255 [04:33<00:38, 109.58it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30040/34255 [04:34<00:38, 109.62it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30060/34255 [04:34<00:38, 109.66it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30080/34255 [04:34<00:38, 109.69it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30100/34255 [04:34<00:37, 109.73it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30120/34255 [04:34<00:37, 109.77it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30140/34255 [04:34<00:37, 109.81it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30160/34255 [04:34<00:37, 109.85it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30180/34255 [04:34<00:37, 109.89it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30200/34255 [04:34<00:36, 109.92it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30220/34255 [04:34<00:36, 109.96it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30240/34255 [04:34<00:36, 110.00it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30260/34255 [04:34<00:36, 110.04it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30280/34255 [04:35<00:36, 110.08it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  88% 30300/34255 [04:35<00:35, 110.12it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30320/34255 [04:35<00:35, 110.16it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30340/34255 [04:35<00:35, 110.20it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30360/34255 [04:35<00:35, 110.24it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30380/34255 [04:35<00:35, 110.28it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30400/34255 [04:35<00:34, 110.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30420/34255 [04:35<00:34, 110.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30440/34255 [04:35<00:34, 110.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30460/34255 [04:35<00:34, 110.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30480/34255 [04:35<00:34, 110.47it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30500/34255 [04:36<00:33, 110.50it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30520/34255 [04:36<00:33, 110.54it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30540/34255 [04:36<00:33, 110.58it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30560/34255 [04:36<00:33, 110.62it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30580/34255 [04:36<00:33, 110.65it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30600/34255 [04:36<00:33, 110.69it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30620/34255 [04:36<00:32, 110.73it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  89% 30640/34255 [04:36<00:32, 110.77it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30660/34255 [04:36<00:32, 110.81it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30680/34255 [04:36<00:32, 110.84it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30700/34255 [04:36<00:32, 110.88it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30720/34255 [04:36<00:31, 110.92it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30740/34255 [04:37<00:31, 110.96it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30760/34255 [04:37<00:31, 110.99it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30780/34255 [04:37<00:31, 111.03it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30800/34255 [04:37<00:31, 111.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30820/34255 [04:37<00:30, 111.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30840/34255 [04:37<00:30, 111.15it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30860/34255 [04:37<00:30, 111.18it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30880/34255 [04:37<00:30, 111.22it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30900/34255 [04:37<00:30, 111.26it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30920/34255 [04:37<00:29, 111.30it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30940/34255 [04:37<00:29, 111.33it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30960/34255 [04:37<00:29, 111.37it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 30980/34255 [04:38<00:29, 111.40it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  90% 31000/34255 [04:38<00:29, 111.44it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31020/34255 [04:38<00:29, 111.48it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31040/34255 [04:38<00:28, 111.51it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31060/34255 [04:38<00:28, 111.55it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31080/34255 [04:38<00:28, 111.59it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31100/34255 [04:38<00:28, 111.62it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31120/34255 [04:38<00:28, 111.66it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31140/34255 [04:38<00:27, 111.69it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31160/34255 [04:38<00:27, 111.73it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31180/34255 [04:38<00:27, 111.77it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31200/34255 [04:39<00:27, 111.80it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31220/34255 [04:39<00:27, 111.84it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31240/34255 [04:39<00:26, 111.88it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31260/34255 [04:39<00:26, 111.91it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31280/34255 [04:39<00:26, 111.95it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31300/34255 [04:39<00:26, 111.99it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31320/34255 [04:39<00:26, 112.02it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  91% 31340/34255 [04:39<00:26, 112.06it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31360/34255 [04:39<00:25, 112.10it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31380/34255 [04:39<00:25, 112.13it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31400/34255 [04:39<00:25, 112.17it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31420/34255 [04:40<00:25, 112.20it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31440/34255 [04:40<00:25, 112.24it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31460/34255 [04:40<00:24, 112.27it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31480/34255 [04:40<00:24, 112.31it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31500/34255 [04:40<00:24, 112.34it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31520/34255 [04:40<00:24, 112.38it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31540/34255 [04:40<00:24, 112.41it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31560/34255 [04:40<00:23, 112.45it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31580/34255 [04:40<00:23, 112.49it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31600/34255 [04:40<00:23, 112.52it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31620/34255 [04:40<00:23, 112.56it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31640/34255 [04:41<00:23, 112.59it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31660/34255 [04:41<00:23, 112.63it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  92% 31680/34255 [04:41<00:22, 112.66it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31700/34255 [04:41<00:22, 112.70it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31720/34255 [04:41<00:22, 112.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31740/34255 [04:41<00:22, 112.77it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31760/34255 [04:41<00:22, 112.81it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31780/34255 [04:41<00:21, 112.84it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31800/34255 [04:41<00:21, 112.88it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31820/34255 [04:41<00:21, 112.92it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31840/34255 [04:41<00:21, 112.95it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31860/34255 [04:41<00:21, 112.99it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31880/34255 [04:42<00:21, 113.02it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31900/34255 [04:42<00:20, 113.06it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31920/34255 [04:42<00:20, 113.10it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31940/34255 [04:42<00:20, 113.14it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31960/34255 [04:42<00:20, 113.18it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 31980/34255 [04:42<00:20, 113.21it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 32000/34255 [04:42<00:19, 113.25it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  93% 32020/34255 [04:42<00:19, 113.29it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32040/34255 [04:42<00:19, 113.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32060/34255 [04:42<00:19, 113.36it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32080/34255 [04:42<00:19, 113.40it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32100/34255 [04:42<00:18, 113.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32120/34255 [04:43<00:18, 113.47it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32140/34255 [04:43<00:18, 113.51it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32160/34255 [04:43<00:18, 113.54it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32180/34255 [04:43<00:18, 113.58it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32200/34255 [04:43<00:18, 113.61it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32220/34255 [04:43<00:17, 113.65it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32240/34255 [04:43<00:17, 113.68it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32260/34255 [04:43<00:17, 113.72it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32280/34255 [04:43<00:17, 113.75it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32300/34255 [04:43<00:17, 113.79it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32320/34255 [04:43<00:16, 113.83it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32340/34255 [04:44<00:16, 113.86it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  94% 32360/34255 [04:44<00:16, 113.90it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32380/34255 [04:44<00:16, 113.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32400/34255 [04:44<00:16, 113.97it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32420/34255 [04:44<00:16, 114.01it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32440/34255 [04:44<00:15, 114.05it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32460/34255 [04:44<00:15, 114.09it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32480/34255 [04:44<00:15, 114.12it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32500/34255 [04:44<00:15, 114.16it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32520/34255 [04:44<00:15, 114.20it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32540/34255 [04:44<00:15, 114.24it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32560/34255 [04:44<00:14, 114.27it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32580/34255 [04:45<00:14, 114.31it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32600/34255 [04:45<00:14, 114.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32620/34255 [04:45<00:14, 114.38it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32640/34255 [04:45<00:14, 114.42it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32660/34255 [04:45<00:13, 114.45it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32680/34255 [04:45<00:13, 114.49it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  95% 32700/34255 [04:45<00:13, 114.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32720/34255 [04:45<00:13, 114.56it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32740/34255 [04:45<00:13, 114.60it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32760/34255 [04:45<00:13, 114.64it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32780/34255 [04:45<00:12, 114.67it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32800/34255 [04:45<00:12, 114.71it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32820/34255 [04:46<00:12, 114.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32840/34255 [04:46<00:12, 114.78it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32860/34255 [04:46<00:12, 114.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32880/34255 [04:46<00:11, 114.85it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32900/34255 [04:46<00:11, 114.89it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32920/34255 [04:46<00:11, 114.92it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32940/34255 [04:46<00:11, 114.96it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32960/34255 [04:46<00:11, 115.00it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 32980/34255 [04:46<00:11, 115.03it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 33000/34255 [04:46<00:10, 115.07it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 33020/34255 [04:46<00:10, 115.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  96% 33040/34255 [04:46<00:10, 115.14it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33060/34255 [04:47<00:10, 115.18it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33080/34255 [04:47<00:10, 115.21it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33100/34255 [04:47<00:10, 115.25it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33120/34255 [04:47<00:09, 115.29it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33140/34255 [04:47<00:09, 115.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33160/34255 [04:47<00:09, 115.36it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33180/34255 [04:47<00:09, 115.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33200/34255 [04:47<00:09, 115.43it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33220/34255 [04:47<00:08, 115.46it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33240/34255 [04:47<00:08, 115.50it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33260/34255 [04:47<00:08, 115.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33280/34255 [04:47<00:08, 115.57it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33300/34255 [04:48<00:08, 115.61it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33320/34255 [04:48<00:08, 115.64it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33340/34255 [04:48<00:07, 115.68it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33360/34255 [04:48<00:07, 115.71it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  97% 33380/34255 [04:48<00:07, 115.75it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33400/34255 [04:48<00:07, 115.78it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33420/34255 [04:48<00:07, 115.82it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33440/34255 [04:48<00:07, 115.85it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33460/34255 [04:48<00:06, 115.89it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33480/34255 [04:48<00:06, 115.92it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33500/34255 [04:48<00:06, 115.96it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33520/34255 [04:48<00:06, 116.00it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33540/34255 [04:49<00:06, 116.03it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33560/34255 [04:49<00:05, 116.06it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33580/34255 [04:49<00:05, 116.10it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33600/34255 [04:49<00:05, 116.13it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33620/34255 [04:49<00:05, 116.17it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33640/34255 [04:49<00:05, 116.21it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33660/34255 [04:49<00:05, 116.24it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33680/34255 [04:49<00:04, 116.28it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33700/34255 [04:49<00:04, 116.32it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33720/34255 [04:49<00:04, 116.35it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  98% 33740/34255 [04:49<00:04, 116.39it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33760/34255 [04:49<00:04, 116.42it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33780/34255 [04:50<00:04, 116.46it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33800/34255 [04:50<00:03, 116.49it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33820/34255 [04:50<00:03, 116.53it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33840/34255 [04:50<00:03, 116.56it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33860/34255 [04:50<00:03, 116.60it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33880/34255 [04:50<00:03, 116.63it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33900/34255 [04:50<00:03, 116.67it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33920/34255 [04:50<00:02, 116.70it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33940/34255 [04:50<00:02, 116.74it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33960/34255 [04:50<00:02, 116.77it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 33980/34255 [04:50<00:02, 116.81it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 34000/34255 [04:50<00:02, 116.84it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 34020/34255 [04:51<00:02, 116.87it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 34040/34255 [04:51<00:01, 116.91it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 34060/34255 [04:51<00:01, 116.94it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3:  99% 34080/34255 [04:51<00:01, 116.98it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34100/34255 [04:51<00:01, 117.01it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34120/34255 [04:51<00:01, 117.04it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34140/34255 [04:51<00:00, 117.08it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34160/34255 [04:51<00:00, 117.11it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34180/34255 [04:51<00:00, 117.14it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34200/34255 [04:51<00:00, 117.17it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34220/34255 [04:51<00:00, 117.20it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Epoch 3: 100% 34240/34255 [04:52<00:00, 117.23it/s, loss=2.66, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.670]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 233.24it/s]\u001b[A\n",
            "Epoch 3: 100% 34255/34255 [04:52<00:00, 117.25it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.670]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 2.693\n",
            "Epoch 4:  80% 27400/34255 [04:20<01:05, 105.30it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 27420/34255 [04:24<01:05, 103.71it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27440/34255 [04:24<01:05, 103.75it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27460/34255 [04:24<01:05, 103.79it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27480/34255 [04:24<01:05, 103.83it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27500/34255 [04:24<01:05, 103.87it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27520/34255 [04:24<01:04, 103.91it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27540/34255 [04:24<01:04, 103.96it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  80% 27560/34255 [04:25<01:04, 104.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27580/34255 [04:25<01:04, 104.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27600/34255 [04:25<01:03, 104.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27620/34255 [04:25<01:03, 104.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27640/34255 [04:25<01:03, 104.16it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27660/34255 [04:25<01:03, 104.21it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27680/34255 [04:25<01:03, 104.25it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27700/34255 [04:25<01:02, 104.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27720/34255 [04:25<01:02, 104.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27740/34255 [04:25<01:02, 104.38it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27760/34255 [04:25<01:02, 104.42it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27780/34255 [04:25<01:01, 104.46it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27800/34255 [04:26<01:01, 104.50it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27820/34255 [04:26<01:01, 104.55it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27840/34255 [04:26<01:01, 104.59it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27860/34255 [04:26<01:01, 104.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27880/34255 [04:26<01:00, 104.67it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  81% 27900/34255 [04:26<01:00, 104.71it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 27920/34255 [04:26<01:00, 104.75it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 27940/34255 [04:26<01:00, 104.79it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 27960/34255 [04:26<01:00, 104.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 27980/34255 [04:26<00:59, 104.88it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28000/34255 [04:26<00:59, 104.92it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28020/34255 [04:26<00:59, 104.96it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28040/34255 [04:27<00:59, 105.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28060/34255 [04:27<00:58, 105.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28080/34255 [04:27<00:58, 105.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28100/34255 [04:27<00:58, 105.13it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28120/34255 [04:27<00:58, 105.17it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28140/34255 [04:27<00:58, 105.21it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28160/34255 [04:27<00:57, 105.25it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28180/34255 [04:27<00:57, 105.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28200/34255 [04:27<00:57, 105.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28220/34255 [04:27<00:57, 105.38it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28240/34255 [04:27<00:57, 105.42it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  82% 28260/34255 [04:27<00:56, 105.46it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28280/34255 [04:28<00:56, 105.50it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28300/34255 [04:28<00:56, 105.54it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28320/34255 [04:28<00:56, 105.58it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28340/34255 [04:28<00:56, 105.62it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28360/34255 [04:28<00:55, 105.66it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28380/34255 [04:28<00:55, 105.70it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28400/34255 [04:28<00:55, 105.74it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28420/34255 [04:28<00:55, 105.77it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28440/34255 [04:28<00:54, 105.82it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28460/34255 [04:28<00:54, 105.86it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28480/34255 [04:28<00:54, 105.90it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28500/34255 [04:29<00:54, 105.94it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28520/34255 [04:29<00:54, 105.98it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28540/34255 [04:29<00:53, 106.02it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28560/34255 [04:29<00:53, 106.06it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28580/34255 [04:29<00:53, 106.10it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  83% 28600/34255 [04:29<00:53, 106.14it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28620/34255 [04:29<00:53, 106.18it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28640/34255 [04:29<00:52, 106.22it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28660/34255 [04:29<00:52, 106.25it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28680/34255 [04:29<00:52, 106.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28700/34255 [04:29<00:52, 106.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28720/34255 [04:30<00:52, 106.37it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28740/34255 [04:30<00:51, 106.41it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28760/34255 [04:30<00:51, 106.45it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28780/34255 [04:30<00:51, 106.48it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28800/34255 [04:30<00:51, 106.52it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28820/34255 [04:30<00:51, 106.56it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28840/34255 [04:30<00:50, 106.60it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28860/34255 [04:30<00:50, 106.64it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28880/34255 [04:30<00:50, 106.68it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28900/34255 [04:30<00:50, 106.72it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28920/34255 [04:30<00:49, 106.76it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  84% 28940/34255 [04:30<00:49, 106.80it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 28960/34255 [04:31<00:49, 106.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 28980/34255 [04:31<00:49, 106.88it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29000/34255 [04:31<00:49, 106.92it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29020/34255 [04:31<00:48, 106.96it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29040/34255 [04:31<00:48, 107.01it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29060/34255 [04:31<00:48, 107.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29080/34255 [04:31<00:48, 107.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29100/34255 [04:31<00:48, 107.13it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29120/34255 [04:31<00:47, 107.17it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29140/34255 [04:31<00:47, 107.21it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29160/34255 [04:31<00:47, 107.25it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29180/34255 [04:31<00:47, 107.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29200/34255 [04:32<00:47, 107.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29220/34255 [04:32<00:46, 107.37it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29240/34255 [04:32<00:46, 107.41it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29260/34255 [04:32<00:46, 107.44it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  85% 29280/34255 [04:32<00:46, 107.48it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29300/34255 [04:32<00:46, 107.52it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29320/34255 [04:32<00:45, 107.56it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29340/34255 [04:32<00:45, 107.60it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29360/34255 [04:32<00:45, 107.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29380/34255 [04:32<00:45, 107.67it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29400/34255 [04:32<00:45, 107.71it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29420/34255 [04:33<00:44, 107.75it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29440/34255 [04:33<00:44, 107.79it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29460/34255 [04:33<00:44, 107.83it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29480/34255 [04:33<00:44, 107.87it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29500/34255 [04:33<00:44, 107.90it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29520/34255 [04:33<00:43, 107.94it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29540/34255 [04:33<00:43, 107.97it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29560/34255 [04:33<00:43, 108.01it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29580/34255 [04:33<00:43, 108.05it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29600/34255 [04:33<00:43, 108.09it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  86% 29620/34255 [04:33<00:42, 108.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29640/34255 [04:34<00:42, 108.16it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29660/34255 [04:34<00:42, 108.20it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29680/34255 [04:34<00:42, 108.23it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29700/34255 [04:34<00:42, 108.27it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29720/34255 [04:34<00:41, 108.31it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29740/34255 [04:34<00:41, 108.35it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29760/34255 [04:34<00:41, 108.38it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29780/34255 [04:34<00:41, 108.42it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29800/34255 [04:34<00:41, 108.46it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29820/34255 [04:34<00:40, 108.50it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29840/34255 [04:34<00:40, 108.54it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29860/34255 [04:35<00:40, 108.57it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29880/34255 [04:35<00:40, 108.61it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29900/34255 [04:35<00:40, 108.65it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29920/34255 [04:35<00:39, 108.69it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29940/34255 [04:35<00:39, 108.73it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  87% 29960/34255 [04:35<00:39, 108.76it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 29980/34255 [04:35<00:39, 108.80it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30000/34255 [04:35<00:39, 108.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30020/34255 [04:35<00:38, 108.88it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30040/34255 [04:35<00:38, 108.92it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30060/34255 [04:35<00:38, 108.96it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30080/34255 [04:35<00:38, 109.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30100/34255 [04:36<00:38, 109.03it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30120/34255 [04:36<00:37, 109.07it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30140/34255 [04:36<00:37, 109.11it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30160/34255 [04:36<00:37, 109.15it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30180/34255 [04:36<00:37, 109.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30200/34255 [04:36<00:37, 109.23it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30220/34255 [04:36<00:36, 109.27it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30240/34255 [04:36<00:36, 109.31it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30260/34255 [04:36<00:36, 109.35it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30280/34255 [04:36<00:36, 109.39it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  88% 30300/34255 [04:36<00:36, 109.43it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30320/34255 [04:36<00:35, 109.46it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30340/34255 [04:37<00:35, 109.50it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30360/34255 [04:37<00:35, 109.54it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30380/34255 [04:37<00:35, 109.58it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30400/34255 [04:37<00:35, 109.62it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30420/34255 [04:37<00:34, 109.65it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30440/34255 [04:37<00:34, 109.69it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30460/34255 [04:37<00:34, 109.73it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30480/34255 [04:37<00:34, 109.77it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30500/34255 [04:37<00:34, 109.81it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30520/34255 [04:37<00:34, 109.85it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30540/34255 [04:37<00:33, 109.89it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30560/34255 [04:37<00:33, 109.93it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30580/34255 [04:38<00:33, 109.97it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30600/34255 [04:38<00:33, 110.01it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30620/34255 [04:38<00:33, 110.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  89% 30640/34255 [04:38<00:32, 110.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30660/34255 [04:38<00:32, 110.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30680/34255 [04:38<00:32, 110.16it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30700/34255 [04:38<00:32, 110.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30720/34255 [04:38<00:32, 110.23it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30740/34255 [04:38<00:31, 110.27it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30760/34255 [04:38<00:31, 110.31it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30780/34255 [04:38<00:31, 110.35it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30800/34255 [04:39<00:31, 110.39it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30820/34255 [04:39<00:31, 110.43it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30840/34255 [04:39<00:30, 110.47it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30860/34255 [04:39<00:30, 110.51it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30880/34255 [04:39<00:30, 110.54it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30900/34255 [04:39<00:30, 110.58it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30920/34255 [04:39<00:30, 110.62it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30940/34255 [04:39<00:29, 110.66it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30960/34255 [04:39<00:29, 110.70it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 30980/34255 [04:39<00:29, 110.74it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  90% 31000/34255 [04:39<00:29, 110.78it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31020/34255 [04:39<00:29, 110.82it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31040/34255 [04:40<00:29, 110.85it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31060/34255 [04:40<00:28, 110.89it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31080/34255 [04:40<00:28, 110.93it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31100/34255 [04:40<00:28, 110.97it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31120/34255 [04:40<00:28, 111.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31140/34255 [04:40<00:28, 111.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31160/34255 [04:40<00:27, 111.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31180/34255 [04:40<00:27, 111.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31200/34255 [04:40<00:27, 111.15it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31220/34255 [04:40<00:27, 111.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31240/34255 [04:40<00:27, 111.22it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31260/34255 [04:40<00:26, 111.26it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31280/34255 [04:41<00:26, 111.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31300/34255 [04:41<00:26, 111.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31320/34255 [04:41<00:26, 111.36it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  91% 31340/34255 [04:41<00:26, 111.40it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31360/34255 [04:41<00:25, 111.44it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31380/34255 [04:41<00:25, 111.48it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31400/34255 [04:41<00:25, 111.52it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31420/34255 [04:41<00:25, 111.55it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31440/34255 [04:41<00:25, 111.59it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31460/34255 [04:41<00:25, 111.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31480/34255 [04:41<00:24, 111.66it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31500/34255 [04:42<00:24, 111.70it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31520/34255 [04:42<00:24, 111.73it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31540/34255 [04:42<00:24, 111.77it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31560/34255 [04:42<00:24, 111.80it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31580/34255 [04:42<00:23, 111.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31600/34255 [04:42<00:23, 111.87it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31620/34255 [04:42<00:23, 111.91it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31640/34255 [04:42<00:23, 111.94it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31660/34255 [04:42<00:23, 111.98it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  92% 31680/34255 [04:42<00:22, 112.01it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31700/34255 [04:42<00:22, 112.05it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31720/34255 [04:43<00:22, 112.08it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31740/34255 [04:43<00:22, 112.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31760/34255 [04:43<00:22, 112.14it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31780/34255 [04:43<00:22, 112.17it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31800/34255 [04:43<00:21, 112.21it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31820/34255 [04:43<00:21, 112.24it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31840/34255 [04:43<00:21, 112.28it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31860/34255 [04:43<00:21, 112.31it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31880/34255 [04:43<00:21, 112.35it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31900/34255 [04:43<00:20, 112.38it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31920/34255 [04:43<00:20, 112.42it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31940/34255 [04:44<00:20, 112.45it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31960/34255 [04:44<00:20, 112.49it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 31980/34255 [04:44<00:20, 112.53it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 32000/34255 [04:44<00:20, 112.56it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  93% 32020/34255 [04:44<00:19, 112.60it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32040/34255 [04:44<00:19, 112.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32060/34255 [04:44<00:19, 112.67it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32080/34255 [04:44<00:19, 112.71it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32100/34255 [04:44<00:19, 112.74it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32120/34255 [04:44<00:18, 112.78it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32140/34255 [04:44<00:18, 112.81it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32160/34255 [04:44<00:18, 112.85it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32180/34255 [04:45<00:18, 112.89it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32200/34255 [04:45<00:18, 112.93it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32220/34255 [04:45<00:18, 112.96it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32240/34255 [04:45<00:17, 113.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32260/34255 [04:45<00:17, 113.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32280/34255 [04:45<00:17, 113.07it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32300/34255 [04:45<00:17, 113.11it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32320/34255 [04:45<00:17, 113.15it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32340/34255 [04:45<00:16, 113.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  94% 32360/34255 [04:45<00:16, 113.22it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32380/34255 [04:45<00:16, 113.26it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32400/34255 [04:45<00:16, 113.30it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32420/34255 [04:46<00:16, 113.34it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32440/34255 [04:46<00:16, 113.37it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32460/34255 [04:46<00:15, 113.41it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32480/34255 [04:46<00:15, 113.44it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32500/34255 [04:46<00:15, 113.48it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32520/34255 [04:46<00:15, 113.52it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32540/34255 [04:46<00:15, 113.55it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32560/34255 [04:46<00:14, 113.59it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32580/34255 [04:46<00:14, 113.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32600/34255 [04:46<00:14, 113.66it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32620/34255 [04:46<00:14, 113.70it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32640/34255 [04:46<00:14, 113.73it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32660/34255 [04:47<00:14, 113.77it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32680/34255 [04:47<00:13, 113.80it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  95% 32700/34255 [04:47<00:13, 113.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32720/34255 [04:47<00:13, 113.87it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32740/34255 [04:47<00:13, 113.91it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32760/34255 [04:47<00:13, 113.94it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32780/34255 [04:47<00:12, 113.98it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32800/34255 [04:47<00:12, 114.02it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32820/34255 [04:47<00:12, 114.05it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32840/34255 [04:47<00:12, 114.09it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32860/34255 [04:47<00:12, 114.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32880/34255 [04:48<00:12, 114.16it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32900/34255 [04:48<00:11, 114.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32920/34255 [04:48<00:11, 114.23it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32940/34255 [04:48<00:11, 114.26it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32960/34255 [04:48<00:11, 114.30it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 32980/34255 [04:48<00:11, 114.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 33000/34255 [04:48<00:10, 114.37it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 33020/34255 [04:48<00:10, 114.40it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  96% 33040/34255 [04:48<00:10, 114.43it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33060/34255 [04:48<00:10, 114.47it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33080/34255 [04:48<00:10, 114.50it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33100/34255 [04:48<00:10, 114.54it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33120/34255 [04:49<00:09, 114.57it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33140/34255 [04:49<00:09, 114.61it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33160/34255 [04:49<00:09, 114.64it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33180/34255 [04:49<00:09, 114.68it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33200/34255 [04:49<00:09, 114.71it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33220/34255 [04:49<00:09, 114.74it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33240/34255 [04:49<00:08, 114.77it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33260/34255 [04:49<00:08, 114.81it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33280/34255 [04:49<00:08, 114.84it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33300/34255 [04:49<00:08, 114.88it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33320/34255 [04:49<00:08, 114.92it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33340/34255 [04:50<00:07, 114.95it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33360/34255 [04:50<00:07, 114.99it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  97% 33380/34255 [04:50<00:07, 115.02it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33400/34255 [04:50<00:07, 115.05it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33420/34255 [04:50<00:07, 115.09it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33440/34255 [04:50<00:07, 115.12it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33460/34255 [04:50<00:06, 115.16it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33480/34255 [04:50<00:06, 115.19it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33500/34255 [04:50<00:06, 115.22it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33520/34255 [04:50<00:06, 115.26it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33540/34255 [04:50<00:06, 115.29it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33560/34255 [04:51<00:06, 115.33it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33580/34255 [04:51<00:05, 115.36it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33600/34255 [04:51<00:05, 115.39it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33620/34255 [04:51<00:05, 115.43it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33640/34255 [04:51<00:05, 115.46it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33660/34255 [04:51<00:05, 115.49it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33680/34255 [04:51<00:04, 115.53it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33700/34255 [04:51<00:04, 115.56it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33720/34255 [04:51<00:04, 115.59it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  98% 33740/34255 [04:51<00:04, 115.63it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33760/34255 [04:51<00:04, 115.66it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33780/34255 [04:51<00:04, 115.70it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33800/34255 [04:52<00:03, 115.73it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33820/34255 [04:52<00:03, 115.76it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33840/34255 [04:52<00:03, 115.80it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33860/34255 [04:52<00:03, 115.83it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33880/34255 [04:52<00:03, 115.87it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33900/34255 [04:52<00:03, 115.90it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33920/34255 [04:52<00:02, 115.93it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33940/34255 [04:52<00:02, 115.97it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33960/34255 [04:52<00:02, 116.00it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 33980/34255 [04:52<00:02, 116.04it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 34000/34255 [04:52<00:02, 116.07it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 34020/34255 [04:53<00:02, 116.11it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 34040/34255 [04:53<00:01, 116.14it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 34060/34255 [04:53<00:01, 116.17it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4:  99% 34080/34255 [04:53<00:01, 116.21it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34100/34255 [04:53<00:01, 116.24it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34120/34255 [04:53<00:01, 116.27it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34140/34255 [04:53<00:00, 116.31it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34160/34255 [04:53<00:00, 116.34it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34180/34255 [04:53<00:00, 116.37it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34200/34255 [04:53<00:00, 116.41it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34220/34255 [04:53<00:00, 116.44it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 4: 100% 34240/34255 [04:53<00:00, 116.48it/s, loss=2.66, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 230.64it/s]\u001b[A\n",
            "Epoch 4: 100% 34255/34255 [04:54<00:00, 116.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27400/34255 [04:19<01:04, 105.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 27420/34255 [04:23<01:05, 104.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27440/34255 [04:23<01:05, 104.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27460/34255 [04:23<01:05, 104.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27480/34255 [04:23<01:04, 104.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27500/34255 [04:23<01:04, 104.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27520/34255 [04:23<01:04, 104.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27540/34255 [04:23<01:04, 104.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  80% 27560/34255 [04:23<01:04, 104.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27580/34255 [04:24<01:03, 104.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27600/34255 [04:24<01:03, 104.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27620/34255 [04:24<01:03, 104.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27640/34255 [04:24<01:03, 104.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27660/34255 [04:24<01:03, 104.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27680/34255 [04:24<01:02, 104.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27700/34255 [04:24<01:02, 104.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27720/34255 [04:24<01:02, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27740/34255 [04:24<01:02, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27760/34255 [04:24<01:01, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27780/34255 [04:24<01:01, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27800/34255 [04:25<01:01, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27820/34255 [04:25<01:01, 104.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27840/34255 [04:25<01:01, 104.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27860/34255 [04:25<01:00, 105.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27880/34255 [04:25<01:00, 105.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  81% 27900/34255 [04:25<01:00, 105.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 27920/34255 [04:25<01:00, 105.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 27940/34255 [04:25<01:00, 105.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 27960/34255 [04:25<00:59, 105.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 27980/34255 [04:25<00:59, 105.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28000/34255 [04:25<00:59, 105.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28020/34255 [04:25<00:59, 105.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28040/34255 [04:26<00:58, 105.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28060/34255 [04:26<00:58, 105.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28080/34255 [04:26<00:58, 105.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28100/34255 [04:26<00:58, 105.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28120/34255 [04:26<00:58, 105.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28140/34255 [04:26<00:57, 105.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28160/34255 [04:26<00:57, 105.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28180/34255 [04:26<00:57, 105.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28200/34255 [04:26<00:57, 105.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28220/34255 [04:26<00:57, 105.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28240/34255 [04:26<00:56, 105.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  82% 28260/34255 [04:27<00:56, 105.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28280/34255 [04:27<00:56, 105.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28300/34255 [04:27<00:56, 105.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28320/34255 [04:27<00:56, 105.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28340/34255 [04:27<00:55, 105.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28360/34255 [04:27<00:55, 106.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28380/34255 [04:27<00:55, 106.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28400/34255 [04:27<00:55, 106.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28420/34255 [04:27<00:54, 106.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28440/34255 [04:27<00:54, 106.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28460/34255 [04:27<00:54, 106.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28480/34255 [04:28<00:54, 106.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28500/34255 [04:28<00:54, 106.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28520/34255 [04:28<00:53, 106.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28540/34255 [04:28<00:53, 106.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28560/34255 [04:28<00:53, 106.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28580/34255 [04:28<00:53, 106.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  83% 28600/34255 [04:28<00:53, 106.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28620/34255 [04:28<00:52, 106.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28640/34255 [04:28<00:52, 106.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28660/34255 [04:28<00:52, 106.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28680/34255 [04:28<00:52, 106.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28700/34255 [04:29<00:52, 106.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28720/34255 [04:29<00:51, 106.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28740/34255 [04:29<00:51, 106.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28760/34255 [04:29<00:51, 106.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28780/34255 [04:29<00:51, 106.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28800/34255 [04:29<00:51, 106.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28820/34255 [04:29<00:50, 106.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28840/34255 [04:29<00:50, 106.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28860/34255 [04:29<00:50, 107.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28880/34255 [04:29<00:50, 107.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28900/34255 [04:29<00:50, 107.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28920/34255 [04:29<00:49, 107.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  84% 28940/34255 [04:30<00:49, 107.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 28960/34255 [04:30<00:49, 107.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 28980/34255 [04:30<00:49, 107.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29000/34255 [04:30<00:48, 107.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29020/34255 [04:30<00:48, 107.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29040/34255 [04:30<00:48, 107.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29060/34255 [04:30<00:48, 107.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29080/34255 [04:30<00:48, 107.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29100/34255 [04:30<00:47, 107.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29120/34255 [04:30<00:47, 107.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29140/34255 [04:30<00:47, 107.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29160/34255 [04:31<00:47, 107.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29180/34255 [04:31<00:47, 107.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29200/34255 [04:31<00:46, 107.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29220/34255 [04:31<00:46, 107.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29240/34255 [04:31<00:46, 107.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29260/34255 [04:31<00:46, 107.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  85% 29280/34255 [04:31<00:46, 107.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29300/34255 [04:31<00:45, 107.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29320/34255 [04:31<00:45, 107.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29340/34255 [04:31<00:45, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29360/34255 [04:31<00:45, 107.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29380/34255 [04:31<00:45, 108.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29400/34255 [04:32<00:44, 108.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29420/34255 [04:32<00:44, 108.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29440/34255 [04:32<00:44, 108.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29460/34255 [04:32<00:44, 108.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29480/34255 [04:32<00:44, 108.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29500/34255 [04:32<00:43, 108.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29520/34255 [04:32<00:43, 108.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29540/34255 [04:32<00:43, 108.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29560/34255 [04:32<00:43, 108.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29580/34255 [04:32<00:43, 108.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29600/34255 [04:32<00:42, 108.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  86% 29620/34255 [04:32<00:42, 108.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29640/34255 [04:33<00:42, 108.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29660/34255 [04:33<00:42, 108.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29680/34255 [04:33<00:42, 108.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29700/34255 [04:33<00:41, 108.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29720/34255 [04:33<00:41, 108.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29740/34255 [04:33<00:41, 108.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29760/34255 [04:33<00:41, 108.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29780/34255 [04:33<00:41, 108.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29800/34255 [04:33<00:40, 108.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29820/34255 [04:33<00:40, 108.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29840/34255 [04:33<00:40, 108.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29860/34255 [04:34<00:40, 108.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29880/34255 [04:34<00:40, 109.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29900/34255 [04:34<00:39, 109.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29920/34255 [04:34<00:39, 109.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29940/34255 [04:34<00:39, 109.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  87% 29960/34255 [04:34<00:39, 109.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 29980/34255 [04:34<00:39, 109.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30000/34255 [04:34<00:38, 109.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30020/34255 [04:34<00:38, 109.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30040/34255 [04:34<00:38, 109.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30060/34255 [04:34<00:38, 109.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30080/34255 [04:34<00:38, 109.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30100/34255 [04:35<00:37, 109.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30120/34255 [04:35<00:37, 109.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30140/34255 [04:35<00:37, 109.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30160/34255 [04:35<00:37, 109.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30180/34255 [04:35<00:37, 109.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30200/34255 [04:35<00:36, 109.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30220/34255 [04:35<00:36, 109.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30240/34255 [04:35<00:36, 109.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30260/34255 [04:35<00:36, 109.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30280/34255 [04:35<00:36, 109.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  88% 30300/34255 [04:35<00:36, 109.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30320/34255 [04:35<00:35, 109.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30340/34255 [04:36<00:35, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30360/34255 [04:36<00:35, 109.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30380/34255 [04:36<00:35, 109.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30400/34255 [04:36<00:35, 110.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30420/34255 [04:36<00:34, 110.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30440/34255 [04:36<00:34, 110.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30460/34255 [04:36<00:34, 110.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30480/34255 [04:36<00:34, 110.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30500/34255 [04:36<00:34, 110.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30520/34255 [04:36<00:33, 110.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30540/34255 [04:36<00:33, 110.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30560/34255 [04:36<00:33, 110.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30580/34255 [04:37<00:33, 110.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30600/34255 [04:37<00:33, 110.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30620/34255 [04:37<00:32, 110.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  89% 30640/34255 [04:37<00:32, 110.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30660/34255 [04:37<00:32, 110.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30680/34255 [04:37<00:32, 110.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30700/34255 [04:37<00:32, 110.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30720/34255 [04:37<00:31, 110.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30740/34255 [04:37<00:31, 110.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30760/34255 [04:37<00:31, 110.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30780/34255 [04:37<00:31, 110.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30800/34255 [04:37<00:31, 110.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30820/34255 [04:38<00:30, 110.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30840/34255 [04:38<00:30, 110.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30860/34255 [04:38<00:30, 110.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30880/34255 [04:38<00:30, 110.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30900/34255 [04:38<00:30, 110.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30920/34255 [04:38<00:30, 111.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30940/34255 [04:38<00:29, 111.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30960/34255 [04:38<00:29, 111.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 30980/34255 [04:38<00:29, 111.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  90% 31000/34255 [04:38<00:29, 111.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31020/34255 [04:38<00:29, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31040/34255 [04:39<00:28, 111.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31060/34255 [04:39<00:28, 111.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31080/34255 [04:39<00:28, 111.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31100/34255 [04:39<00:28, 111.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31120/34255 [04:39<00:28, 111.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31140/34255 [04:39<00:27, 111.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31160/34255 [04:39<00:27, 111.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31180/34255 [04:39<00:27, 111.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31200/34255 [04:39<00:27, 111.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31220/34255 [04:39<00:27, 111.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31240/34255 [04:39<00:27, 111.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31260/34255 [04:39<00:26, 111.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31280/34255 [04:40<00:26, 111.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31300/34255 [04:40<00:26, 111.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31320/34255 [04:40<00:26, 111.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  91% 31340/34255 [04:40<00:26, 111.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31360/34255 [04:40<00:25, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31380/34255 [04:40<00:25, 111.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31400/34255 [04:40<00:25, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31420/34255 [04:40<00:25, 111.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31440/34255 [04:40<00:25, 112.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31460/34255 [04:40<00:24, 112.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31480/34255 [04:40<00:24, 112.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31500/34255 [04:40<00:24, 112.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31520/34255 [04:41<00:24, 112.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31540/34255 [04:41<00:24, 112.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31560/34255 [04:41<00:24, 112.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31580/34255 [04:41<00:23, 112.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31600/34255 [04:41<00:23, 112.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31620/34255 [04:41<00:23, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31640/34255 [04:41<00:23, 112.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31660/34255 [04:41<00:23, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  92% 31680/34255 [04:41<00:22, 112.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31700/34255 [04:41<00:22, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31720/34255 [04:41<00:22, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31740/34255 [04:42<00:22, 112.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31760/34255 [04:42<00:22, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31780/34255 [04:42<00:21, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31800/34255 [04:42<00:21, 112.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31820/34255 [04:42<00:21, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31840/34255 [04:42<00:21, 112.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31860/34255 [04:42<00:21, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31880/34255 [04:42<00:21, 112.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31900/34255 [04:42<00:20, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31920/34255 [04:42<00:20, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31940/34255 [04:42<00:20, 112.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31960/34255 [04:42<00:20, 112.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 31980/34255 [04:43<00:20, 112.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 32000/34255 [04:43<00:19, 113.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  93% 32020/34255 [04:43<00:19, 113.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32040/34255 [04:43<00:19, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32060/34255 [04:43<00:19, 113.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32080/34255 [04:43<00:19, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32100/34255 [04:43<00:19, 113.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32120/34255 [04:43<00:18, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32140/34255 [04:43<00:18, 113.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32160/34255 [04:43<00:18, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32180/34255 [04:43<00:18, 113.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32200/34255 [04:44<00:18, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32220/34255 [04:44<00:17, 113.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32240/34255 [04:44<00:17, 113.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32260/34255 [04:44<00:17, 113.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32280/34255 [04:44<00:17, 113.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32300/34255 [04:44<00:17, 113.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32320/34255 [04:44<00:17, 113.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32340/34255 [04:44<00:16, 113.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  94% 32360/34255 [04:44<00:16, 113.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32380/34255 [04:44<00:16, 113.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32400/34255 [04:44<00:16, 113.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32420/34255 [04:44<00:16, 113.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32440/34255 [04:45<00:15, 113.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32460/34255 [04:45<00:15, 113.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32480/34255 [04:45<00:15, 113.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32500/34255 [04:45<00:15, 113.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32520/34255 [04:45<00:15, 113.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32540/34255 [04:45<00:15, 113.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32560/34255 [04:45<00:14, 114.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32580/34255 [04:45<00:14, 114.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32600/34255 [04:45<00:14, 114.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32620/34255 [04:45<00:14, 114.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32640/34255 [04:45<00:14, 114.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32660/34255 [04:46<00:13, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32680/34255 [04:46<00:13, 114.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  95% 32700/34255 [04:46<00:13, 114.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32720/34255 [04:46<00:13, 114.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32740/34255 [04:46<00:13, 114.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32760/34255 [04:46<00:13, 114.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32780/34255 [04:46<00:12, 114.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32800/34255 [04:46<00:12, 114.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32820/34255 [04:46<00:12, 114.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32840/34255 [04:46<00:12, 114.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32860/34255 [04:46<00:12, 114.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32880/34255 [04:46<00:11, 114.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32900/34255 [04:47<00:11, 114.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32920/34255 [04:47<00:11, 114.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32940/34255 [04:47<00:11, 114.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32960/34255 [04:47<00:11, 114.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 32980/34255 [04:47<00:11, 114.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 33000/34255 [04:47<00:10, 114.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 33020/34255 [04:47<00:10, 114.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  96% 33040/34255 [04:47<00:10, 114.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33060/34255 [04:47<00:10, 114.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33080/34255 [04:47<00:10, 114.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33100/34255 [04:47<00:10, 114.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33120/34255 [04:47<00:09, 115.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33140/34255 [04:48<00:09, 115.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33160/34255 [04:48<00:09, 115.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33180/34255 [04:48<00:09, 115.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33200/34255 [04:48<00:09, 115.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33220/34255 [04:48<00:08, 115.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33240/34255 [04:48<00:08, 115.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33260/34255 [04:48<00:08, 115.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33280/34255 [04:48<00:08, 115.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33300/34255 [04:48<00:08, 115.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33320/34255 [04:48<00:08, 115.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33340/34255 [04:48<00:07, 115.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33360/34255 [04:49<00:07, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  97% 33380/34255 [04:49<00:07, 115.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33400/34255 [04:49<00:07, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33420/34255 [04:49<00:07, 115.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33440/34255 [04:49<00:07, 115.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33460/34255 [04:49<00:06, 115.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33480/34255 [04:49<00:06, 115.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33500/34255 [04:49<00:06, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33520/34255 [04:49<00:06, 115.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33540/34255 [04:49<00:06, 115.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33560/34255 [04:49<00:06, 115.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33580/34255 [04:49<00:05, 115.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33600/34255 [04:50<00:05, 115.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33620/34255 [04:50<00:05, 115.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33640/34255 [04:50<00:05, 115.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33660/34255 [04:50<00:05, 115.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33680/34255 [04:50<00:04, 115.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33700/34255 [04:50<00:04, 116.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33720/34255 [04:50<00:04, 116.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  98% 33740/34255 [04:50<00:04, 116.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33760/34255 [04:50<00:04, 116.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33780/34255 [04:50<00:04, 116.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33800/34255 [04:50<00:03, 116.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33820/34255 [04:51<00:03, 116.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33840/34255 [04:51<00:03, 116.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33860/34255 [04:51<00:03, 116.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33880/34255 [04:51<00:03, 116.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33900/34255 [04:51<00:03, 116.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33920/34255 [04:51<00:02, 116.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33940/34255 [04:51<00:02, 116.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33960/34255 [04:51<00:02, 116.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 33980/34255 [04:51<00:02, 116.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 34000/34255 [04:51<00:02, 116.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 34020/34255 [04:51<00:02, 116.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 34040/34255 [04:51<00:01, 116.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 34060/34255 [04:52<00:01, 116.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5:  99% 34080/34255 [04:52<00:01, 116.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34100/34255 [04:52<00:01, 116.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34120/34255 [04:52<00:01, 116.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34140/34255 [04:52<00:00, 116.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34160/34255 [04:52<00:00, 116.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34180/34255 [04:52<00:00, 116.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34200/34255 [04:52<00:00, 116.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34220/34255 [04:52<00:00, 116.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Epoch 5: 100% 34240/34255 [04:52<00:00, 116.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 231.07it/s]\u001b[A\n",
            "Epoch 5: 100% 34255/34255 [04:52<00:00, 116.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.660]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 2.692\n",
            "Epoch 6:  80% 27400/34255 [04:19<01:04, 105.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 27420/34255 [04:23<01:05, 103.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27440/34255 [04:24<01:05, 103.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27460/34255 [04:24<01:05, 103.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27480/34255 [04:24<01:05, 103.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27500/34255 [04:24<01:04, 104.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27520/34255 [04:24<01:04, 104.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27540/34255 [04:24<01:04, 104.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  80% 27560/34255 [04:24<01:04, 104.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27580/34255 [04:24<01:04, 104.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27600/34255 [04:24<01:03, 104.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27620/34255 [04:24<01:03, 104.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27640/34255 [04:24<01:03, 104.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27660/34255 [04:25<01:03, 104.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27680/34255 [04:25<01:02, 104.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27700/34255 [04:25<01:02, 104.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27720/34255 [04:25<01:02, 104.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27740/34255 [04:25<01:02, 104.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27760/34255 [04:25<01:02, 104.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27780/34255 [04:25<01:01, 104.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27800/34255 [04:25<01:01, 104.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27820/34255 [04:25<01:01, 104.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27840/34255 [04:25<01:01, 104.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27860/34255 [04:25<01:01, 104.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27880/34255 [04:26<01:00, 104.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  81% 27900/34255 [04:26<01:00, 104.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 27920/34255 [04:26<01:00, 104.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 27940/34255 [04:26<01:00, 104.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 27960/34255 [04:26<00:59, 104.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 27980/34255 [04:26<00:59, 104.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28000/34255 [04:26<00:59, 105.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28020/34255 [04:26<00:59, 105.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28040/34255 [04:26<00:59, 105.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28060/34255 [04:26<00:58, 105.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28080/34255 [04:26<00:58, 105.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28100/34255 [04:27<00:58, 105.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28120/34255 [04:27<00:58, 105.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28140/34255 [04:27<00:58, 105.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28160/34255 [04:27<00:57, 105.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28180/34255 [04:27<00:57, 105.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28200/34255 [04:27<00:57, 105.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28220/34255 [04:27<00:57, 105.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28240/34255 [04:27<00:57, 105.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  82% 28260/34255 [04:27<00:56, 105.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28280/34255 [04:27<00:56, 105.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28300/34255 [04:27<00:56, 105.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28320/34255 [04:28<00:56, 105.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28340/34255 [04:28<00:55, 105.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28360/34255 [04:28<00:55, 105.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28380/34255 [04:28<00:55, 105.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28400/34255 [04:28<00:55, 105.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28420/34255 [04:28<00:55, 105.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28440/34255 [04:28<00:54, 105.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28460/34255 [04:28<00:54, 105.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28480/34255 [04:28<00:54, 105.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28500/34255 [04:28<00:54, 106.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28520/34255 [04:28<00:54, 106.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28540/34255 [04:29<00:53, 106.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28560/34255 [04:29<00:53, 106.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28580/34255 [04:29<00:53, 106.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  83% 28600/34255 [04:29<00:53, 106.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28620/34255 [04:29<00:53, 106.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28640/34255 [04:29<00:52, 106.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28660/34255 [04:29<00:52, 106.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28680/34255 [04:29<00:52, 106.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28700/34255 [04:29<00:52, 106.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28720/34255 [04:29<00:52, 106.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28740/34255 [04:29<00:51, 106.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28760/34255 [04:30<00:51, 106.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28780/34255 [04:30<00:51, 106.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28800/34255 [04:30<00:51, 106.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28820/34255 [04:30<00:50, 106.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28840/34255 [04:30<00:50, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28860/34255 [04:30<00:50, 106.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28880/34255 [04:30<00:50, 106.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28900/34255 [04:30<00:50, 106.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28920/34255 [04:30<00:49, 106.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  84% 28940/34255 [04:30<00:49, 106.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 28960/34255 [04:30<00:49, 106.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 28980/34255 [04:30<00:49, 106.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29000/34255 [04:31<00:49, 106.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29020/34255 [04:31<00:48, 107.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29040/34255 [04:31<00:48, 107.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29060/34255 [04:31<00:48, 107.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29080/34255 [04:31<00:48, 107.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29100/34255 [04:31<00:48, 107.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29120/34255 [04:31<00:47, 107.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29140/34255 [04:31<00:47, 107.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29160/34255 [04:31<00:47, 107.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29180/34255 [04:31<00:47, 107.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29200/34255 [04:31<00:47, 107.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29220/34255 [04:32<00:46, 107.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29240/34255 [04:32<00:46, 107.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29260/34255 [04:32<00:46, 107.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  85% 29280/34255 [04:32<00:46, 107.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29300/34255 [04:32<00:46, 107.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29320/34255 [04:32<00:45, 107.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29340/34255 [04:32<00:45, 107.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29360/34255 [04:32<00:45, 107.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29380/34255 [04:32<00:45, 107.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29400/34255 [04:32<00:45, 107.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29420/34255 [04:32<00:44, 107.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29440/34255 [04:32<00:44, 107.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29460/34255 [04:33<00:44, 107.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29480/34255 [04:33<00:44, 107.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29500/34255 [04:33<00:44, 107.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29520/34255 [04:33<00:43, 108.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29540/34255 [04:33<00:43, 108.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29560/34255 [04:33<00:43, 108.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29580/34255 [04:33<00:43, 108.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29600/34255 [04:33<00:43, 108.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  86% 29620/34255 [04:33<00:42, 108.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29640/34255 [04:33<00:42, 108.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29660/34255 [04:33<00:42, 108.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29680/34255 [04:34<00:42, 108.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29700/34255 [04:34<00:42, 108.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29720/34255 [04:34<00:41, 108.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29740/34255 [04:34<00:41, 108.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29760/34255 [04:34<00:41, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29780/34255 [04:34<00:41, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29800/34255 [04:34<00:41, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29820/34255 [04:34<00:40, 108.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29840/34255 [04:34<00:40, 108.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29860/34255 [04:34<00:40, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29880/34255 [04:34<00:40, 108.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29900/34255 [04:35<00:40, 108.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29920/34255 [04:35<00:39, 108.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29940/34255 [04:35<00:39, 108.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  87% 29960/34255 [04:35<00:39, 108.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 29980/34255 [04:35<00:39, 108.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30000/34255 [04:35<00:39, 108.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30020/34255 [04:35<00:38, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30040/34255 [04:35<00:38, 108.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30060/34255 [04:35<00:38, 109.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30080/34255 [04:35<00:38, 109.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30100/34255 [04:35<00:38, 109.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30120/34255 [04:35<00:37, 109.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30140/34255 [04:36<00:37, 109.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30160/34255 [04:36<00:37, 109.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30180/34255 [04:36<00:37, 109.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30200/34255 [04:36<00:37, 109.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30220/34255 [04:36<00:36, 109.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30240/34255 [04:36<00:36, 109.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30260/34255 [04:36<00:36, 109.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30280/34255 [04:36<00:36, 109.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  88% 30300/34255 [04:36<00:36, 109.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30320/34255 [04:36<00:35, 109.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30340/34255 [04:36<00:35, 109.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30360/34255 [04:36<00:35, 109.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30380/34255 [04:37<00:35, 109.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30400/34255 [04:37<00:35, 109.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30420/34255 [04:37<00:34, 109.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30440/34255 [04:37<00:34, 109.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30460/34255 [04:37<00:34, 109.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30480/34255 [04:37<00:34, 109.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30500/34255 [04:37<00:34, 109.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30520/34255 [04:37<00:33, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30540/34255 [04:37<00:33, 109.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30560/34255 [04:37<00:33, 109.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30580/34255 [04:37<00:33, 110.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30600/34255 [04:38<00:33, 110.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30620/34255 [04:38<00:33, 110.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  89% 30640/34255 [04:38<00:32, 110.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30660/34255 [04:38<00:32, 110.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30680/34255 [04:38<00:32, 110.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30700/34255 [04:38<00:32, 110.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30720/34255 [04:38<00:32, 110.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30740/34255 [04:38<00:31, 110.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30760/34255 [04:38<00:31, 110.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30780/34255 [04:38<00:31, 110.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30800/34255 [04:38<00:31, 110.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30820/34255 [04:38<00:31, 110.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30840/34255 [04:39<00:30, 110.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30860/34255 [04:39<00:30, 110.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30880/34255 [04:39<00:30, 110.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30900/34255 [04:39<00:30, 110.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30920/34255 [04:39<00:30, 110.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30940/34255 [04:39<00:29, 110.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30960/34255 [04:39<00:29, 110.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 30980/34255 [04:39<00:29, 110.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  90% 31000/34255 [04:39<00:29, 110.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31020/34255 [04:39<00:29, 110.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31040/34255 [04:39<00:28, 110.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31060/34255 [04:40<00:28, 110.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31080/34255 [04:40<00:28, 110.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31100/34255 [04:40<00:28, 110.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31120/34255 [04:40<00:28, 111.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31140/34255 [04:40<00:28, 111.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31160/34255 [04:40<00:27, 111.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31180/34255 [04:40<00:27, 111.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31200/34255 [04:40<00:27, 111.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31220/34255 [04:40<00:27, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31240/34255 [04:40<00:27, 111.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31260/34255 [04:40<00:26, 111.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31280/34255 [04:40<00:26, 111.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31300/34255 [04:41<00:26, 111.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31320/34255 [04:41<00:26, 111.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  91% 31340/34255 [04:41<00:26, 111.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31360/34255 [04:41<00:25, 111.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31380/34255 [04:41<00:25, 111.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31400/34255 [04:41<00:25, 111.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31420/34255 [04:41<00:25, 111.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31440/34255 [04:41<00:25, 111.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31460/34255 [04:41<00:25, 111.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31480/34255 [04:41<00:24, 111.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31500/34255 [04:41<00:24, 111.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31520/34255 [04:42<00:24, 111.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31540/34255 [04:42<00:24, 111.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31560/34255 [04:42<00:24, 111.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31580/34255 [04:42<00:23, 111.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31600/34255 [04:42<00:23, 111.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31620/34255 [04:42<00:23, 111.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31640/34255 [04:42<00:23, 111.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31660/34255 [04:42<00:23, 112.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  92% 31680/34255 [04:42<00:22, 112.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31700/34255 [04:42<00:22, 112.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31720/34255 [04:42<00:22, 112.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31740/34255 [04:43<00:22, 112.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31760/34255 [04:43<00:22, 112.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31780/34255 [04:43<00:22, 112.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31800/34255 [04:43<00:21, 112.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31820/34255 [04:43<00:21, 112.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31840/34255 [04:43<00:21, 112.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31860/34255 [04:43<00:21, 112.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31880/34255 [04:43<00:21, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31900/34255 [04:43<00:20, 112.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31920/34255 [04:43<00:20, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31940/34255 [04:43<00:20, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31960/34255 [04:43<00:20, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 31980/34255 [04:44<00:20, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 32000/34255 [04:44<00:20, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  93% 32020/34255 [04:44<00:19, 112.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32040/34255 [04:44<00:19, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32060/34255 [04:44<00:19, 112.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32080/34255 [04:44<00:19, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32100/34255 [04:44<00:19, 112.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32120/34255 [04:44<00:18, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32140/34255 [04:44<00:18, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32160/34255 [04:44<00:18, 112.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32180/34255 [04:44<00:18, 112.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32200/34255 [04:45<00:18, 112.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32220/34255 [04:45<00:18, 113.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32240/34255 [04:45<00:17, 113.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32260/34255 [04:45<00:17, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32280/34255 [04:45<00:17, 113.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32300/34255 [04:45<00:17, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32320/34255 [04:45<00:17, 113.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32340/34255 [04:45<00:16, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  94% 32360/34255 [04:45<00:16, 113.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32380/34255 [04:45<00:16, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32400/34255 [04:45<00:16, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32420/34255 [04:46<00:16, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32440/34255 [04:46<00:16, 113.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32460/34255 [04:46<00:15, 113.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32480/34255 [04:46<00:15, 113.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32500/34255 [04:46<00:15, 113.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32520/34255 [04:46<00:15, 113.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32540/34255 [04:46<00:15, 113.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32560/34255 [04:46<00:14, 113.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32580/34255 [04:46<00:14, 113.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32600/34255 [04:46<00:14, 113.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32620/34255 [04:46<00:14, 113.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32640/34255 [04:46<00:14, 113.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32660/34255 [04:47<00:14, 113.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32680/34255 [04:47<00:13, 113.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  95% 32700/34255 [04:47<00:13, 113.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32720/34255 [04:47<00:13, 113.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32740/34255 [04:47<00:13, 113.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32760/34255 [04:47<00:13, 113.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32780/34255 [04:47<00:12, 113.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32800/34255 [04:47<00:12, 114.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32820/34255 [04:47<00:12, 114.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32840/34255 [04:47<00:12, 114.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32860/34255 [04:47<00:12, 114.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32880/34255 [04:48<00:12, 114.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32900/34255 [04:48<00:11, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32920/34255 [04:48<00:11, 114.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32940/34255 [04:48<00:11, 114.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32960/34255 [04:48<00:11, 114.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 32980/34255 [04:48<00:11, 114.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 33000/34255 [04:48<00:10, 114.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 33020/34255 [04:48<00:10, 114.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  96% 33040/34255 [04:48<00:10, 114.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33060/34255 [04:48<00:10, 114.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33080/34255 [04:48<00:10, 114.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33100/34255 [04:48<00:10, 114.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33120/34255 [04:49<00:09, 114.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33140/34255 [04:49<00:09, 114.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33160/34255 [04:49<00:09, 114.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33180/34255 [04:49<00:09, 114.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33200/34255 [04:49<00:09, 114.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33220/34255 [04:49<00:09, 114.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33240/34255 [04:49<00:08, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33260/34255 [04:49<00:08, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33280/34255 [04:49<00:08, 114.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33300/34255 [04:49<00:08, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33320/34255 [04:49<00:08, 114.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33340/34255 [04:49<00:07, 114.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33360/34255 [04:50<00:07, 115.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  97% 33380/34255 [04:50<00:07, 115.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33400/34255 [04:50<00:07, 115.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33420/34255 [04:50<00:07, 115.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33440/34255 [04:50<00:07, 115.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33460/34255 [04:50<00:06, 115.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33480/34255 [04:50<00:06, 115.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33500/34255 [04:50<00:06, 115.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33520/34255 [04:50<00:06, 115.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33540/34255 [04:50<00:06, 115.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33560/34255 [04:50<00:06, 115.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33580/34255 [04:50<00:05, 115.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33600/34255 [04:51<00:05, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33620/34255 [04:51<00:05, 115.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33640/34255 [04:51<00:05, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33660/34255 [04:51<00:05, 115.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33680/34255 [04:51<00:04, 115.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33700/34255 [04:51<00:04, 115.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33720/34255 [04:51<00:04, 115.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  98% 33740/34255 [04:51<00:04, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33760/34255 [04:51<00:04, 115.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33780/34255 [04:51<00:04, 115.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33800/34255 [04:51<00:03, 115.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33820/34255 [04:52<00:03, 115.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33840/34255 [04:52<00:03, 115.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33860/34255 [04:52<00:03, 115.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33880/34255 [04:52<00:03, 115.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33900/34255 [04:52<00:03, 115.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33920/34255 [04:52<00:02, 115.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33940/34255 [04:52<00:02, 116.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33960/34255 [04:52<00:02, 116.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 33980/34255 [04:52<00:02, 116.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 34000/34255 [04:52<00:02, 116.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 34020/34255 [04:52<00:02, 116.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 34040/34255 [04:53<00:01, 116.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 34060/34255 [04:53<00:01, 116.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6:  99% 34080/34255 [04:53<00:01, 116.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34100/34255 [04:53<00:01, 116.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34120/34255 [04:53<00:01, 116.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34140/34255 [04:53<00:00, 116.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34160/34255 [04:53<00:00, 116.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34180/34255 [04:53<00:00, 116.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34200/34255 [04:53<00:00, 116.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34220/34255 [04:53<00:00, 116.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 6: 100% 34240/34255 [04:53<00:00, 116.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 227.84it/s]\u001b[A\n",
            "Epoch 6: 100% 34255/34255 [04:53<00:00, 116.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 2.691\n",
            "Epoch 7:  80% 27400/34255 [04:20<01:05, 105.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 27420/34255 [04:24<01:06, 103.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27440/34255 [04:24<01:05, 103.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27460/34255 [04:25<01:05, 103.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27480/34255 [04:25<01:05, 103.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27500/34255 [04:25<01:05, 103.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27520/34255 [04:25<01:04, 103.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27540/34255 [04:25<01:04, 103.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  80% 27560/34255 [04:25<01:04, 103.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27580/34255 [04:25<01:04, 103.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27600/34255 [04:25<01:04, 103.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27620/34255 [04:25<01:03, 103.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27640/34255 [04:25<01:03, 103.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27660/34255 [04:25<01:03, 104.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27680/34255 [04:26<01:03, 104.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27700/34255 [04:26<01:02, 104.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27720/34255 [04:26<01:02, 104.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27740/34255 [04:26<01:02, 104.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27760/34255 [04:26<01:02, 104.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27780/34255 [04:26<01:02, 104.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27800/34255 [04:26<01:01, 104.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27820/34255 [04:26<01:01, 104.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27840/34255 [04:26<01:01, 104.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27860/34255 [04:26<01:01, 104.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27880/34255 [04:26<01:01, 104.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  81% 27900/34255 [04:27<01:00, 104.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 27920/34255 [04:27<01:00, 104.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 27940/34255 [04:27<01:00, 104.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 27960/34255 [04:27<01:00, 104.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 27980/34255 [04:27<00:59, 104.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28000/34255 [04:27<00:59, 104.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28020/34255 [04:27<00:59, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28040/34255 [04:27<00:59, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28060/34255 [04:27<00:59, 104.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28080/34255 [04:27<00:58, 104.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28100/34255 [04:27<00:58, 104.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28120/34255 [04:27<00:58, 104.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28140/34255 [04:28<00:58, 104.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28160/34255 [04:28<00:58, 105.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28180/34255 [04:28<00:57, 105.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28200/34255 [04:28<00:57, 105.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28220/34255 [04:28<00:57, 105.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28240/34255 [04:28<00:57, 105.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  82% 28260/34255 [04:28<00:56, 105.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28280/34255 [04:28<00:56, 105.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28300/34255 [04:28<00:56, 105.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28320/34255 [04:28<00:56, 105.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28340/34255 [04:28<00:56, 105.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28360/34255 [04:28<00:55, 105.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28380/34255 [04:29<00:55, 105.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28400/34255 [04:29<00:55, 105.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28420/34255 [04:29<00:55, 105.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28440/34255 [04:29<00:55, 105.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28460/34255 [04:29<00:54, 105.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28480/34255 [04:29<00:54, 105.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28500/34255 [04:29<00:54, 105.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28520/34255 [04:29<00:54, 105.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28540/34255 [04:29<00:54, 105.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28560/34255 [04:29<00:53, 105.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28580/34255 [04:29<00:53, 105.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  83% 28600/34255 [04:30<00:53, 105.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28620/34255 [04:30<00:53, 105.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28640/34255 [04:30<00:52, 105.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28660/34255 [04:30<00:52, 106.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28680/34255 [04:30<00:52, 106.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28700/34255 [04:30<00:52, 106.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28720/34255 [04:30<00:52, 106.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28740/34255 [04:30<00:51, 106.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28760/34255 [04:30<00:51, 106.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28780/34255 [04:30<00:51, 106.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28800/34255 [04:30<00:51, 106.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28820/34255 [04:30<00:51, 106.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28840/34255 [04:31<00:50, 106.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28860/34255 [04:31<00:50, 106.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28880/34255 [04:31<00:50, 106.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28900/34255 [04:31<00:50, 106.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28920/34255 [04:31<00:50, 106.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  84% 28940/34255 [04:31<00:49, 106.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 28960/34255 [04:31<00:49, 106.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 28980/34255 [04:31<00:49, 106.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29000/34255 [04:31<00:49, 106.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29020/34255 [04:31<00:49, 106.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29040/34255 [04:31<00:48, 106.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29060/34255 [04:31<00:48, 106.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29080/34255 [04:32<00:48, 106.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29100/34255 [04:32<00:48, 106.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29120/34255 [04:32<00:48, 106.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29140/34255 [04:32<00:47, 107.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29160/34255 [04:32<00:47, 107.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29180/34255 [04:32<00:47, 107.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29200/34255 [04:32<00:47, 107.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29220/34255 [04:32<00:46, 107.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29240/34255 [04:32<00:46, 107.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29260/34255 [04:32<00:46, 107.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  85% 29280/34255 [04:32<00:46, 107.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29300/34255 [04:33<00:46, 107.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29320/34255 [04:33<00:45, 107.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29340/34255 [04:33<00:45, 107.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29360/34255 [04:33<00:45, 107.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29380/34255 [04:33<00:45, 107.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29400/34255 [04:33<00:45, 107.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29420/34255 [04:33<00:44, 107.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29440/34255 [04:33<00:44, 107.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29460/34255 [04:33<00:44, 107.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29480/34255 [04:33<00:44, 107.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29500/34255 [04:33<00:44, 107.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29520/34255 [04:33<00:43, 107.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29540/34255 [04:34<00:43, 107.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29560/34255 [04:34<00:43, 107.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29580/34255 [04:34<00:43, 107.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29600/34255 [04:34<00:43, 107.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  86% 29620/34255 [04:34<00:42, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29640/34255 [04:34<00:42, 107.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29660/34255 [04:34<00:42, 108.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29680/34255 [04:34<00:42, 108.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29700/34255 [04:34<00:42, 108.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29720/34255 [04:34<00:41, 108.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29740/34255 [04:34<00:41, 108.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29760/34255 [04:34<00:41, 108.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29780/34255 [04:35<00:41, 108.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29800/34255 [04:35<00:41, 108.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29820/34255 [04:35<00:40, 108.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29840/34255 [04:35<00:40, 108.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29860/34255 [04:35<00:40, 108.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29880/34255 [04:35<00:40, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29900/34255 [04:35<00:40, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29920/34255 [04:35<00:39, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29940/34255 [04:35<00:39, 108.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  87% 29960/34255 [04:35<00:39, 108.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 29980/34255 [04:35<00:39, 108.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30000/34255 [04:36<00:39, 108.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30020/34255 [04:36<00:38, 108.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30040/34255 [04:36<00:38, 108.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30060/34255 [04:36<00:38, 108.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30080/34255 [04:36<00:38, 108.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30100/34255 [04:36<00:38, 108.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30120/34255 [04:36<00:37, 108.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30140/34255 [04:36<00:37, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30160/34255 [04:36<00:37, 108.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30180/34255 [04:36<00:37, 109.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30200/34255 [04:36<00:37, 109.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30220/34255 [04:37<00:36, 109.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30240/34255 [04:37<00:36, 109.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30260/34255 [04:37<00:36, 109.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30280/34255 [04:37<00:36, 109.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  88% 30300/34255 [04:37<00:36, 109.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30320/34255 [04:37<00:36, 109.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30340/34255 [04:37<00:35, 109.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30360/34255 [04:37<00:35, 109.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30380/34255 [04:37<00:35, 109.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30400/34255 [04:37<00:35, 109.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30420/34255 [04:37<00:35, 109.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30440/34255 [04:38<00:34, 109.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30460/34255 [04:38<00:34, 109.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30480/34255 [04:38<00:34, 109.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30500/34255 [04:38<00:34, 109.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30520/34255 [04:38<00:34, 109.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30540/34255 [04:38<00:33, 109.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30560/34255 [04:38<00:33, 109.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30580/34255 [04:38<00:33, 109.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30600/34255 [04:38<00:33, 109.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30620/34255 [04:38<00:33, 109.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  89% 30640/34255 [04:38<00:32, 109.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30660/34255 [04:38<00:32, 109.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30680/34255 [04:39<00:32, 109.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30700/34255 [04:39<00:32, 109.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30720/34255 [04:39<00:32, 110.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30740/34255 [04:39<00:31, 110.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30760/34255 [04:39<00:31, 110.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30780/34255 [04:39<00:31, 110.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30800/34255 [04:39<00:31, 110.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30820/34255 [04:39<00:31, 110.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30840/34255 [04:39<00:30, 110.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30860/34255 [04:39<00:30, 110.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30880/34255 [04:39<00:30, 110.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30900/34255 [04:40<00:30, 110.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30920/34255 [04:40<00:30, 110.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30940/34255 [04:40<00:30, 110.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30960/34255 [04:40<00:29, 110.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 30980/34255 [04:40<00:29, 110.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  90% 31000/34255 [04:40<00:29, 110.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31020/34255 [04:40<00:29, 110.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31040/34255 [04:40<00:29, 110.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31060/34255 [04:40<00:28, 110.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31080/34255 [04:40<00:28, 110.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31100/34255 [04:40<00:28, 110.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31120/34255 [04:40<00:28, 110.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31140/34255 [04:41<00:28, 110.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31160/34255 [04:41<00:27, 110.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31180/34255 [04:41<00:27, 110.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31200/34255 [04:41<00:27, 110.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31220/34255 [04:41<00:27, 110.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31240/34255 [04:41<00:27, 110.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31260/34255 [04:41<00:26, 111.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31280/34255 [04:41<00:26, 111.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31300/34255 [04:41<00:26, 111.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31320/34255 [04:41<00:26, 111.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  91% 31340/34255 [04:41<00:26, 111.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31360/34255 [04:42<00:26, 111.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31380/34255 [04:42<00:25, 111.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31400/34255 [04:42<00:25, 111.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31420/34255 [04:42<00:25, 111.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31440/34255 [04:42<00:25, 111.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31460/34255 [04:42<00:25, 111.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31480/34255 [04:42<00:24, 111.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31500/34255 [04:42<00:24, 111.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31520/34255 [04:42<00:24, 111.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31540/34255 [04:42<00:24, 111.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31560/34255 [04:42<00:24, 111.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31580/34255 [04:42<00:23, 111.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31600/34255 [04:43<00:23, 111.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31620/34255 [04:43<00:23, 111.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31640/34255 [04:43<00:23, 111.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31660/34255 [04:43<00:23, 111.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  92% 31680/34255 [04:43<00:23, 111.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31700/34255 [04:43<00:22, 111.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31720/34255 [04:43<00:22, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31740/34255 [04:43<00:22, 111.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31760/34255 [04:43<00:22, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31780/34255 [04:43<00:22, 111.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31800/34255 [04:43<00:21, 112.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31820/34255 [04:44<00:21, 112.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31840/34255 [04:44<00:21, 112.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31860/34255 [04:44<00:21, 112.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31880/34255 [04:44<00:21, 112.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31900/34255 [04:44<00:20, 112.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31920/34255 [04:44<00:20, 112.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31940/34255 [04:44<00:20, 112.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31960/34255 [04:44<00:20, 112.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 31980/34255 [04:44<00:20, 112.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 32000/34255 [04:44<00:20, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  93% 32020/34255 [04:44<00:19, 112.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32040/34255 [04:45<00:19, 112.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32060/34255 [04:45<00:19, 112.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32080/34255 [04:45<00:19, 112.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32100/34255 [04:45<00:19, 112.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32120/34255 [04:45<00:18, 112.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32140/34255 [04:45<00:18, 112.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32160/34255 [04:45<00:18, 112.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32180/34255 [04:45<00:18, 112.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32200/34255 [04:45<00:18, 112.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32220/34255 [04:45<00:18, 112.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32240/34255 [04:45<00:17, 112.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32260/34255 [04:45<00:17, 112.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32280/34255 [04:46<00:17, 112.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32300/34255 [04:46<00:17, 112.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32320/34255 [04:46<00:17, 112.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32340/34255 [04:46<00:16, 112.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  94% 32360/34255 [04:46<00:16, 112.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32380/34255 [04:46<00:16, 113.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32400/34255 [04:46<00:16, 113.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32420/34255 [04:46<00:16, 113.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32440/34255 [04:46<00:16, 113.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32460/34255 [04:46<00:15, 113.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32480/34255 [04:46<00:15, 113.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32500/34255 [04:47<00:15, 113.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32520/34255 [04:47<00:15, 113.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32540/34255 [04:47<00:15, 113.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32560/34255 [04:47<00:14, 113.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32580/34255 [04:47<00:14, 113.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32600/34255 [04:47<00:14, 113.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32620/34255 [04:47<00:14, 113.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32640/34255 [04:47<00:14, 113.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32660/34255 [04:47<00:14, 113.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32680/34255 [04:47<00:13, 113.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  95% 32700/34255 [04:47<00:13, 113.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32720/34255 [04:47<00:13, 113.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32740/34255 [04:48<00:13, 113.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32760/34255 [04:48<00:13, 113.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32780/34255 [04:48<00:12, 113.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32800/34255 [04:48<00:12, 113.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32820/34255 [04:48<00:12, 113.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32840/34255 [04:48<00:12, 113.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32860/34255 [04:48<00:12, 113.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32880/34255 [04:48<00:12, 113.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32900/34255 [04:48<00:11, 113.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32920/34255 [04:48<00:11, 113.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32940/34255 [04:48<00:11, 113.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32960/34255 [04:49<00:11, 114.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 32980/34255 [04:49<00:11, 114.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 33000/34255 [04:49<00:11, 114.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 33020/34255 [04:49<00:10, 114.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  96% 33040/34255 [04:49<00:10, 114.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33060/34255 [04:49<00:10, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33080/34255 [04:49<00:10, 114.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33100/34255 [04:49<00:10, 114.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33120/34255 [04:49<00:09, 114.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33140/34255 [04:49<00:09, 114.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33160/34255 [04:49<00:09, 114.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33180/34255 [04:50<00:09, 114.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33200/34255 [04:50<00:09, 114.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33220/34255 [04:50<00:09, 114.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33240/34255 [04:50<00:08, 114.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33260/34255 [04:50<00:08, 114.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33280/34255 [04:50<00:08, 114.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33300/34255 [04:50<00:08, 114.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33320/34255 [04:50<00:08, 114.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33340/34255 [04:50<00:07, 114.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33360/34255 [04:50<00:07, 114.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  97% 33380/34255 [04:50<00:07, 114.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33400/34255 [04:50<00:07, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33420/34255 [04:51<00:07, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33440/34255 [04:51<00:07, 114.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33460/34255 [04:51<00:06, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33480/34255 [04:51<00:06, 114.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33500/34255 [04:51<00:06, 114.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33520/34255 [04:51<00:06, 114.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33540/34255 [04:51<00:06, 115.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33560/34255 [04:51<00:06, 115.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33580/34255 [04:51<00:05, 115.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33600/34255 [04:51<00:05, 115.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33620/34255 [04:51<00:05, 115.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33640/34255 [04:52<00:05, 115.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33660/34255 [04:52<00:05, 115.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33680/34255 [04:52<00:04, 115.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33700/34255 [04:52<00:04, 115.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33720/34255 [04:52<00:04, 115.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  98% 33740/34255 [04:52<00:04, 115.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33760/34255 [04:52<00:04, 115.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33780/34255 [04:52<00:04, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33800/34255 [04:52<00:03, 115.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33820/34255 [04:52<00:03, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33840/34255 [04:52<00:03, 115.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33860/34255 [04:52<00:03, 115.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33880/34255 [04:53<00:03, 115.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33900/34255 [04:53<00:03, 115.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33920/34255 [04:53<00:02, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33940/34255 [04:53<00:02, 115.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33960/34255 [04:53<00:02, 115.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 33980/34255 [04:53<00:02, 115.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 34000/34255 [04:53<00:02, 115.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 34020/34255 [04:53<00:02, 115.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 34040/34255 [04:53<00:01, 115.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 34060/34255 [04:53<00:01, 115.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7:  99% 34080/34255 [04:53<00:01, 115.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34100/34255 [04:54<00:01, 115.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34120/34255 [04:54<00:01, 116.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34140/34255 [04:54<00:00, 116.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34160/34255 [04:54<00:00, 116.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34180/34255 [04:54<00:00, 116.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34200/34255 [04:54<00:00, 116.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34220/34255 [04:54<00:00, 116.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 7: 100% 34240/34255 [04:54<00:00, 116.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 229.01it/s]\u001b[A\n",
            "Epoch 7: 100% 34255/34255 [04:54<00:00, 116.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27400/34255 [04:21<01:05, 104.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 27420/34255 [04:25<01:06, 103.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27440/34255 [04:25<01:06, 103.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27460/34255 [04:25<01:05, 103.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27480/34255 [04:26<01:05, 103.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27500/34255 [04:26<01:05, 103.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27520/34255 [04:26<01:05, 103.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27540/34255 [04:26<01:04, 103.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  80% 27560/34255 [04:26<01:04, 103.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27580/34255 [04:26<01:04, 103.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27600/34255 [04:26<01:04, 103.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27620/34255 [04:26<01:04, 103.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27640/34255 [04:26<01:03, 103.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27660/34255 [04:26<01:03, 103.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27680/34255 [04:26<01:03, 103.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27700/34255 [04:27<01:03, 103.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27720/34255 [04:27<01:02, 103.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27740/34255 [04:27<01:02, 103.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27760/34255 [04:27<01:02, 103.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27780/34255 [04:27<01:02, 103.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27800/34255 [04:27<01:02, 103.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27820/34255 [04:27<01:01, 104.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27840/34255 [04:27<01:01, 104.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27860/34255 [04:27<01:01, 104.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27880/34255 [04:27<01:01, 104.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  81% 27900/34255 [04:27<01:01, 104.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 27920/34255 [04:27<01:00, 104.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 27940/34255 [04:27<01:00, 104.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 27960/34255 [04:28<01:00, 104.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 27980/34255 [04:28<01:00, 104.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28000/34255 [04:28<00:59, 104.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28020/34255 [04:28<00:59, 104.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28040/34255 [04:28<00:59, 104.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28060/34255 [04:28<00:59, 104.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28080/34255 [04:28<00:59, 104.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28100/34255 [04:28<00:58, 104.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28120/34255 [04:28<00:58, 104.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28140/34255 [04:28<00:58, 104.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28160/34255 [04:28<00:58, 104.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28180/34255 [04:29<00:58, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28200/34255 [04:29<00:57, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28220/34255 [04:29<00:57, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28240/34255 [04:29<00:57, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  82% 28260/34255 [04:29<00:57, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28280/34255 [04:29<00:56, 104.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28300/34255 [04:29<00:56, 104.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28320/34255 [04:29<00:56, 105.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28340/34255 [04:29<00:56, 105.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28360/34255 [04:29<00:56, 105.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28380/34255 [04:29<00:55, 105.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28400/34255 [04:30<00:55, 105.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28420/34255 [04:30<00:55, 105.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28440/34255 [04:30<00:55, 105.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28460/34255 [04:30<00:55, 105.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28480/34255 [04:30<00:54, 105.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28500/34255 [04:30<00:54, 105.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28520/34255 [04:30<00:54, 105.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28540/34255 [04:30<00:54, 105.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28560/34255 [04:30<00:53, 105.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28580/34255 [04:30<00:53, 105.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  83% 28600/34255 [04:30<00:53, 105.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28620/34255 [04:31<00:53, 105.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28640/34255 [04:31<00:53, 105.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28660/34255 [04:31<00:52, 105.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28680/34255 [04:31<00:52, 105.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28700/34255 [04:31<00:52, 105.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28720/34255 [04:31<00:52, 105.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28740/34255 [04:31<00:52, 105.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28760/34255 [04:31<00:51, 105.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28780/34255 [04:31<00:51, 105.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28800/34255 [04:31<00:51, 105.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28820/34255 [04:31<00:51, 105.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28840/34255 [04:32<00:51, 106.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28860/34255 [04:32<00:50, 106.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28880/34255 [04:32<00:50, 106.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28900/34255 [04:32<00:50, 106.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28920/34255 [04:32<00:50, 106.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  84% 28940/34255 [04:32<00:50, 106.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 28960/34255 [04:32<00:49, 106.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 28980/34255 [04:32<00:49, 106.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29000/34255 [04:32<00:49, 106.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29020/34255 [04:32<00:49, 106.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29040/34255 [04:32<00:49, 106.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29060/34255 [04:32<00:48, 106.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29080/34255 [04:33<00:48, 106.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29100/34255 [04:33<00:48, 106.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29120/34255 [04:33<00:48, 106.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29140/34255 [04:33<00:47, 106.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29160/34255 [04:33<00:47, 106.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29180/34255 [04:33<00:47, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29200/34255 [04:33<00:47, 106.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29220/34255 [04:33<00:47, 106.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29240/34255 [04:33<00:46, 106.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29260/34255 [04:33<00:46, 106.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  85% 29280/34255 [04:34<00:46, 106.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29300/34255 [04:34<00:46, 106.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29320/34255 [04:34<00:46, 106.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29340/34255 [04:34<00:45, 106.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29360/34255 [04:34<00:45, 107.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29380/34255 [04:34<00:45, 107.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29400/34255 [04:34<00:45, 107.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29420/34255 [04:34<00:45, 107.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29440/34255 [04:34<00:44, 107.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29460/34255 [04:34<00:44, 107.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29480/34255 [04:34<00:44, 107.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29500/34255 [04:35<00:44, 107.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29520/34255 [04:35<00:44, 107.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29540/34255 [04:35<00:43, 107.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29560/34255 [04:35<00:43, 107.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29580/34255 [04:35<00:43, 107.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29600/34255 [04:35<00:43, 107.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  86% 29620/34255 [04:35<00:43, 107.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29640/34255 [04:35<00:42, 107.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29660/34255 [04:35<00:42, 107.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29680/34255 [04:35<00:42, 107.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29700/34255 [04:35<00:42, 107.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29720/34255 [04:35<00:42, 107.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29740/34255 [04:36<00:41, 107.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29760/34255 [04:36<00:41, 107.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29780/34255 [04:36<00:41, 107.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29800/34255 [04:36<00:41, 107.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29820/34255 [04:36<00:41, 107.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29840/34255 [04:36<00:40, 107.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29860/34255 [04:36<00:40, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29880/34255 [04:36<00:40, 107.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29900/34255 [04:36<00:40, 108.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29920/34255 [04:36<00:40, 108.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29940/34255 [04:36<00:39, 108.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  87% 29960/34255 [04:37<00:39, 108.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 29980/34255 [04:37<00:39, 108.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30000/34255 [04:37<00:39, 108.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30020/34255 [04:37<00:39, 108.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30040/34255 [04:37<00:38, 108.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30060/34255 [04:37<00:38, 108.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30080/34255 [04:37<00:38, 108.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30100/34255 [04:37<00:38, 108.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30120/34255 [04:37<00:38, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30140/34255 [04:37<00:37, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30160/34255 [04:37<00:37, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30180/34255 [04:37<00:37, 108.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30200/34255 [04:38<00:37, 108.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30220/34255 [04:38<00:37, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30240/34255 [04:38<00:36, 108.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30260/34255 [04:38<00:36, 108.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30280/34255 [04:38<00:36, 108.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  88% 30300/34255 [04:38<00:36, 108.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30320/34255 [04:38<00:36, 108.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30340/34255 [04:38<00:35, 108.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30360/34255 [04:38<00:35, 108.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30380/34255 [04:38<00:35, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30400/34255 [04:38<00:35, 108.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30420/34255 [04:39<00:35, 109.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30440/34255 [04:39<00:34, 109.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30460/34255 [04:39<00:34, 109.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30480/34255 [04:39<00:34, 109.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30500/34255 [04:39<00:34, 109.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30520/34255 [04:39<00:34, 109.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30540/34255 [04:39<00:34, 109.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30560/34255 [04:39<00:33, 109.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30580/34255 [04:39<00:33, 109.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30600/34255 [04:39<00:33, 109.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30620/34255 [04:39<00:33, 109.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  89% 30640/34255 [04:39<00:33, 109.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30660/34255 [04:40<00:32, 109.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30680/34255 [04:40<00:32, 109.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30700/34255 [04:40<00:32, 109.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30720/34255 [04:40<00:32, 109.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30740/34255 [04:40<00:32, 109.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30760/34255 [04:40<00:31, 109.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30780/34255 [04:40<00:31, 109.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30800/34255 [04:40<00:31, 109.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30820/34255 [04:40<00:31, 109.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30840/34255 [04:40<00:31, 109.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30860/34255 [04:40<00:30, 109.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30880/34255 [04:41<00:30, 109.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30900/34255 [04:41<00:30, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30920/34255 [04:41<00:30, 109.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30940/34255 [04:41<00:30, 109.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30960/34255 [04:41<00:29, 110.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 30980/34255 [04:41<00:29, 110.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  90% 31000/34255 [04:41<00:29, 110.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31020/34255 [04:41<00:29, 110.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31040/34255 [04:41<00:29, 110.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31060/34255 [04:41<00:28, 110.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31080/34255 [04:41<00:28, 110.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31100/34255 [04:42<00:28, 110.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31120/34255 [04:42<00:28, 110.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31140/34255 [04:42<00:28, 110.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31160/34255 [04:42<00:28, 110.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31180/34255 [04:42<00:27, 110.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31200/34255 [04:42<00:27, 110.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31220/34255 [04:42<00:27, 110.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31240/34255 [04:42<00:27, 110.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31260/34255 [04:42<00:27, 110.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31280/34255 [04:42<00:26, 110.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31300/34255 [04:42<00:26, 110.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31320/34255 [04:42<00:26, 110.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  91% 31340/34255 [04:43<00:26, 110.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31360/34255 [04:43<00:26, 110.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31380/34255 [04:43<00:25, 110.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31400/34255 [04:43<00:25, 110.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31420/34255 [04:43<00:25, 110.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31440/34255 [04:43<00:25, 110.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31460/34255 [04:43<00:25, 110.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31480/34255 [04:43<00:25, 110.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31500/34255 [04:43<00:24, 111.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31520/34255 [04:43<00:24, 111.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31540/34255 [04:43<00:24, 111.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31560/34255 [04:44<00:24, 111.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31580/34255 [04:44<00:24, 111.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31600/34255 [04:44<00:23, 111.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31620/34255 [04:44<00:23, 111.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31640/34255 [04:44<00:23, 111.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31660/34255 [04:44<00:23, 111.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  92% 31680/34255 [04:44<00:23, 111.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31700/34255 [04:44<00:22, 111.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31720/34255 [04:44<00:22, 111.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31740/34255 [04:44<00:22, 111.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31760/34255 [04:44<00:22, 111.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31780/34255 [04:44<00:22, 111.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31800/34255 [04:45<00:22, 111.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31820/34255 [04:45<00:21, 111.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31840/34255 [04:45<00:21, 111.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31860/34255 [04:45<00:21, 111.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31880/34255 [04:45<00:21, 111.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31900/34255 [04:45<00:21, 111.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31920/34255 [04:45<00:20, 111.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31940/34255 [04:45<00:20, 111.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31960/34255 [04:45<00:20, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 31980/34255 [04:45<00:20, 111.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 32000/34255 [04:45<00:20, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  93% 32020/34255 [04:46<00:19, 111.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32040/34255 [04:46<00:19, 111.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32060/34255 [04:46<00:19, 112.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32080/34255 [04:46<00:19, 112.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32100/34255 [04:46<00:19, 112.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32120/34255 [04:46<00:19, 112.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32140/34255 [04:46<00:18, 112.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32160/34255 [04:46<00:18, 112.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32180/34255 [04:46<00:18, 112.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32200/34255 [04:46<00:18, 112.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32220/34255 [04:46<00:18, 112.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32240/34255 [04:46<00:17, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32260/34255 [04:47<00:17, 112.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32280/34255 [04:47<00:17, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32300/34255 [04:47<00:17, 112.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32320/34255 [04:47<00:17, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32340/34255 [04:47<00:17, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  94% 32360/34255 [04:47<00:16, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32380/34255 [04:47<00:16, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32400/34255 [04:47<00:16, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32420/34255 [04:47<00:16, 112.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32440/34255 [04:47<00:16, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32460/34255 [04:47<00:15, 112.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32480/34255 [04:48<00:15, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32500/34255 [04:48<00:15, 112.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32520/34255 [04:48<00:15, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32540/34255 [04:48<00:15, 112.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32560/34255 [04:48<00:15, 112.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32580/34255 [04:48<00:14, 112.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32600/34255 [04:48<00:14, 112.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32620/34255 [04:48<00:14, 112.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32640/34255 [04:48<00:14, 113.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32660/34255 [04:48<00:14, 113.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32680/34255 [04:48<00:13, 113.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  95% 32700/34255 [04:49<00:13, 113.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32720/34255 [04:49<00:13, 113.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32740/34255 [04:49<00:13, 113.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32760/34255 [04:49<00:13, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32780/34255 [04:49<00:13, 113.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32800/34255 [04:49<00:12, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32820/34255 [04:49<00:12, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32840/34255 [04:49<00:12, 113.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32860/34255 [04:49<00:12, 113.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32880/34255 [04:49<00:12, 113.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32900/34255 [04:50<00:11, 113.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32920/34255 [04:50<00:11, 113.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32940/34255 [04:50<00:11, 113.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32960/34255 [04:50<00:11, 113.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 32980/34255 [04:50<00:11, 113.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 33000/34255 [04:50<00:11, 113.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 33020/34255 [04:50<00:10, 113.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  96% 33040/34255 [04:50<00:10, 113.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33060/34255 [04:50<00:10, 113.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33080/34255 [04:50<00:10, 113.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33100/34255 [04:50<00:10, 113.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33120/34255 [04:50<00:09, 113.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33140/34255 [04:51<00:09, 113.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33160/34255 [04:51<00:09, 113.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33180/34255 [04:51<00:09, 113.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33200/34255 [04:51<00:09, 113.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33220/34255 [04:51<00:09, 113.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33240/34255 [04:51<00:08, 114.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33260/34255 [04:51<00:08, 114.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33280/34255 [04:51<00:08, 114.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33300/34255 [04:51<00:08, 114.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33320/34255 [04:51<00:08, 114.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33340/34255 [04:51<00:08, 114.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33360/34255 [04:52<00:07, 114.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  97% 33380/34255 [04:52<00:07, 114.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33400/34255 [04:52<00:07, 114.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33420/34255 [04:52<00:07, 114.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33440/34255 [04:52<00:07, 114.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33460/34255 [04:52<00:06, 114.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33480/34255 [04:52<00:06, 114.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33500/34255 [04:52<00:06, 114.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33520/34255 [04:52<00:06, 114.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33540/34255 [04:52<00:06, 114.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33560/34255 [04:52<00:06, 114.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33580/34255 [04:53<00:05, 114.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33600/34255 [04:53<00:05, 114.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33620/34255 [04:53<00:05, 114.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33640/34255 [04:53<00:05, 114.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33660/34255 [04:53<00:05, 114.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33680/34255 [04:53<00:05, 114.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33700/34255 [04:53<00:04, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33720/34255 [04:53<00:04, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  98% 33740/34255 [04:53<00:04, 114.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33760/34255 [04:53<00:04, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33780/34255 [04:53<00:04, 114.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33800/34255 [04:54<00:03, 114.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33820/34255 [04:54<00:03, 114.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33840/34255 [04:54<00:03, 115.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33860/34255 [04:54<00:03, 115.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33880/34255 [04:54<00:03, 115.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33900/34255 [04:54<00:03, 115.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33920/34255 [04:54<00:02, 115.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33940/34255 [04:54<00:02, 115.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33960/34255 [04:54<00:02, 115.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 33980/34255 [04:54<00:02, 115.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 34000/34255 [04:54<00:02, 115.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 34020/34255 [04:55<00:02, 115.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 34040/34255 [04:55<00:01, 115.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 34060/34255 [04:55<00:01, 115.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8:  99% 34080/34255 [04:55<00:01, 115.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34100/34255 [04:55<00:01, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34120/34255 [04:55<00:01, 115.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34140/34255 [04:55<00:00, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34160/34255 [04:55<00:00, 115.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34180/34255 [04:55<00:00, 115.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34200/34255 [04:55<00:00, 115.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34220/34255 [04:55<00:00, 115.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 8: 100% 34240/34255 [04:56<00:00, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 225.71it/s]\u001b[A\n",
            "Epoch 8: 100% 34255/34255 [04:56<00:00, 115.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27400/34255 [04:21<01:05, 104.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 27420/34255 [04:26<01:06, 103.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27440/34255 [04:26<01:06, 103.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27460/34255 [04:26<01:05, 103.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27480/34255 [04:26<01:05, 103.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27500/34255 [04:26<01:05, 103.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27520/34255 [04:26<01:05, 103.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27540/34255 [04:26<01:05, 103.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  80% 27560/34255 [04:26<01:04, 103.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27580/34255 [04:26<01:04, 103.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27600/34255 [04:27<01:04, 103.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27620/34255 [04:27<01:04, 103.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27640/34255 [04:27<01:03, 103.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27660/34255 [04:27<01:03, 103.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27680/34255 [04:27<01:03, 103.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27700/34255 [04:27<01:03, 103.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27720/34255 [04:27<01:03, 103.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27740/34255 [04:27<01:02, 103.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27760/34255 [04:27<01:02, 103.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27780/34255 [04:27<01:02, 103.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27800/34255 [04:27<01:02, 103.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27820/34255 [04:28<01:01, 103.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27840/34255 [04:28<01:01, 103.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27860/34255 [04:28<01:01, 103.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27880/34255 [04:28<01:01, 103.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  81% 27900/34255 [04:28<01:01, 103.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 27920/34255 [04:28<01:00, 104.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 27940/34255 [04:28<01:00, 104.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 27960/34255 [04:28<01:00, 104.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 27980/34255 [04:28<01:00, 104.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28000/34255 [04:28<01:00, 104.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28020/34255 [04:28<00:59, 104.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28040/34255 [04:28<00:59, 104.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28060/34255 [04:29<00:59, 104.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28080/34255 [04:29<00:59, 104.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28100/34255 [04:29<00:58, 104.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28120/34255 [04:29<00:58, 104.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28140/34255 [04:29<00:58, 104.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28160/34255 [04:29<00:58, 104.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28180/34255 [04:29<00:58, 104.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28200/34255 [04:29<00:57, 104.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28220/34255 [04:29<00:57, 104.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28240/34255 [04:29<00:57, 104.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  82% 28260/34255 [04:29<00:57, 104.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28280/34255 [04:30<00:57, 104.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28300/34255 [04:30<00:56, 104.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28320/34255 [04:30<00:56, 104.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28340/34255 [04:30<00:56, 104.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28360/34255 [04:30<00:56, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28380/34255 [04:30<00:55, 104.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28400/34255 [04:30<00:55, 104.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28420/34255 [04:30<00:55, 105.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28440/34255 [04:30<00:55, 105.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28460/34255 [04:30<00:55, 105.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28480/34255 [04:30<00:54, 105.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28500/34255 [04:30<00:54, 105.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28520/34255 [04:31<00:54, 105.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28540/34255 [04:31<00:54, 105.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28560/34255 [04:31<00:54, 105.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28580/34255 [04:31<00:53, 105.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  83% 28600/34255 [04:31<00:53, 105.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28620/34255 [04:31<00:53, 105.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28640/34255 [04:31<00:53, 105.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28660/34255 [04:31<00:53, 105.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28680/34255 [04:31<00:52, 105.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28700/34255 [04:31<00:52, 105.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28720/34255 [04:31<00:52, 105.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28740/34255 [04:32<00:52, 105.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28760/34255 [04:32<00:51, 105.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28780/34255 [04:32<00:51, 105.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28800/34255 [04:32<00:51, 105.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28820/34255 [04:32<00:51, 105.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28840/34255 [04:32<00:51, 105.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28860/34255 [04:32<00:50, 105.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28880/34255 [04:32<00:50, 105.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28900/34255 [04:32<00:50, 105.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28920/34255 [04:32<00:50, 106.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  84% 28940/34255 [04:32<00:50, 106.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 28960/34255 [04:33<00:49, 106.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 28980/34255 [04:33<00:49, 106.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29000/34255 [04:33<00:49, 106.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29020/34255 [04:33<00:49, 106.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29040/34255 [04:33<00:49, 106.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29060/34255 [04:33<00:48, 106.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29080/34255 [04:33<00:48, 106.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29100/34255 [04:33<00:48, 106.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29120/34255 [04:33<00:48, 106.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29140/34255 [04:33<00:48, 106.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29160/34255 [04:33<00:47, 106.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29180/34255 [04:33<00:47, 106.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29200/34255 [04:34<00:47, 106.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29220/34255 [04:34<00:47, 106.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29240/34255 [04:34<00:47, 106.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29260/34255 [04:34<00:46, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  85% 29280/34255 [04:34<00:46, 106.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29300/34255 [04:34<00:46, 106.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29320/34255 [04:34<00:46, 106.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29340/34255 [04:34<00:46, 106.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29360/34255 [04:34<00:45, 106.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29380/34255 [04:34<00:45, 106.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29400/34255 [04:34<00:45, 106.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29420/34255 [04:35<00:45, 106.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29440/34255 [04:35<00:44, 107.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29460/34255 [04:35<00:44, 107.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29480/34255 [04:35<00:44, 107.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29500/34255 [04:35<00:44, 107.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29520/34255 [04:35<00:44, 107.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29540/34255 [04:35<00:43, 107.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29560/34255 [04:35<00:43, 107.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29580/34255 [04:35<00:43, 107.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29600/34255 [04:35<00:43, 107.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  86% 29620/34255 [04:35<00:43, 107.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29640/34255 [04:35<00:42, 107.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29660/34255 [04:36<00:42, 107.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29680/34255 [04:36<00:42, 107.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29700/34255 [04:36<00:42, 107.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29720/34255 [04:36<00:42, 107.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29740/34255 [04:36<00:41, 107.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29760/34255 [04:36<00:41, 107.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29780/34255 [04:36<00:41, 107.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29800/34255 [04:36<00:41, 107.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29820/34255 [04:36<00:41, 107.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29840/34255 [04:36<00:40, 107.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29860/34255 [04:36<00:40, 107.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29880/34255 [04:36<00:40, 107.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29900/34255 [04:37<00:40, 107.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29920/34255 [04:37<00:40, 107.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29940/34255 [04:37<00:39, 108.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  87% 29960/34255 [04:37<00:39, 108.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 29980/34255 [04:37<00:39, 108.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30000/34255 [04:37<00:39, 108.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30020/34255 [04:37<00:39, 108.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30040/34255 [04:37<00:38, 108.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30060/34255 [04:37<00:38, 108.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30080/34255 [04:37<00:38, 108.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30100/34255 [04:37<00:38, 108.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30120/34255 [04:37<00:38, 108.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30140/34255 [04:38<00:37, 108.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30160/34255 [04:38<00:37, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30180/34255 [04:38<00:37, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30200/34255 [04:38<00:37, 108.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30220/34255 [04:38<00:37, 108.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30240/34255 [04:38<00:36, 108.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30260/34255 [04:38<00:36, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30280/34255 [04:38<00:36, 108.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  88% 30300/34255 [04:38<00:36, 108.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30320/34255 [04:38<00:36, 108.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30340/34255 [04:38<00:35, 108.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30360/34255 [04:38<00:35, 108.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30380/34255 [04:39<00:35, 108.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30400/34255 [04:39<00:35, 108.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30420/34255 [04:39<00:35, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30440/34255 [04:39<00:35, 108.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30460/34255 [04:39<00:34, 109.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30480/34255 [04:39<00:34, 109.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30500/34255 [04:39<00:34, 109.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30520/34255 [04:39<00:34, 109.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30540/34255 [04:39<00:34, 109.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30560/34255 [04:39<00:33, 109.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30580/34255 [04:39<00:33, 109.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30600/34255 [04:39<00:33, 109.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30620/34255 [04:40<00:33, 109.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  89% 30640/34255 [04:40<00:33, 109.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30660/34255 [04:40<00:32, 109.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30680/34255 [04:40<00:32, 109.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30700/34255 [04:40<00:32, 109.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30720/34255 [04:40<00:32, 109.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30740/34255 [04:40<00:32, 109.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30760/34255 [04:40<00:31, 109.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30780/34255 [04:40<00:31, 109.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30800/34255 [04:40<00:31, 109.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30820/34255 [04:40<00:31, 109.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30840/34255 [04:41<00:31, 109.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30860/34255 [04:41<00:30, 109.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30880/34255 [04:41<00:30, 109.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30900/34255 [04:41<00:30, 109.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30920/34255 [04:41<00:30, 109.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30940/34255 [04:41<00:30, 109.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30960/34255 [04:41<00:29, 109.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 30980/34255 [04:41<00:29, 110.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  90% 31000/34255 [04:41<00:29, 110.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31020/34255 [04:41<00:29, 110.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31040/34255 [04:41<00:29, 110.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31060/34255 [04:41<00:29, 110.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31080/34255 [04:42<00:28, 110.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31100/34255 [04:42<00:28, 110.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31120/34255 [04:42<00:28, 110.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31140/34255 [04:42<00:28, 110.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31160/34255 [04:42<00:28, 110.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31180/34255 [04:42<00:27, 110.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31200/34255 [04:42<00:27, 110.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31220/34255 [04:42<00:27, 110.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31240/34255 [04:42<00:27, 110.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31260/34255 [04:42<00:27, 110.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31280/34255 [04:42<00:26, 110.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31300/34255 [04:43<00:26, 110.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31320/34255 [04:43<00:26, 110.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  91% 31340/34255 [04:43<00:26, 110.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31360/34255 [04:43<00:26, 110.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31380/34255 [04:43<00:25, 110.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31400/34255 [04:43<00:25, 110.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31420/34255 [04:43<00:25, 110.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31440/34255 [04:43<00:25, 110.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31460/34255 [04:43<00:25, 110.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31480/34255 [04:43<00:25, 110.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31500/34255 [04:43<00:24, 110.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31520/34255 [04:44<00:24, 110.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31540/34255 [04:44<00:24, 111.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31560/34255 [04:44<00:24, 111.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31580/34255 [04:44<00:24, 111.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31600/34255 [04:44<00:23, 111.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31620/34255 [04:44<00:23, 111.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31640/34255 [04:44<00:23, 111.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31660/34255 [04:44<00:23, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  92% 31680/34255 [04:44<00:23, 111.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31700/34255 [04:44<00:22, 111.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31720/34255 [04:44<00:22, 111.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31740/34255 [04:45<00:22, 111.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31760/34255 [04:45<00:22, 111.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31780/34255 [04:45<00:22, 111.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31800/34255 [04:45<00:22, 111.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31820/34255 [04:45<00:21, 111.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31840/34255 [04:45<00:21, 111.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31860/34255 [04:45<00:21, 111.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31880/34255 [04:45<00:21, 111.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31900/34255 [04:45<00:21, 111.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31920/34255 [04:45<00:20, 111.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31940/34255 [04:45<00:20, 111.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31960/34255 [04:46<00:20, 111.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 31980/34255 [04:46<00:20, 111.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 32000/34255 [04:46<00:20, 111.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  93% 32020/34255 [04:46<00:19, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32040/34255 [04:46<00:19, 111.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32060/34255 [04:46<00:19, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32080/34255 [04:46<00:19, 111.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32100/34255 [04:46<00:19, 111.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32120/34255 [04:46<00:19, 112.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32140/34255 [04:46<00:18, 112.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32160/34255 [04:46<00:18, 112.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32180/34255 [04:46<00:18, 112.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32200/34255 [04:47<00:18, 112.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32220/34255 [04:47<00:18, 112.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32240/34255 [04:47<00:17, 112.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32260/34255 [04:47<00:17, 112.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32280/34255 [04:47<00:17, 112.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32300/34255 [04:47<00:17, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32320/34255 [04:47<00:17, 112.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32340/34255 [04:47<00:17, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  94% 32360/34255 [04:47<00:16, 112.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32380/34255 [04:47<00:16, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32400/34255 [04:47<00:16, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32420/34255 [04:48<00:16, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32440/34255 [04:48<00:16, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32460/34255 [04:48<00:15, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32480/34255 [04:48<00:15, 112.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32500/34255 [04:48<00:15, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32520/34255 [04:48<00:15, 112.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32540/34255 [04:48<00:15, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32560/34255 [04:48<00:15, 112.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32580/34255 [04:48<00:14, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32600/34255 [04:48<00:14, 112.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32620/34255 [04:48<00:14, 112.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32640/34255 [04:49<00:14, 112.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32660/34255 [04:49<00:14, 112.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32680/34255 [04:49<00:13, 112.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  95% 32700/34255 [04:49<00:13, 113.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32720/34255 [04:49<00:13, 113.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32740/34255 [04:49<00:13, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32760/34255 [04:49<00:13, 113.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32780/34255 [04:49<00:13, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32800/34255 [04:49<00:12, 113.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32820/34255 [04:49<00:12, 113.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32840/34255 [04:49<00:12, 113.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32860/34255 [04:50<00:12, 113.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32880/34255 [04:50<00:12, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32900/34255 [04:50<00:11, 113.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32920/34255 [04:50<00:11, 113.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32940/34255 [04:50<00:11, 113.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32960/34255 [04:50<00:11, 113.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 32980/34255 [04:50<00:11, 113.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 33000/34255 [04:50<00:11, 113.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 33020/34255 [04:50<00:10, 113.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  96% 33040/34255 [04:50<00:10, 113.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33060/34255 [04:50<00:10, 113.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33080/34255 [04:51<00:10, 113.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33100/34255 [04:51<00:10, 113.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33120/34255 [04:51<00:09, 113.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33140/34255 [04:51<00:09, 113.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33160/34255 [04:51<00:09, 113.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33180/34255 [04:51<00:09, 113.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33200/34255 [04:51<00:09, 113.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33220/34255 [04:51<00:09, 113.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33240/34255 [04:51<00:08, 113.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33260/34255 [04:51<00:08, 113.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33280/34255 [04:51<00:08, 114.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33300/34255 [04:51<00:08, 114.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33320/34255 [04:52<00:08, 114.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33340/34255 [04:52<00:08, 114.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33360/34255 [04:52<00:07, 114.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  97% 33380/34255 [04:52<00:07, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33400/34255 [04:52<00:07, 114.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33420/34255 [04:52<00:07, 114.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33440/34255 [04:52<00:07, 114.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33460/34255 [04:52<00:06, 114.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33480/34255 [04:52<00:06, 114.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33500/34255 [04:52<00:06, 114.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33520/34255 [04:52<00:06, 114.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33540/34255 [04:53<00:06, 114.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33560/34255 [04:53<00:06, 114.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33580/34255 [04:53<00:05, 114.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33600/34255 [04:53<00:05, 114.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33620/34255 [04:53<00:05, 114.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33640/34255 [04:53<00:05, 114.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33660/34255 [04:53<00:05, 114.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33680/34255 [04:53<00:05, 114.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33700/34255 [04:53<00:04, 114.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33720/34255 [04:53<00:04, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  98% 33740/34255 [04:53<00:04, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33760/34255 [04:53<00:04, 114.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33780/34255 [04:54<00:04, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33800/34255 [04:54<00:03, 114.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33820/34255 [04:54<00:03, 114.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33840/34255 [04:54<00:03, 114.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33860/34255 [04:54<00:03, 115.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33880/34255 [04:54<00:03, 115.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33900/34255 [04:54<00:03, 115.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33920/34255 [04:54<00:02, 115.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33940/34255 [04:54<00:02, 115.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33960/34255 [04:54<00:02, 115.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 33980/34255 [04:54<00:02, 115.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 34000/34255 [04:54<00:02, 115.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 34020/34255 [04:55<00:02, 115.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 34040/34255 [04:55<00:01, 115.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 34060/34255 [04:55<00:01, 115.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9:  99% 34080/34255 [04:55<00:01, 115.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34100/34255 [04:55<00:01, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34120/34255 [04:55<00:01, 115.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34140/34255 [04:55<00:00, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34160/34255 [04:55<00:00, 115.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34180/34255 [04:55<00:00, 115.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34200/34255 [04:55<00:00, 115.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34220/34255 [04:55<00:00, 115.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 9: 100% 34240/34255 [04:56<00:00, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 228.96it/s]\u001b[A\n",
            "Epoch 9: 100% 34255/34255 [04:56<00:00, 115.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27400/34255 [04:21<01:05, 104.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 27420/34255 [04:26<01:06, 102.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27440/34255 [04:26<01:06, 103.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27460/34255 [04:26<01:05, 103.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27480/34255 [04:26<01:05, 103.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27500/34255 [04:26<01:05, 103.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27520/34255 [04:26<01:05, 103.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27540/34255 [04:26<01:05, 103.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  80% 27560/34255 [04:26<01:04, 103.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27580/34255 [04:26<01:04, 103.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27600/34255 [04:27<01:04, 103.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27620/34255 [04:27<01:04, 103.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27640/34255 [04:27<01:03, 103.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27660/34255 [04:27<01:03, 103.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27680/34255 [04:27<01:03, 103.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27700/34255 [04:27<01:03, 103.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27720/34255 [04:27<01:03, 103.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27740/34255 [04:27<01:02, 103.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27760/34255 [04:27<01:02, 103.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27780/34255 [04:27<01:02, 103.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27800/34255 [04:27<01:02, 103.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27820/34255 [04:27<01:01, 103.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27840/34255 [04:28<01:01, 103.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27860/34255 [04:28<01:01, 103.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27880/34255 [04:28<01:01, 103.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  81% 27900/34255 [04:28<01:01, 103.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 27920/34255 [04:28<01:00, 104.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 27940/34255 [04:28<01:00, 104.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 27960/34255 [04:28<01:00, 104.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 27980/34255 [04:28<01:00, 104.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28000/34255 [04:28<01:00, 104.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28020/34255 [04:28<00:59, 104.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28040/34255 [04:28<00:59, 104.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28060/34255 [04:28<00:59, 104.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28080/34255 [04:29<00:59, 104.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28100/34255 [04:29<00:58, 104.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28120/34255 [04:29<00:58, 104.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28140/34255 [04:29<00:58, 104.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28160/34255 [04:29<00:58, 104.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28180/34255 [04:29<00:58, 104.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28200/34255 [04:29<00:57, 104.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28220/34255 [04:29<00:57, 104.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28240/34255 [04:29<00:57, 104.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  82% 28260/34255 [04:29<00:57, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28280/34255 [04:29<00:57, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28300/34255 [04:30<00:56, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28320/34255 [04:30<00:56, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28340/34255 [04:30<00:56, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28360/34255 [04:30<00:56, 104.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28380/34255 [04:30<00:55, 104.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28400/34255 [04:30<00:55, 105.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28420/34255 [04:30<00:55, 105.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28440/34255 [04:30<00:55, 105.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28460/34255 [04:30<00:55, 105.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28480/34255 [04:30<00:54, 105.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28500/34255 [04:30<00:54, 105.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28520/34255 [04:30<00:54, 105.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28540/34255 [04:31<00:54, 105.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28560/34255 [04:31<00:54, 105.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28580/34255 [04:31<00:53, 105.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  83% 28600/34255 [04:31<00:53, 105.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28620/34255 [04:31<00:53, 105.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28640/34255 [04:31<00:53, 105.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28660/34255 [04:31<00:53, 105.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28680/34255 [04:31<00:52, 105.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28700/34255 [04:31<00:52, 105.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28720/34255 [04:31<00:52, 105.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28740/34255 [04:31<00:52, 105.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28760/34255 [04:32<00:51, 105.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28780/34255 [04:32<00:51, 105.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28800/34255 [04:32<00:51, 105.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28820/34255 [04:32<00:51, 105.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28840/34255 [04:32<00:51, 105.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28860/34255 [04:32<00:50, 105.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28880/34255 [04:32<00:50, 105.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28900/34255 [04:32<00:50, 106.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28920/34255 [04:32<00:50, 106.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  84% 28940/34255 [04:32<00:50, 106.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 28960/34255 [04:32<00:49, 106.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 28980/34255 [04:32<00:49, 106.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29000/34255 [04:33<00:49, 106.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29020/34255 [04:33<00:49, 106.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29040/34255 [04:33<00:49, 106.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29060/34255 [04:33<00:48, 106.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29080/34255 [04:33<00:48, 106.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29100/34255 [04:33<00:48, 106.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29120/34255 [04:33<00:48, 106.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29140/34255 [04:33<00:48, 106.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29160/34255 [04:33<00:47, 106.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29180/34255 [04:33<00:47, 106.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29200/34255 [04:33<00:47, 106.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29220/34255 [04:33<00:47, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29240/34255 [04:34<00:46, 106.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29260/34255 [04:34<00:46, 106.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  85% 29280/34255 [04:34<00:46, 106.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29300/34255 [04:34<00:46, 106.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29320/34255 [04:34<00:46, 106.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29340/34255 [04:34<00:45, 106.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29360/34255 [04:34<00:45, 106.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29380/34255 [04:34<00:45, 106.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29400/34255 [04:34<00:45, 107.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29420/34255 [04:34<00:45, 107.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29440/34255 [04:34<00:44, 107.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29460/34255 [04:34<00:44, 107.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29480/34255 [04:35<00:44, 107.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29500/34255 [04:35<00:44, 107.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29520/34255 [04:35<00:44, 107.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29540/34255 [04:35<00:43, 107.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29560/34255 [04:35<00:43, 107.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29580/34255 [04:35<00:43, 107.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29600/34255 [04:35<00:43, 107.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  86% 29620/34255 [04:35<00:43, 107.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29640/34255 [04:35<00:42, 107.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29660/34255 [04:35<00:42, 107.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29680/34255 [04:35<00:42, 107.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29700/34255 [04:35<00:42, 107.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29720/34255 [04:36<00:42, 107.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29740/34255 [04:36<00:41, 107.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29760/34255 [04:36<00:41, 107.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29780/34255 [04:36<00:41, 107.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29800/34255 [04:36<00:41, 107.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29820/34255 [04:36<00:41, 107.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29840/34255 [04:36<00:40, 107.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29860/34255 [04:36<00:40, 107.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29880/34255 [04:36<00:40, 107.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29900/34255 [04:36<00:40, 108.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29920/34255 [04:36<00:40, 108.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29940/34255 [04:36<00:39, 108.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  87% 29960/34255 [04:37<00:39, 108.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 29980/34255 [04:37<00:39, 108.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30000/34255 [04:37<00:39, 108.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30020/34255 [04:37<00:39, 108.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30040/34255 [04:37<00:38, 108.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30060/34255 [04:37<00:38, 108.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30080/34255 [04:37<00:38, 108.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30100/34255 [04:37<00:38, 108.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30120/34255 [04:37<00:38, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30140/34255 [04:37<00:37, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30160/34255 [04:37<00:37, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30180/34255 [04:37<00:37, 108.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30200/34255 [04:38<00:37, 108.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30220/34255 [04:38<00:37, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30240/34255 [04:38<00:36, 108.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30260/34255 [04:38<00:36, 108.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30280/34255 [04:38<00:36, 108.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  88% 30300/34255 [04:38<00:36, 108.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30320/34255 [04:38<00:36, 108.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30340/34255 [04:38<00:35, 108.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30360/34255 [04:38<00:35, 108.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30380/34255 [04:38<00:35, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30400/34255 [04:38<00:35, 108.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30420/34255 [04:39<00:35, 109.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30440/34255 [04:39<00:34, 109.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30460/34255 [04:39<00:34, 109.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30480/34255 [04:39<00:34, 109.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30500/34255 [04:39<00:34, 109.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30520/34255 [04:39<00:34, 109.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30540/34255 [04:39<00:34, 109.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30560/34255 [04:39<00:33, 109.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30580/34255 [04:39<00:33, 109.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30600/34255 [04:39<00:33, 109.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30620/34255 [04:39<00:33, 109.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  89% 30640/34255 [04:39<00:33, 109.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30660/34255 [04:40<00:32, 109.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30680/34255 [04:40<00:32, 109.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30700/34255 [04:40<00:32, 109.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30720/34255 [04:40<00:32, 109.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30740/34255 [04:40<00:32, 109.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30760/34255 [04:40<00:31, 109.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30780/34255 [04:40<00:31, 109.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30800/34255 [04:40<00:31, 109.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30820/34255 [04:40<00:31, 109.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30840/34255 [04:40<00:31, 109.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30860/34255 [04:40<00:30, 109.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30880/34255 [04:40<00:30, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30900/34255 [04:41<00:30, 109.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30920/34255 [04:41<00:30, 109.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30940/34255 [04:41<00:30, 110.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30960/34255 [04:41<00:29, 110.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 30980/34255 [04:41<00:29, 110.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  90% 31000/34255 [04:41<00:29, 110.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31020/34255 [04:41<00:29, 110.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31040/34255 [04:41<00:29, 110.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31060/34255 [04:41<00:28, 110.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31080/34255 [04:41<00:28, 110.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31100/34255 [04:41<00:28, 110.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31120/34255 [04:42<00:28, 110.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31140/34255 [04:42<00:28, 110.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31160/34255 [04:42<00:28, 110.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31180/34255 [04:42<00:27, 110.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31200/34255 [04:42<00:27, 110.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31220/34255 [04:42<00:27, 110.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31240/34255 [04:42<00:27, 110.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31260/34255 [04:42<00:27, 110.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31280/34255 [04:42<00:26, 110.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31300/34255 [04:42<00:26, 110.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31320/34255 [04:42<00:26, 110.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  91% 31340/34255 [04:43<00:26, 110.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31360/34255 [04:43<00:26, 110.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31380/34255 [04:43<00:25, 110.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31400/34255 [04:43<00:25, 110.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31420/34255 [04:43<00:25, 110.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31440/34255 [04:43<00:25, 110.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31460/34255 [04:43<00:25, 110.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31480/34255 [04:43<00:25, 110.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31500/34255 [04:43<00:24, 111.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31520/34255 [04:43<00:24, 111.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31540/34255 [04:43<00:24, 111.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31560/34255 [04:44<00:24, 111.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31580/34255 [04:44<00:24, 111.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31600/34255 [04:44<00:23, 111.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31620/34255 [04:44<00:23, 111.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31640/34255 [04:44<00:23, 111.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31660/34255 [04:44<00:23, 111.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  92% 31680/34255 [04:44<00:23, 111.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31700/34255 [04:44<00:22, 111.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31720/34255 [04:44<00:22, 111.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31740/34255 [04:44<00:22, 111.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31760/34255 [04:44<00:22, 111.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31780/34255 [04:44<00:22, 111.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31800/34255 [04:45<00:22, 111.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31820/34255 [04:45<00:21, 111.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31840/34255 [04:45<00:21, 111.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31860/34255 [04:45<00:21, 111.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31880/34255 [04:45<00:21, 111.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31900/34255 [04:45<00:21, 111.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31920/34255 [04:45<00:20, 111.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31940/34255 [04:45<00:20, 111.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31960/34255 [04:45<00:20, 111.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 31980/34255 [04:45<00:20, 111.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 32000/34255 [04:45<00:20, 111.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  93% 32020/34255 [04:46<00:19, 111.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32040/34255 [04:46<00:19, 111.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32060/34255 [04:46<00:19, 112.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32080/34255 [04:46<00:19, 112.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32100/34255 [04:46<00:19, 112.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32120/34255 [04:46<00:19, 112.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32140/34255 [04:46<00:18, 112.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32160/34255 [04:46<00:18, 112.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32180/34255 [04:46<00:18, 112.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32200/34255 [04:46<00:18, 112.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32220/34255 [04:46<00:18, 112.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32240/34255 [04:47<00:17, 112.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32260/34255 [04:47<00:17, 112.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32280/34255 [04:47<00:17, 112.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32300/34255 [04:47<00:17, 112.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32320/34255 [04:47<00:17, 112.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32340/34255 [04:47<00:17, 112.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  94% 32360/34255 [04:47<00:16, 112.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32380/34255 [04:47<00:16, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32400/34255 [04:47<00:16, 112.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32420/34255 [04:47<00:16, 112.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32440/34255 [04:47<00:16, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32460/34255 [04:47<00:15, 112.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32480/34255 [04:48<00:15, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32500/34255 [04:48<00:15, 112.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32520/34255 [04:48<00:15, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32540/34255 [04:48<00:15, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32560/34255 [04:48<00:15, 112.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32580/34255 [04:48<00:14, 112.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32600/34255 [04:48<00:14, 112.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32620/34255 [04:48<00:14, 113.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32640/34255 [04:48<00:14, 113.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32660/34255 [04:48<00:14, 113.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32680/34255 [04:48<00:13, 113.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  95% 32700/34255 [04:49<00:13, 113.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32720/34255 [04:49<00:13, 113.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32740/34255 [04:49<00:13, 113.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32760/34255 [04:49<00:13, 113.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32780/34255 [04:49<00:13, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32800/34255 [04:49<00:12, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32820/34255 [04:49<00:12, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32840/34255 [04:49<00:12, 113.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32860/34255 [04:49<00:12, 113.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32880/34255 [04:49<00:12, 113.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32900/34255 [04:49<00:11, 113.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32920/34255 [04:49<00:11, 113.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32940/34255 [04:50<00:11, 113.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32960/34255 [04:50<00:11, 113.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 32980/34255 [04:50<00:11, 113.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 33000/34255 [04:50<00:11, 113.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 33020/34255 [04:50<00:10, 113.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  96% 33040/34255 [04:50<00:10, 113.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33060/34255 [04:50<00:10, 113.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33080/34255 [04:50<00:10, 113.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33100/34255 [04:50<00:10, 113.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33120/34255 [04:50<00:09, 113.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33140/34255 [04:50<00:09, 113.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33160/34255 [04:51<00:09, 113.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33180/34255 [04:51<00:09, 113.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33200/34255 [04:51<00:09, 114.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33220/34255 [04:51<00:09, 114.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33240/34255 [04:51<00:08, 114.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33260/34255 [04:51<00:08, 114.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33280/34255 [04:51<00:08, 114.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33300/34255 [04:51<00:08, 114.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33320/34255 [04:51<00:08, 114.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33340/34255 [04:51<00:08, 114.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33360/34255 [04:51<00:07, 114.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  97% 33380/34255 [04:51<00:07, 114.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33400/34255 [04:52<00:07, 114.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33420/34255 [04:52<00:07, 114.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33440/34255 [04:52<00:07, 114.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33460/34255 [04:52<00:06, 114.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33480/34255 [04:52<00:06, 114.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33500/34255 [04:52<00:06, 114.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33520/34255 [04:52<00:06, 114.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33540/34255 [04:52<00:06, 114.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33560/34255 [04:52<00:06, 114.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33580/34255 [04:52<00:05, 114.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33600/34255 [04:52<00:05, 114.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33620/34255 [04:53<00:05, 114.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33640/34255 [04:53<00:05, 114.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33660/34255 [04:53<00:05, 114.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33680/34255 [04:53<00:05, 114.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33700/34255 [04:53<00:04, 114.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33720/34255 [04:53<00:04, 114.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  98% 33740/34255 [04:53<00:04, 114.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33760/34255 [04:53<00:04, 114.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33780/34255 [04:53<00:04, 115.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33800/34255 [04:53<00:03, 115.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33820/34255 [04:53<00:03, 115.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33840/34255 [04:53<00:03, 115.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33860/34255 [04:54<00:03, 115.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33880/34255 [04:54<00:03, 115.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33900/34255 [04:54<00:03, 115.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33920/34255 [04:54<00:02, 115.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33940/34255 [04:54<00:02, 115.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33960/34255 [04:54<00:02, 115.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 33980/34255 [04:54<00:02, 115.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 34000/34255 [04:54<00:02, 115.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 34020/34255 [04:54<00:02, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 34040/34255 [04:54<00:01, 115.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 34060/34255 [04:54<00:01, 115.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10:  99% 34080/34255 [04:55<00:01, 115.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34100/34255 [04:55<00:01, 115.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34120/34255 [04:55<00:01, 115.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34140/34255 [04:55<00:00, 115.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34160/34255 [04:55<00:00, 115.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34180/34255 [04:55<00:00, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34200/34255 [04:55<00:00, 115.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34220/34255 [04:55<00:00, 115.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 10: 100% 34240/34255 [04:55<00:00, 115.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 231.03it/s]\u001b[A\n",
            "Epoch 10: 100% 34255/34255 [04:55<00:00, 115.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27400/34255 [04:21<01:05, 104.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 27420/34255 [04:25<01:06, 103.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27440/34255 [04:26<01:06, 103.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27460/34255 [04:26<01:05, 103.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27480/34255 [04:26<01:05, 103.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27500/34255 [04:26<01:05, 103.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27520/34255 [04:26<01:05, 103.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27540/34255 [04:26<01:04, 103.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  80% 27560/34255 [04:26<01:04, 103.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27580/34255 [04:26<01:04, 103.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27600/34255 [04:26<01:04, 103.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27620/34255 [04:26<01:04, 103.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27640/34255 [04:26<01:03, 103.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27660/34255 [04:27<01:03, 103.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27680/34255 [04:27<01:03, 103.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27700/34255 [04:27<01:03, 103.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27720/34255 [04:27<01:03, 103.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27740/34255 [04:27<01:02, 103.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27760/34255 [04:27<01:02, 103.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27780/34255 [04:27<01:02, 103.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27800/34255 [04:27<01:02, 103.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27820/34255 [04:27<01:01, 103.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27840/34255 [04:27<01:01, 103.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27860/34255 [04:27<01:01, 104.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27880/34255 [04:27<01:01, 104.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  81% 27900/34255 [04:28<01:01, 104.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 27920/34255 [04:28<01:00, 104.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 27940/34255 [04:28<01:00, 104.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 27960/34255 [04:28<01:00, 104.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 27980/34255 [04:28<01:00, 104.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28000/34255 [04:28<00:59, 104.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28020/34255 [04:28<00:59, 104.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28040/34255 [04:28<00:59, 104.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28060/34255 [04:28<00:59, 104.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28080/34255 [04:28<00:59, 104.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28100/34255 [04:28<00:58, 104.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28120/34255 [04:29<00:58, 104.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28140/34255 [04:29<00:58, 104.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28160/34255 [04:29<00:58, 104.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28180/34255 [04:29<00:58, 104.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28200/34255 [04:29<00:57, 104.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28220/34255 [04:29<00:57, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28240/34255 [04:29<00:57, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  82% 28260/34255 [04:29<00:57, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28280/34255 [04:29<00:56, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28300/34255 [04:29<00:56, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28320/34255 [04:29<00:56, 104.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28340/34255 [04:29<00:56, 104.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28360/34255 [04:30<00:56, 105.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28380/34255 [04:30<00:55, 105.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28400/34255 [04:30<00:55, 105.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28420/34255 [04:30<00:55, 105.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28440/34255 [04:30<00:55, 105.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28460/34255 [04:30<00:55, 105.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28480/34255 [04:30<00:54, 105.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28500/34255 [04:30<00:54, 105.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28520/34255 [04:30<00:54, 105.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28540/34255 [04:30<00:54, 105.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28560/34255 [04:30<00:54, 105.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28580/34255 [04:31<00:53, 105.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  83% 28600/34255 [04:31<00:53, 105.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28620/34255 [04:31<00:53, 105.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28640/34255 [04:31<00:53, 105.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28660/34255 [04:31<00:52, 105.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28680/34255 [04:31<00:52, 105.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28700/34255 [04:31<00:52, 105.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28720/34255 [04:31<00:52, 105.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28740/34255 [04:31<00:52, 105.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28760/34255 [04:31<00:51, 105.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28780/34255 [04:31<00:51, 105.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28800/34255 [04:32<00:51, 105.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28820/34255 [04:32<00:51, 105.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28840/34255 [04:32<00:51, 105.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28860/34255 [04:32<00:50, 106.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28880/34255 [04:32<00:50, 106.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28900/34255 [04:32<00:50, 106.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28920/34255 [04:32<00:50, 106.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  84% 28940/34255 [04:32<00:50, 106.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 28960/34255 [04:32<00:49, 106.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 28980/34255 [04:32<00:49, 106.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29000/34255 [04:32<00:49, 106.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29020/34255 [04:32<00:49, 106.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29040/34255 [04:33<00:49, 106.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29060/34255 [04:33<00:48, 106.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29080/34255 [04:33<00:48, 106.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29100/34255 [04:33<00:48, 106.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29120/34255 [04:33<00:48, 106.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29140/34255 [04:33<00:48, 106.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29160/34255 [04:33<00:47, 106.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29180/34255 [04:33<00:47, 106.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29200/34255 [04:33<00:47, 106.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29220/34255 [04:33<00:47, 106.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29240/34255 [04:33<00:46, 106.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29260/34255 [04:34<00:46, 106.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  85% 29280/34255 [04:34<00:46, 106.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29300/34255 [04:34<00:46, 106.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29320/34255 [04:34<00:46, 106.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29340/34255 [04:34<00:45, 106.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29360/34255 [04:34<00:45, 106.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29380/34255 [04:34<00:45, 107.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29400/34255 [04:34<00:45, 107.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29420/34255 [04:34<00:45, 107.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29440/34255 [04:34<00:44, 107.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29460/34255 [04:34<00:44, 107.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29480/34255 [04:34<00:44, 107.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29500/34255 [04:35<00:44, 107.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29520/34255 [04:35<00:44, 107.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29540/34255 [04:35<00:43, 107.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29560/34255 [04:35<00:43, 107.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29580/34255 [04:35<00:43, 107.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29600/34255 [04:35<00:43, 107.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  86% 29620/34255 [04:35<00:43, 107.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29640/34255 [04:35<00:42, 107.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29660/34255 [04:35<00:42, 107.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29680/34255 [04:35<00:42, 107.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29700/34255 [04:35<00:42, 107.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29720/34255 [04:36<00:42, 107.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29740/34255 [04:36<00:41, 107.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29760/34255 [04:36<00:41, 107.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29780/34255 [04:36<00:41, 107.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29800/34255 [04:36<00:41, 107.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29820/34255 [04:36<00:41, 107.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29840/34255 [04:36<00:40, 107.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29860/34255 [04:36<00:40, 107.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29880/34255 [04:36<00:40, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29900/34255 [04:36<00:40, 107.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29920/34255 [04:36<00:40, 108.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29940/34255 [04:37<00:39, 108.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  87% 29960/34255 [04:37<00:39, 108.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 29980/34255 [04:37<00:39, 108.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30000/34255 [04:37<00:39, 108.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30020/34255 [04:37<00:39, 108.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30040/34255 [04:37<00:38, 108.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30060/34255 [04:37<00:38, 108.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30080/34255 [04:37<00:38, 108.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30100/34255 [04:37<00:38, 108.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30120/34255 [04:37<00:38, 108.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30140/34255 [04:37<00:37, 108.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30160/34255 [04:38<00:37, 108.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30180/34255 [04:38<00:37, 108.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30200/34255 [04:38<00:37, 108.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30220/34255 [04:38<00:37, 108.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30240/34255 [04:38<00:36, 108.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30260/34255 [04:38<00:36, 108.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30280/34255 [04:38<00:36, 108.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  88% 30300/34255 [04:38<00:36, 108.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30320/34255 [04:38<00:36, 108.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30340/34255 [04:38<00:35, 108.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30360/34255 [04:38<00:35, 108.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30380/34255 [04:39<00:35, 108.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30400/34255 [04:39<00:35, 108.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30420/34255 [04:39<00:35, 108.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30440/34255 [04:39<00:34, 109.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30460/34255 [04:39<00:34, 109.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30480/34255 [04:39<00:34, 109.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30500/34255 [04:39<00:34, 109.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30520/34255 [04:39<00:34, 109.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30540/34255 [04:39<00:34, 109.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30560/34255 [04:39<00:33, 109.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30580/34255 [04:39<00:33, 109.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30600/34255 [04:39<00:33, 109.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30620/34255 [04:40<00:33, 109.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  89% 30640/34255 [04:40<00:33, 109.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30660/34255 [04:40<00:32, 109.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30680/34255 [04:40<00:32, 109.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30700/34255 [04:40<00:32, 109.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30720/34255 [04:40<00:32, 109.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30740/34255 [04:40<00:32, 109.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30760/34255 [04:40<00:31, 109.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30780/34255 [04:40<00:31, 109.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30800/34255 [04:40<00:31, 109.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30820/34255 [04:40<00:31, 109.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30840/34255 [04:40<00:31, 109.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30860/34255 [04:41<00:30, 109.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30880/34255 [04:41<00:30, 109.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30900/34255 [04:41<00:30, 109.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30920/34255 [04:41<00:30, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30940/34255 [04:41<00:30, 109.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30960/34255 [04:41<00:29, 109.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 30980/34255 [04:41<00:29, 110.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  90% 31000/34255 [04:41<00:29, 110.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31020/34255 [04:41<00:29, 110.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31040/34255 [04:41<00:29, 110.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31060/34255 [04:41<00:28, 110.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31080/34255 [04:42<00:28, 110.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31100/34255 [04:42<00:28, 110.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31120/34255 [04:42<00:28, 110.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31140/34255 [04:42<00:28, 110.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31160/34255 [04:42<00:28, 110.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31180/34255 [04:42<00:27, 110.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31200/34255 [04:42<00:27, 110.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31220/34255 [04:42<00:27, 110.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31240/34255 [04:42<00:27, 110.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31260/34255 [04:42<00:27, 110.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31280/34255 [04:42<00:26, 110.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31300/34255 [04:42<00:26, 110.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31320/34255 [04:43<00:26, 110.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  91% 31340/34255 [04:43<00:26, 110.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31360/34255 [04:43<00:26, 110.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31380/34255 [04:43<00:25, 110.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31400/34255 [04:43<00:25, 110.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31420/34255 [04:43<00:25, 110.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31440/34255 [04:43<00:25, 110.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31460/34255 [04:43<00:25, 110.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31480/34255 [04:43<00:25, 110.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31500/34255 [04:43<00:24, 111.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31520/34255 [04:43<00:24, 111.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31540/34255 [04:43<00:24, 111.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31560/34255 [04:44<00:24, 111.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31580/34255 [04:44<00:24, 111.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31600/34255 [04:44<00:23, 111.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31620/34255 [04:44<00:23, 111.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31640/34255 [04:44<00:23, 111.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31660/34255 [04:44<00:23, 111.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  92% 31680/34255 [04:44<00:23, 111.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31700/34255 [04:44<00:22, 111.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31720/34255 [04:44<00:22, 111.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31740/34255 [04:44<00:22, 111.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31760/34255 [04:44<00:22, 111.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31780/34255 [04:44<00:22, 111.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31800/34255 [04:45<00:22, 111.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31820/34255 [04:45<00:21, 111.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31840/34255 [04:45<00:21, 111.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31860/34255 [04:45<00:21, 111.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31880/34255 [04:45<00:21, 111.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31900/34255 [04:45<00:21, 111.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31920/34255 [04:45<00:20, 111.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31940/34255 [04:45<00:20, 111.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31960/34255 [04:45<00:20, 111.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 31980/34255 [04:45<00:20, 111.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 32000/34255 [04:45<00:20, 111.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  93% 32020/34255 [04:46<00:19, 111.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32040/34255 [04:46<00:19, 111.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32060/34255 [04:46<00:19, 112.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32080/34255 [04:46<00:19, 112.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32100/34255 [04:46<00:19, 112.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32120/34255 [04:46<00:19, 112.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32140/34255 [04:46<00:18, 112.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32160/34255 [04:46<00:18, 112.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32180/34255 [04:46<00:18, 112.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32200/34255 [04:46<00:18, 112.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32220/34255 [04:46<00:18, 112.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32240/34255 [04:46<00:17, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32260/34255 [04:47<00:17, 112.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32280/34255 [04:47<00:17, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32300/34255 [04:47<00:17, 112.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32320/34255 [04:47<00:17, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32340/34255 [04:47<00:17, 112.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  94% 32360/34255 [04:47<00:16, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32380/34255 [04:47<00:16, 112.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32400/34255 [04:47<00:16, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32420/34255 [04:47<00:16, 112.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32440/34255 [04:47<00:16, 112.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32460/34255 [04:47<00:15, 112.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32480/34255 [04:48<00:15, 112.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32500/34255 [04:48<00:15, 112.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32520/34255 [04:48<00:15, 112.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32540/34255 [04:48<00:15, 112.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32560/34255 [04:48<00:15, 112.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32580/34255 [04:48<00:14, 112.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32600/34255 [04:48<00:14, 112.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32620/34255 [04:48<00:14, 113.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32640/34255 [04:48<00:14, 113.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32660/34255 [04:48<00:14, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32680/34255 [04:48<00:13, 113.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  95% 32700/34255 [04:48<00:13, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32720/34255 [04:49<00:13, 113.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32740/34255 [04:49<00:13, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32760/34255 [04:49<00:13, 113.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32780/34255 [04:49<00:13, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32800/34255 [04:49<00:12, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32820/34255 [04:49<00:12, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32840/34255 [04:49<00:12, 113.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32860/34255 [04:49<00:12, 113.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32880/34255 [04:49<00:12, 113.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32900/34255 [04:49<00:11, 113.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32920/34255 [04:49<00:11, 113.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32940/34255 [04:50<00:11, 113.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32960/34255 [04:50<00:11, 113.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 32980/34255 [04:50<00:11, 113.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 33000/34255 [04:50<00:11, 113.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 33020/34255 [04:50<00:10, 113.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  96% 33040/34255 [04:50<00:10, 113.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33060/34255 [04:50<00:10, 113.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33080/34255 [04:50<00:10, 113.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33100/34255 [04:50<00:10, 113.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33120/34255 [04:50<00:09, 113.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33140/34255 [04:50<00:09, 113.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33160/34255 [04:51<00:09, 113.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33180/34255 [04:51<00:09, 113.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33200/34255 [04:51<00:09, 114.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33220/34255 [04:51<00:09, 114.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33240/34255 [04:51<00:08, 114.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33260/34255 [04:51<00:08, 114.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33280/34255 [04:51<00:08, 114.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33300/34255 [04:51<00:08, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33320/34255 [04:51<00:08, 114.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33340/34255 [04:51<00:08, 114.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33360/34255 [04:51<00:07, 114.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  97% 33380/34255 [04:51<00:07, 114.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33400/34255 [04:52<00:07, 114.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33420/34255 [04:52<00:07, 114.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33440/34255 [04:52<00:07, 114.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33460/34255 [04:52<00:06, 114.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33480/34255 [04:52<00:06, 114.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33500/34255 [04:52<00:06, 114.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33520/34255 [04:52<00:06, 114.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33540/34255 [04:52<00:06, 114.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33560/34255 [04:52<00:06, 114.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33580/34255 [04:52<00:05, 114.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33600/34255 [04:53<00:05, 114.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33620/34255 [04:53<00:05, 114.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33640/34255 [04:53<00:05, 114.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33660/34255 [04:53<00:05, 114.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33680/34255 [04:53<00:05, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33700/34255 [04:53<00:04, 114.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33720/34255 [04:53<00:04, 114.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  98% 33740/34255 [04:53<00:04, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33760/34255 [04:53<00:04, 114.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33780/34255 [04:53<00:04, 114.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33800/34255 [04:53<00:03, 115.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33820/34255 [04:54<00:03, 115.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33840/34255 [04:54<00:03, 115.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33860/34255 [04:54<00:03, 115.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33880/34255 [04:54<00:03, 115.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33900/34255 [04:54<00:03, 115.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33920/34255 [04:54<00:02, 115.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33940/34255 [04:54<00:02, 115.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33960/34255 [04:54<00:02, 115.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 33980/34255 [04:54<00:02, 115.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 34000/34255 [04:54<00:02, 115.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 34020/34255 [04:54<00:02, 115.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 34040/34255 [04:54<00:01, 115.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 34060/34255 [04:55<00:01, 115.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11:  99% 34080/34255 [04:55<00:01, 115.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34100/34255 [04:55<00:01, 115.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34120/34255 [04:55<00:01, 115.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34140/34255 [04:55<00:00, 115.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34160/34255 [04:55<00:00, 115.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34180/34255 [04:55<00:00, 115.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34200/34255 [04:55<00:00, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34220/34255 [04:55<00:00, 115.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 11: 100% 34240/34255 [04:55<00:00, 115.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 227.82it/s]\u001b[A\n",
            "Epoch 11: 100% 34255/34255 [04:55<00:00, 115.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27400/34255 [04:21<01:05, 104.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 27420/34255 [04:25<01:06, 103.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27440/34255 [04:25<01:06, 103.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27460/34255 [04:26<01:05, 103.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27480/34255 [04:26<01:05, 103.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27500/34255 [04:26<01:05, 103.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27520/34255 [04:26<01:05, 103.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27540/34255 [04:26<01:04, 103.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  80% 27560/34255 [04:26<01:04, 103.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27580/34255 [04:26<01:04, 103.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27600/34255 [04:26<01:04, 103.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27620/34255 [04:26<01:04, 103.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27640/34255 [04:26<01:03, 103.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27660/34255 [04:26<01:03, 103.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27680/34255 [04:27<01:03, 103.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27700/34255 [04:27<01:03, 103.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27720/34255 [04:27<01:02, 103.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27740/34255 [04:27<01:02, 103.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27760/34255 [04:27<01:02, 103.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27780/34255 [04:27<01:02, 103.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27800/34255 [04:27<01:02, 103.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27820/34255 [04:27<01:01, 103.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27840/34255 [04:27<01:01, 103.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27860/34255 [04:27<01:01, 104.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27880/34255 [04:27<01:01, 104.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  81% 27900/34255 [04:27<01:01, 104.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 27920/34255 [04:28<01:00, 104.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 27940/34255 [04:28<01:00, 104.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 27960/34255 [04:28<01:00, 104.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 27980/34255 [04:28<01:00, 104.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28000/34255 [04:28<00:59, 104.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28020/34255 [04:28<00:59, 104.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28040/34255 [04:28<00:59, 104.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28060/34255 [04:28<00:59, 104.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28080/34255 [04:28<00:59, 104.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28100/34255 [04:28<00:58, 104.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28120/34255 [04:29<00:58, 104.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28140/34255 [04:29<00:58, 104.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28160/34255 [04:29<00:58, 104.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28180/34255 [04:29<00:58, 104.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28200/34255 [04:29<00:57, 104.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28220/34255 [04:29<00:57, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28240/34255 [04:29<00:57, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  82% 28260/34255 [04:29<00:57, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28280/34255 [04:29<00:56, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28300/34255 [04:29<00:56, 104.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28320/34255 [04:29<00:56, 104.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28340/34255 [04:29<00:56, 104.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28360/34255 [04:30<00:56, 105.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28380/34255 [04:30<00:55, 105.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28400/34255 [04:30<00:55, 105.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28420/34255 [04:30<00:55, 105.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28440/34255 [04:30<00:55, 105.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28460/34255 [04:30<00:55, 105.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28480/34255 [04:30<00:54, 105.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28500/34255 [04:30<00:54, 105.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28520/34255 [04:30<00:54, 105.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28540/34255 [04:30<00:54, 105.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28560/34255 [04:31<00:54, 105.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28580/34255 [04:31<00:53, 105.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  83% 28600/34255 [04:31<00:53, 105.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28620/34255 [04:31<00:53, 105.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28640/34255 [04:31<00:53, 105.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28660/34255 [04:31<00:53, 105.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28680/34255 [04:31<00:52, 105.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28700/34255 [04:31<00:52, 105.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28720/34255 [04:31<00:52, 105.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28740/34255 [04:31<00:52, 105.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28760/34255 [04:32<00:51, 105.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28780/34255 [04:32<00:51, 105.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28800/34255 [04:32<00:51, 105.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28820/34255 [04:32<00:51, 105.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28840/34255 [04:32<00:51, 105.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28860/34255 [04:32<00:50, 105.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28880/34255 [04:32<00:50, 105.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28900/34255 [04:32<00:50, 106.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28920/34255 [04:32<00:50, 106.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  84% 28940/34255 [04:32<00:50, 106.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 28960/34255 [04:32<00:49, 106.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 28980/34255 [04:32<00:49, 106.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29000/34255 [04:33<00:49, 106.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29020/34255 [04:33<00:49, 106.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29040/34255 [04:33<00:49, 106.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29060/34255 [04:33<00:48, 106.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29080/34255 [04:33<00:48, 106.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29100/34255 [04:33<00:48, 106.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29120/34255 [04:33<00:48, 106.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29140/34255 [04:33<00:48, 106.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29160/34255 [04:33<00:47, 106.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29180/34255 [04:33<00:47, 106.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29200/34255 [04:33<00:47, 106.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29220/34255 [04:34<00:47, 106.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29240/34255 [04:34<00:47, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29260/34255 [04:34<00:46, 106.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  85% 29280/34255 [04:34<00:46, 106.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29300/34255 [04:34<00:46, 106.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29320/34255 [04:34<00:46, 106.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29340/34255 [04:34<00:45, 106.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29360/34255 [04:34<00:45, 106.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29380/34255 [04:34<00:45, 106.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29400/34255 [04:34<00:45, 106.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29420/34255 [04:34<00:45, 107.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29440/34255 [04:34<00:44, 107.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29460/34255 [04:35<00:44, 107.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29480/34255 [04:35<00:44, 107.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29500/34255 [04:35<00:44, 107.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29520/34255 [04:35<00:44, 107.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29540/34255 [04:35<00:43, 107.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29560/34255 [04:35<00:43, 107.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29580/34255 [04:35<00:43, 107.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29600/34255 [04:35<00:43, 107.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  86% 29620/34255 [04:35<00:43, 107.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29640/34255 [04:35<00:42, 107.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29660/34255 [04:35<00:42, 107.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29680/34255 [04:36<00:42, 107.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29700/34255 [04:36<00:42, 107.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29720/34255 [04:36<00:42, 107.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29740/34255 [04:36<00:41, 107.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29760/34255 [04:36<00:41, 107.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29780/34255 [04:36<00:41, 107.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29800/34255 [04:36<00:41, 107.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29820/34255 [04:36<00:41, 107.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29840/34255 [04:36<00:40, 107.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29860/34255 [04:36<00:40, 107.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29880/34255 [04:36<00:40, 107.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29900/34255 [04:36<00:40, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29920/34255 [04:37<00:40, 107.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29940/34255 [04:37<00:39, 108.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  87% 29960/34255 [04:37<00:39, 108.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 29980/34255 [04:37<00:39, 108.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30000/34255 [04:37<00:39, 108.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30020/34255 [04:37<00:39, 108.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30040/34255 [04:37<00:38, 108.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30060/34255 [04:37<00:38, 108.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30080/34255 [04:37<00:38, 108.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30100/34255 [04:37<00:38, 108.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30120/34255 [04:37<00:38, 108.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30140/34255 [04:37<00:37, 108.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30160/34255 [04:38<00:37, 108.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30180/34255 [04:38<00:37, 108.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30200/34255 [04:38<00:37, 108.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30220/34255 [04:38<00:37, 108.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30240/34255 [04:38<00:36, 108.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30260/34255 [04:38<00:36, 108.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30280/34255 [04:38<00:36, 108.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  88% 30300/34255 [04:38<00:36, 108.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30320/34255 [04:38<00:36, 108.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30340/34255 [04:38<00:35, 108.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30360/34255 [04:38<00:35, 108.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30380/34255 [04:38<00:35, 108.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30400/34255 [04:39<00:35, 108.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30420/34255 [04:39<00:35, 108.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30440/34255 [04:39<00:34, 109.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30460/34255 [04:39<00:34, 109.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30480/34255 [04:39<00:34, 109.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30500/34255 [04:39<00:34, 109.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30520/34255 [04:39<00:34, 109.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30540/34255 [04:39<00:34, 109.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30560/34255 [04:39<00:33, 109.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30580/34255 [04:39<00:33, 109.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30600/34255 [04:39<00:33, 109.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30620/34255 [04:40<00:33, 109.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  89% 30640/34255 [04:40<00:33, 109.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30660/34255 [04:40<00:32, 109.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30680/34255 [04:40<00:32, 109.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30700/34255 [04:40<00:32, 109.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30720/34255 [04:40<00:32, 109.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30740/34255 [04:40<00:32, 109.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30760/34255 [04:40<00:31, 109.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30780/34255 [04:40<00:31, 109.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30800/34255 [04:40<00:31, 109.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30820/34255 [04:40<00:31, 109.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30840/34255 [04:40<00:31, 109.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30860/34255 [04:41<00:30, 109.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30880/34255 [04:41<00:30, 109.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30900/34255 [04:41<00:30, 109.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30920/34255 [04:41<00:30, 109.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30940/34255 [04:41<00:30, 109.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30960/34255 [04:41<00:29, 110.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 30980/34255 [04:41<00:29, 110.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  90% 31000/34255 [04:41<00:29, 110.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31020/34255 [04:41<00:29, 110.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31040/34255 [04:41<00:29, 110.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31060/34255 [04:41<00:28, 110.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31080/34255 [04:41<00:28, 110.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31100/34255 [04:42<00:28, 110.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31120/34255 [04:42<00:28, 110.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31140/34255 [04:42<00:28, 110.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31160/34255 [04:42<00:28, 110.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31180/34255 [04:42<00:27, 110.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31200/34255 [04:42<00:27, 110.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31220/34255 [04:42<00:27, 110.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31240/34255 [04:42<00:27, 110.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31260/34255 [04:42<00:27, 110.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31280/34255 [04:42<00:26, 110.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31300/34255 [04:42<00:26, 110.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31320/34255 [04:43<00:26, 110.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  91% 31340/34255 [04:43<00:26, 110.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31360/34255 [04:43<00:26, 110.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31380/34255 [04:43<00:25, 110.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31400/34255 [04:43<00:25, 110.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31420/34255 [04:43<00:25, 110.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31440/34255 [04:43<00:25, 110.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31460/34255 [04:43<00:25, 110.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31480/34255 [04:43<00:25, 110.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31500/34255 [04:43<00:24, 110.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31520/34255 [04:43<00:24, 111.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31540/34255 [04:44<00:24, 111.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31560/34255 [04:44<00:24, 111.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31580/34255 [04:44<00:24, 111.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31600/34255 [04:44<00:23, 111.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31620/34255 [04:44<00:23, 111.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31640/34255 [04:44<00:23, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31660/34255 [04:44<00:23, 111.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  92% 31680/34255 [04:44<00:23, 111.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31700/34255 [04:44<00:22, 111.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31720/34255 [04:44<00:22, 111.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31740/34255 [04:44<00:22, 111.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31760/34255 [04:45<00:22, 111.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31780/34255 [04:45<00:22, 111.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31800/34255 [04:45<00:22, 111.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31820/34255 [04:45<00:21, 111.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31840/34255 [04:45<00:21, 111.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31860/34255 [04:45<00:21, 111.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31880/34255 [04:45<00:21, 111.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31900/34255 [04:45<00:21, 111.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31920/34255 [04:45<00:20, 111.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31940/34255 [04:45<00:20, 111.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31960/34255 [04:45<00:20, 111.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 31980/34255 [04:46<00:20, 111.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 32000/34255 [04:46<00:20, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  93% 32020/34255 [04:46<00:19, 111.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32040/34255 [04:46<00:19, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32060/34255 [04:46<00:19, 111.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32080/34255 [04:46<00:19, 112.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32100/34255 [04:46<00:19, 112.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32120/34255 [04:46<00:19, 112.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32140/34255 [04:46<00:18, 112.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32160/34255 [04:46<00:18, 112.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32180/34255 [04:46<00:18, 112.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32200/34255 [04:46<00:18, 112.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32220/34255 [04:47<00:18, 112.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32240/34255 [04:47<00:17, 112.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32260/34255 [04:47<00:17, 112.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32280/34255 [04:47<00:17, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32300/34255 [04:47<00:17, 112.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32320/34255 [04:47<00:17, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32340/34255 [04:47<00:17, 112.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  94% 32360/34255 [04:47<00:16, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32380/34255 [04:47<00:16, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32400/34255 [04:47<00:16, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32420/34255 [04:47<00:16, 112.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32440/34255 [04:48<00:16, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32460/34255 [04:48<00:15, 112.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32480/34255 [04:48<00:15, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32500/34255 [04:48<00:15, 112.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32520/34255 [04:48<00:15, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32540/34255 [04:48<00:15, 112.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32560/34255 [04:48<00:15, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32580/34255 [04:48<00:14, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32600/34255 [04:48<00:14, 112.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32620/34255 [04:48<00:14, 112.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32640/34255 [04:48<00:14, 112.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32660/34255 [04:49<00:14, 113.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32680/34255 [04:49<00:13, 113.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  95% 32700/34255 [04:49<00:13, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32720/34255 [04:49<00:13, 113.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32740/34255 [04:49<00:13, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32760/34255 [04:49<00:13, 113.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32780/34255 [04:49<00:13, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32800/34255 [04:49<00:12, 113.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32820/34255 [04:49<00:12, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32840/34255 [04:49<00:12, 113.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32860/34255 [04:49<00:12, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32880/34255 [04:49<00:12, 113.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32900/34255 [04:50<00:11, 113.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32920/34255 [04:50<00:11, 113.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32940/34255 [04:50<00:11, 113.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32960/34255 [04:50<00:11, 113.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 32980/34255 [04:50<00:11, 113.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 33000/34255 [04:50<00:11, 113.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 33020/34255 [04:50<00:10, 113.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  96% 33040/34255 [04:50<00:10, 113.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33060/34255 [04:50<00:10, 113.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33080/34255 [04:50<00:10, 113.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33100/34255 [04:50<00:10, 113.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33120/34255 [04:50<00:09, 113.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33140/34255 [04:51<00:09, 113.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33160/34255 [04:51<00:09, 113.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33180/34255 [04:51<00:09, 113.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33200/34255 [04:51<00:09, 113.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33220/34255 [04:51<00:09, 113.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33240/34255 [04:51<00:08, 114.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33260/34255 [04:51<00:08, 114.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33280/34255 [04:51<00:08, 114.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33300/34255 [04:51<00:08, 114.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33320/34255 [04:51<00:08, 114.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33340/34255 [04:51<00:08, 114.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33360/34255 [04:52<00:07, 114.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  97% 33380/34255 [04:52<00:07, 114.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33400/34255 [04:52<00:07, 114.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33420/34255 [04:52<00:07, 114.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33440/34255 [04:52<00:07, 114.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33460/34255 [04:52<00:06, 114.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33480/34255 [04:52<00:06, 114.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33500/34255 [04:52<00:06, 114.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33520/34255 [04:52<00:06, 114.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33540/34255 [04:52<00:06, 114.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33560/34255 [04:52<00:06, 114.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33580/34255 [04:52<00:05, 114.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33600/34255 [04:53<00:05, 114.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33620/34255 [04:53<00:05, 114.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33640/34255 [04:53<00:05, 114.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33660/34255 [04:53<00:05, 114.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33680/34255 [04:53<00:05, 114.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33700/34255 [04:53<00:04, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33720/34255 [04:53<00:04, 114.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  98% 33740/34255 [04:53<00:04, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33760/34255 [04:53<00:04, 114.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33780/34255 [04:53<00:04, 114.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33800/34255 [04:53<00:03, 114.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33820/34255 [04:54<00:03, 115.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33840/34255 [04:54<00:03, 115.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33860/34255 [04:54<00:03, 115.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33880/34255 [04:54<00:03, 115.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33900/34255 [04:54<00:03, 115.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33920/34255 [04:54<00:02, 115.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33940/34255 [04:54<00:02, 115.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33960/34255 [04:54<00:02, 115.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 33980/34255 [04:54<00:02, 115.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 34000/34255 [04:54<00:02, 115.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 34020/34255 [04:54<00:02, 115.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 34040/34255 [04:54<00:01, 115.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 34060/34255 [04:55<00:01, 115.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12:  99% 34080/34255 [04:55<00:01, 115.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34100/34255 [04:55<00:01, 115.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34120/34255 [04:55<00:01, 115.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34140/34255 [04:55<00:00, 115.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34160/34255 [04:55<00:00, 115.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34180/34255 [04:55<00:00, 115.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34200/34255 [04:55<00:00, 115.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34220/34255 [04:55<00:00, 115.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 12: 100% 34240/34255 [04:55<00:00, 115.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 227.35it/s]\u001b[A\n",
            "Epoch 12: 100% 34255/34255 [04:55<00:00, 115.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27400/34255 [04:23<01:05, 104.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  80% 27420/34255 [04:27<01:06, 102.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27440/34255 [04:27<01:06, 102.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27460/34255 [04:27<01:06, 102.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27480/34255 [04:27<01:06, 102.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27500/34255 [04:27<01:05, 102.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27520/34255 [04:28<01:05, 102.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27540/34255 [04:28<01:05, 102.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  80% 27560/34255 [04:28<01:05, 102.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27580/34255 [04:28<01:04, 102.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27600/34255 [04:28<01:04, 102.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27620/34255 [04:28<01:04, 102.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27640/34255 [04:28<01:04, 102.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27660/34255 [04:28<01:04, 102.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27680/34255 [04:28<01:03, 102.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27700/34255 [04:28<01:03, 103.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27720/34255 [04:28<01:03, 103.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27740/34255 [04:29<01:03, 103.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27760/34255 [04:29<01:02, 103.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27780/34255 [04:29<01:02, 103.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27800/34255 [04:29<01:02, 103.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27820/34255 [04:29<01:02, 103.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27840/34255 [04:29<01:02, 103.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27860/34255 [04:29<01:01, 103.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27880/34255 [04:29<01:01, 103.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  81% 27900/34255 [04:29<01:01, 103.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 27920/34255 [04:29<01:01, 103.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 27940/34255 [04:29<01:00, 103.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 27960/34255 [04:29<01:00, 103.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 27980/34255 [04:30<01:00, 103.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28000/34255 [04:30<01:00, 103.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28020/34255 [04:30<01:00, 103.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28040/34255 [04:30<00:59, 103.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28060/34255 [04:30<00:59, 103.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28080/34255 [04:30<00:59, 103.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28100/34255 [04:30<00:59, 103.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28120/34255 [04:30<00:59, 103.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28140/34255 [04:30<00:58, 103.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28160/34255 [04:30<00:58, 103.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28180/34255 [04:30<00:58, 104.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28200/34255 [04:31<00:58, 104.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28220/34255 [04:31<00:57, 104.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28240/34255 [04:31<00:57, 104.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  82% 28260/34255 [04:31<00:57, 104.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28280/34255 [04:31<00:57, 104.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28300/34255 [04:31<00:57, 104.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28320/34255 [04:31<00:56, 104.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28340/34255 [04:31<00:56, 104.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28360/34255 [04:31<00:56, 104.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28380/34255 [04:31<00:56, 104.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28400/34255 [04:31<00:56, 104.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28420/34255 [04:31<00:55, 104.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28440/34255 [04:32<00:55, 104.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28460/34255 [04:32<00:55, 104.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28480/34255 [04:32<00:55, 104.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28500/34255 [04:32<00:54, 104.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28520/34255 [04:32<00:54, 104.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28540/34255 [04:32<00:54, 104.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28560/34255 [04:32<00:54, 104.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28580/34255 [04:32<00:54, 104.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  83% 28600/34255 [04:32<00:53, 104.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28620/34255 [04:32<00:53, 104.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28640/34255 [04:32<00:53, 104.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28660/34255 [04:32<00:53, 104.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28680/34255 [04:33<00:53, 105.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28700/34255 [04:33<00:52, 105.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28720/34255 [04:33<00:52, 105.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28740/34255 [04:33<00:52, 105.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28760/34255 [04:33<00:52, 105.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28780/34255 [04:33<00:52, 105.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28800/34255 [04:33<00:51, 105.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28820/34255 [04:33<00:51, 105.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28840/34255 [04:33<00:51, 105.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28860/34255 [04:33<00:51, 105.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28880/34255 [04:33<00:50, 105.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28900/34255 [04:34<00:50, 105.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28920/34255 [04:34<00:50, 105.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  84% 28940/34255 [04:34<00:50, 105.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 28960/34255 [04:34<00:50, 105.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 28980/34255 [04:34<00:49, 105.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29000/34255 [04:34<00:49, 105.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29020/34255 [04:34<00:49, 105.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29040/34255 [04:34<00:49, 105.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29060/34255 [04:34<00:49, 105.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29080/34255 [04:34<00:48, 105.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29100/34255 [04:34<00:48, 105.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29120/34255 [04:35<00:48, 105.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29140/34255 [04:35<00:48, 105.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29160/34255 [04:35<00:48, 105.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29180/34255 [04:35<00:47, 105.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29200/34255 [04:35<00:47, 106.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29220/34255 [04:35<00:47, 106.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29240/34255 [04:35<00:47, 106.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29260/34255 [04:35<00:47, 106.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  85% 29280/34255 [04:35<00:46, 106.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29300/34255 [04:35<00:46, 106.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29320/34255 [04:35<00:46, 106.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29340/34255 [04:36<00:46, 106.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29360/34255 [04:36<00:46, 106.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29380/34255 [04:36<00:45, 106.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29400/34255 [04:36<00:45, 106.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29420/34255 [04:36<00:45, 106.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29440/34255 [04:36<00:45, 106.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29460/34255 [04:36<00:45, 106.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29480/34255 [04:36<00:44, 106.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29500/34255 [04:36<00:44, 106.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29520/34255 [04:36<00:44, 106.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29540/34255 [04:36<00:44, 106.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29560/34255 [04:36<00:43, 106.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29580/34255 [04:37<00:43, 106.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29600/34255 [04:37<00:43, 106.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  86% 29620/34255 [04:37<00:43, 106.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29640/34255 [04:37<00:43, 106.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29660/34255 [04:37<00:42, 106.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29680/34255 [04:37<00:42, 106.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29700/34255 [04:37<00:42, 106.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29720/34255 [04:37<00:42, 107.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29740/34255 [04:37<00:42, 107.07it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29760/34255 [04:37<00:41, 107.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29780/34255 [04:37<00:41, 107.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29800/34255 [04:38<00:41, 107.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29820/34255 [04:38<00:41, 107.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29840/34255 [04:38<00:41, 107.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29860/34255 [04:38<00:40, 107.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29880/34255 [04:38<00:40, 107.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29900/34255 [04:38<00:40, 107.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29920/34255 [04:38<00:40, 107.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29940/34255 [04:38<00:40, 107.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  87% 29960/34255 [04:38<00:39, 107.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 29980/34255 [04:38<00:39, 107.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30000/34255 [04:38<00:39, 107.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30020/34255 [04:39<00:39, 107.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30040/34255 [04:39<00:39, 107.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30060/34255 [04:39<00:38, 107.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30080/34255 [04:39<00:38, 107.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30100/34255 [04:39<00:38, 107.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30120/34255 [04:39<00:38, 107.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30140/34255 [04:39<00:38, 107.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30160/34255 [04:39<00:37, 107.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30180/34255 [04:39<00:37, 107.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30200/34255 [04:39<00:37, 107.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30220/34255 [04:39<00:37, 107.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30240/34255 [04:40<00:37, 107.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30260/34255 [04:40<00:36, 108.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30280/34255 [04:40<00:36, 108.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  88% 30300/34255 [04:40<00:36, 108.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30320/34255 [04:40<00:36, 108.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30340/34255 [04:40<00:36, 108.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30360/34255 [04:40<00:35, 108.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30380/34255 [04:40<00:35, 108.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30400/34255 [04:40<00:35, 108.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30420/34255 [04:40<00:35, 108.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30440/34255 [04:40<00:35, 108.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30460/34255 [04:41<00:35, 108.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30480/34255 [04:41<00:34, 108.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30500/34255 [04:41<00:34, 108.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30520/34255 [04:41<00:34, 108.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30540/34255 [04:41<00:34, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30560/34255 [04:41<00:34, 108.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30580/34255 [04:41<00:33, 108.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30600/34255 [04:41<00:33, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30620/34255 [04:41<00:33, 108.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  89% 30640/34255 [04:41<00:33, 108.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30660/34255 [04:41<00:33, 108.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30680/34255 [04:42<00:32, 108.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30700/34255 [04:42<00:32, 108.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30720/34255 [04:42<00:32, 108.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30740/34255 [04:42<00:32, 108.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30760/34255 [04:42<00:32, 108.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30780/34255 [04:42<00:31, 108.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30800/34255 [04:42<00:31, 108.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30820/34255 [04:42<00:31, 109.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30840/34255 [04:42<00:31, 109.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30860/34255 [04:42<00:31, 109.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30880/34255 [04:42<00:30, 109.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30900/34255 [04:43<00:30, 109.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30920/34255 [04:43<00:30, 109.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30940/34255 [04:43<00:30, 109.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30960/34255 [04:43<00:30, 109.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 30980/34255 [04:43<00:29, 109.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  90% 31000/34255 [04:43<00:29, 109.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31020/34255 [04:43<00:29, 109.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31040/34255 [04:43<00:29, 109.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31060/34255 [04:43<00:29, 109.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31080/34255 [04:43<00:28, 109.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31100/34255 [04:43<00:28, 109.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31120/34255 [04:44<00:28, 109.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31140/34255 [04:44<00:28, 109.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31160/34255 [04:44<00:28, 109.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31180/34255 [04:44<00:28, 109.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31200/34255 [04:44<00:27, 109.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31220/34255 [04:44<00:27, 109.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31240/34255 [04:44<00:27, 109.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31260/34255 [04:44<00:27, 109.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31280/34255 [04:44<00:27, 109.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31300/34255 [04:44<00:26, 109.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31320/34255 [04:44<00:26, 109.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  91% 31340/34255 [04:45<00:26, 109.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31360/34255 [04:45<00:26, 110.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31380/34255 [04:45<00:26, 110.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31400/34255 [04:45<00:25, 110.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31420/34255 [04:45<00:25, 110.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31440/34255 [04:45<00:25, 110.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31460/34255 [04:45<00:25, 110.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31480/34255 [04:45<00:25, 110.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31500/34255 [04:45<00:24, 110.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31520/34255 [04:45<00:24, 110.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31540/34255 [04:45<00:24, 110.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31560/34255 [04:45<00:24, 110.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31580/34255 [04:46<00:24, 110.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31600/34255 [04:46<00:24, 110.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31620/34255 [04:46<00:23, 110.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31640/34255 [04:46<00:23, 110.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31660/34255 [04:46<00:23, 110.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  92% 31680/34255 [04:46<00:23, 110.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31700/34255 [04:46<00:23, 110.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31720/34255 [04:46<00:22, 110.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31740/34255 [04:46<00:22, 110.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31760/34255 [04:46<00:22, 110.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31780/34255 [04:46<00:22, 110.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31800/34255 [04:46<00:22, 110.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31820/34255 [04:47<00:21, 110.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31840/34255 [04:47<00:21, 110.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31860/34255 [04:47<00:21, 110.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31880/34255 [04:47<00:21, 110.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31900/34255 [04:47<00:21, 110.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31920/34255 [04:47<00:21, 111.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31940/34255 [04:47<00:20, 111.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31960/34255 [04:47<00:20, 111.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 31980/34255 [04:47<00:20, 111.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 32000/34255 [04:47<00:20, 111.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  93% 32020/34255 [04:47<00:20, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32040/34255 [04:48<00:19, 111.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32060/34255 [04:48<00:19, 111.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32080/34255 [04:48<00:19, 111.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32100/34255 [04:48<00:19, 111.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32120/34255 [04:48<00:19, 111.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32140/34255 [04:48<00:18, 111.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32160/34255 [04:48<00:18, 111.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32180/34255 [04:48<00:18, 111.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32200/34255 [04:48<00:18, 111.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32220/34255 [04:48<00:18, 111.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32240/34255 [04:48<00:18, 111.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32260/34255 [04:48<00:17, 111.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32280/34255 [04:49<00:17, 111.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32300/34255 [04:49<00:17, 111.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32320/34255 [04:49<00:17, 111.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32340/34255 [04:49<00:17, 111.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  94% 32360/34255 [04:49<00:16, 111.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32380/34255 [04:49<00:16, 111.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32400/34255 [04:49<00:16, 111.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32420/34255 [04:49<00:16, 111.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32440/34255 [04:49<00:16, 111.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32460/34255 [04:49<00:16, 111.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32480/34255 [04:49<00:15, 112.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32500/34255 [04:50<00:15, 112.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32520/34255 [04:50<00:15, 112.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32540/34255 [04:50<00:15, 112.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32560/34255 [04:50<00:15, 112.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32580/34255 [04:50<00:14, 112.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32600/34255 [04:50<00:14, 112.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32620/34255 [04:50<00:14, 112.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32640/34255 [04:50<00:14, 112.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32660/34255 [04:50<00:14, 112.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32680/34255 [04:50<00:14, 112.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  95% 32700/34255 [04:50<00:13, 112.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32720/34255 [04:50<00:13, 112.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32740/34255 [04:51<00:13, 112.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32760/34255 [04:51<00:13, 112.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32780/34255 [04:51<00:13, 112.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32800/34255 [04:51<00:12, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32820/34255 [04:51<00:12, 112.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32840/34255 [04:51<00:12, 112.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32860/34255 [04:51<00:12, 112.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32880/34255 [04:51<00:12, 112.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32900/34255 [04:51<00:12, 112.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32920/34255 [04:51<00:11, 112.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32940/34255 [04:51<00:11, 112.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32960/34255 [04:52<00:11, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 32980/34255 [04:52<00:11, 112.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 33000/34255 [04:52<00:11, 112.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 33020/34255 [04:52<00:10, 112.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  96% 33040/34255 [04:52<00:10, 113.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33060/34255 [04:52<00:10, 113.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33080/34255 [04:52<00:10, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33100/34255 [04:52<00:10, 113.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33120/34255 [04:52<00:10, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33140/34255 [04:52<00:09, 113.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33160/34255 [04:52<00:09, 113.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33180/34255 [04:52<00:09, 113.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33200/34255 [04:53<00:09, 113.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33220/34255 [04:53<00:09, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33240/34255 [04:53<00:08, 113.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33260/34255 [04:53<00:08, 113.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33280/34255 [04:53<00:08, 113.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33300/34255 [04:53<00:08, 113.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33320/34255 [04:53<00:08, 113.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33340/34255 [04:53<00:08, 113.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33360/34255 [04:53<00:07, 113.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  97% 33380/34255 [04:53<00:07, 113.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33400/34255 [04:53<00:07, 113.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33420/34255 [04:54<00:07, 113.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33440/34255 [04:54<00:07, 113.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33460/34255 [04:54<00:06, 113.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33480/34255 [04:54<00:06, 113.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33500/34255 [04:54<00:06, 113.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33520/34255 [04:54<00:06, 113.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33540/34255 [04:54<00:06, 113.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33560/34255 [04:54<00:06, 113.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33580/34255 [04:54<00:05, 113.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33600/34255 [04:54<00:05, 113.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33620/34255 [04:54<00:05, 113.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33640/34255 [04:55<00:05, 114.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33660/34255 [04:55<00:05, 114.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33680/34255 [04:55<00:05, 114.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33700/34255 [04:55<00:04, 114.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33720/34255 [04:55<00:04, 114.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  98% 33740/34255 [04:55<00:04, 114.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33760/34255 [04:55<00:04, 114.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33780/34255 [04:55<00:04, 114.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33800/34255 [04:55<00:03, 114.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33820/34255 [04:55<00:03, 114.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33840/34255 [04:55<00:03, 114.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33860/34255 [04:55<00:03, 114.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33880/34255 [04:56<00:03, 114.43it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33900/34255 [04:56<00:03, 114.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33920/34255 [04:56<00:02, 114.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33940/34255 [04:56<00:02, 114.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33960/34255 [04:56<00:02, 114.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 33980/34255 [04:56<00:02, 114.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 34000/34255 [04:56<00:02, 114.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 34020/34255 [04:56<00:02, 114.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 34040/34255 [04:56<00:01, 114.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 34060/34255 [04:56<00:01, 114.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13:  99% 34080/34255 [04:56<00:01, 114.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34100/34255 [04:57<00:01, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34120/34255 [04:57<00:01, 114.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34140/34255 [04:57<00:01, 114.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34160/34255 [04:57<00:00, 114.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34180/34255 [04:57<00:00, 114.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34200/34255 [04:57<00:00, 114.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34220/34255 [04:57<00:00, 114.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 13: 100% 34240/34255 [04:57<00:00, 115.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 226.78it/s]\u001b[A\n",
            "Epoch 13: 100% 34255/34255 [04:57<00:00, 115.03it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27400/34255 [04:23<01:06, 103.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  80% 27420/34255 [04:28<01:06, 102.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27440/34255 [04:28<01:06, 102.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27460/34255 [04:28<01:06, 102.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27480/34255 [04:28<01:06, 102.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27500/34255 [04:28<01:05, 102.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27520/34255 [04:28<01:05, 102.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27540/34255 [04:28<01:05, 102.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  80% 27560/34255 [04:28<01:05, 102.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27580/34255 [04:28<01:05, 102.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27600/34255 [04:28<01:04, 102.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27620/34255 [04:29<01:04, 102.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27640/34255 [04:29<01:04, 102.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27660/34255 [04:29<01:04, 102.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27680/34255 [04:29<01:03, 102.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27700/34255 [04:29<01:03, 102.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27720/34255 [04:29<01:03, 102.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27740/34255 [04:29<01:03, 102.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27760/34255 [04:29<01:03, 102.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27780/34255 [04:29<01:02, 102.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27800/34255 [04:29<01:02, 103.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27820/34255 [04:29<01:02, 103.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27840/34255 [04:30<01:02, 103.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27860/34255 [04:30<01:02, 103.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27880/34255 [04:30<01:01, 103.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  81% 27900/34255 [04:30<01:01, 103.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 27920/34255 [04:30<01:01, 103.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 27940/34255 [04:30<01:01, 103.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 27960/34255 [04:30<01:00, 103.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 27980/34255 [04:30<01:00, 103.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28000/34255 [04:30<01:00, 103.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28020/34255 [04:30<01:00, 103.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28040/34255 [04:30<01:00, 103.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28060/34255 [04:31<00:59, 103.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28080/34255 [04:31<00:59, 103.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28100/34255 [04:31<00:59, 103.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28120/34255 [04:31<00:59, 103.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28140/34255 [04:31<00:58, 103.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28160/34255 [04:31<00:58, 103.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28180/34255 [04:31<00:58, 103.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28200/34255 [04:31<00:58, 103.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28220/34255 [04:31<00:58, 103.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28240/34255 [04:31<00:57, 103.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  82% 28260/34255 [04:31<00:57, 103.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28280/34255 [04:31<00:57, 103.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28300/34255 [04:32<00:57, 104.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28320/34255 [04:32<00:57, 104.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28340/34255 [04:32<00:56, 104.10it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28360/34255 [04:32<00:56, 104.14it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28380/34255 [04:32<00:56, 104.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28400/34255 [04:32<00:56, 104.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28420/34255 [04:32<00:55, 104.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28440/34255 [04:32<00:55, 104.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28460/34255 [04:32<00:55, 104.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28480/34255 [04:32<00:55, 104.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28500/34255 [04:32<00:55, 104.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28520/34255 [04:33<00:54, 104.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28540/34255 [04:33<00:54, 104.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28560/34255 [04:33<00:54, 104.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28580/34255 [04:33<00:54, 104.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  83% 28600/34255 [04:33<00:54, 104.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28620/34255 [04:33<00:53, 104.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28640/34255 [04:33<00:53, 104.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28660/34255 [04:33<00:53, 104.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28680/34255 [04:33<00:53, 104.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28700/34255 [04:33<00:53, 104.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28720/34255 [04:33<00:52, 104.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28740/34255 [04:34<00:52, 104.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28760/34255 [04:34<00:52, 104.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28780/34255 [04:34<00:52, 104.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28800/34255 [04:34<00:51, 105.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28820/34255 [04:34<00:51, 105.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28840/34255 [04:34<00:51, 105.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28860/34255 [04:34<00:51, 105.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28880/34255 [04:34<00:51, 105.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28900/34255 [04:34<00:50, 105.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28920/34255 [04:34<00:50, 105.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  84% 28940/34255 [04:34<00:50, 105.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 28960/34255 [04:34<00:50, 105.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 28980/34255 [04:35<00:50, 105.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29000/34255 [04:35<00:49, 105.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29020/34255 [04:35<00:49, 105.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29040/34255 [04:35<00:49, 105.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29060/34255 [04:35<00:49, 105.50it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29080/34255 [04:35<00:49, 105.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29100/34255 [04:35<00:48, 105.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29120/34255 [04:35<00:48, 105.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29140/34255 [04:35<00:48, 105.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29160/34255 [04:35<00:48, 105.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29180/34255 [04:36<00:48, 105.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29200/34255 [04:36<00:47, 105.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29220/34255 [04:36<00:47, 105.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29240/34255 [04:36<00:47, 105.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29260/34255 [04:36<00:47, 105.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  85% 29280/34255 [04:36<00:46, 105.92it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29300/34255 [04:36<00:46, 105.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29320/34255 [04:36<00:46, 106.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29340/34255 [04:36<00:46, 106.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29360/34255 [04:36<00:46, 106.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29380/34255 [04:36<00:45, 106.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29400/34255 [04:36<00:45, 106.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29420/34255 [04:37<00:45, 106.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29440/34255 [04:37<00:45, 106.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29460/34255 [04:37<00:45, 106.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29480/34255 [04:37<00:44, 106.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29500/34255 [04:37<00:44, 106.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29520/34255 [04:37<00:44, 106.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29540/34255 [04:37<00:44, 106.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29560/34255 [04:37<00:44, 106.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29580/34255 [04:37<00:43, 106.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29600/34255 [04:37<00:43, 106.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  86% 29620/34255 [04:37<00:43, 106.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29640/34255 [04:37<00:43, 106.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29660/34255 [04:38<00:43, 106.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29680/34255 [04:38<00:42, 106.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29700/34255 [04:38<00:42, 106.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29720/34255 [04:38<00:42, 106.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29740/34255 [04:38<00:42, 106.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29760/34255 [04:38<00:42, 106.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29780/34255 [04:38<00:41, 106.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29800/34255 [04:38<00:41, 106.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29820/34255 [04:38<00:41, 106.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29840/34255 [04:38<00:41, 107.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29860/34255 [04:38<00:41, 107.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29880/34255 [04:39<00:40, 107.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29900/34255 [04:39<00:40, 107.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29920/34255 [04:39<00:40, 107.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29940/34255 [04:39<00:40, 107.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  87% 29960/34255 [04:39<00:40, 107.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 29980/34255 [04:39<00:39, 107.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30000/34255 [04:39<00:39, 107.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30020/34255 [04:39<00:39, 107.36it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30040/34255 [04:39<00:39, 107.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30060/34255 [04:39<00:39, 107.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30080/34255 [04:39<00:38, 107.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30100/34255 [04:39<00:38, 107.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30120/34255 [04:40<00:38, 107.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30140/34255 [04:40<00:38, 107.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30160/34255 [04:40<00:38, 107.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30180/34255 [04:40<00:37, 107.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30200/34255 [04:40<00:37, 107.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30220/34255 [04:40<00:37, 107.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30240/34255 [04:40<00:37, 107.78it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30260/34255 [04:40<00:37, 107.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30280/34255 [04:40<00:36, 107.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  88% 30300/34255 [04:40<00:36, 107.89it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30320/34255 [04:40<00:36, 107.93it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30340/34255 [04:41<00:36, 107.96it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30360/34255 [04:41<00:36, 108.00it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30380/34255 [04:41<00:35, 108.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30400/34255 [04:41<00:35, 108.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30420/34255 [04:41<00:35, 108.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30440/34255 [04:41<00:35, 108.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30460/34255 [04:41<00:35, 108.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30480/34255 [04:41<00:34, 108.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30500/34255 [04:41<00:34, 108.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30520/34255 [04:41<00:34, 108.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30540/34255 [04:41<00:34, 108.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30560/34255 [04:41<00:34, 108.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30580/34255 [04:42<00:33, 108.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30600/34255 [04:42<00:33, 108.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30620/34255 [04:42<00:33, 108.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  89% 30640/34255 [04:42<00:33, 108.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30660/34255 [04:42<00:33, 108.57it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30680/34255 [04:42<00:32, 108.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30700/34255 [04:42<00:32, 108.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30720/34255 [04:42<00:32, 108.68it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30740/34255 [04:42<00:32, 108.71it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30760/34255 [04:42<00:32, 108.75it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30780/34255 [04:42<00:31, 108.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30800/34255 [04:43<00:31, 108.82it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30820/34255 [04:43<00:31, 108.86it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30840/34255 [04:43<00:31, 108.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30860/34255 [04:43<00:31, 108.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30880/34255 [04:43<00:30, 108.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30900/34255 [04:43<00:30, 109.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30920/34255 [04:43<00:30, 109.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30940/34255 [04:43<00:30, 109.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30960/34255 [04:43<00:30, 109.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 30980/34255 [04:43<00:30, 109.16it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  90% 31000/34255 [04:43<00:29, 109.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31020/34255 [04:43<00:29, 109.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31040/34255 [04:44<00:29, 109.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31060/34255 [04:44<00:29, 109.30it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31080/34255 [04:44<00:29, 109.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31100/34255 [04:44<00:28, 109.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31120/34255 [04:44<00:28, 109.41it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31140/34255 [04:44<00:28, 109.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31160/34255 [04:44<00:28, 109.48it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31180/34255 [04:44<00:28, 109.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31200/34255 [04:44<00:27, 109.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31220/34255 [04:44<00:27, 109.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31240/34255 [04:44<00:27, 109.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31260/34255 [04:45<00:27, 109.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31280/34255 [04:45<00:27, 109.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31300/34255 [04:45<00:26, 109.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31320/34255 [04:45<00:26, 109.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  91% 31340/34255 [04:45<00:26, 109.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31360/34255 [04:45<00:26, 109.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31380/34255 [04:45<00:26, 109.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31400/34255 [04:45<00:25, 109.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31420/34255 [04:45<00:25, 109.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31440/34255 [04:45<00:25, 109.99it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31460/34255 [04:45<00:25, 110.02it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31480/34255 [04:46<00:25, 110.06it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31500/34255 [04:46<00:25, 110.09it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31520/34255 [04:46<00:24, 110.13it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31540/34255 [04:46<00:24, 110.17it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31560/34255 [04:46<00:24, 110.20it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31580/34255 [04:46<00:24, 110.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31600/34255 [04:46<00:24, 110.27it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31620/34255 [04:46<00:23, 110.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31640/34255 [04:46<00:23, 110.34it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31660/34255 [04:46<00:23, 110.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  92% 31680/34255 [04:46<00:23, 110.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31700/34255 [04:46<00:23, 110.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31720/34255 [04:47<00:22, 110.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31740/34255 [04:47<00:22, 110.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31760/34255 [04:47<00:22, 110.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31780/34255 [04:47<00:22, 110.60it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31800/34255 [04:47<00:22, 110.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31820/34255 [04:47<00:22, 110.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31840/34255 [04:47<00:21, 110.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31860/34255 [04:47<00:21, 110.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31880/34255 [04:47<00:21, 110.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31900/34255 [04:47<00:21, 110.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31920/34255 [04:47<00:21, 110.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31940/34255 [04:48<00:20, 110.88it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31960/34255 [04:48<00:20, 110.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 31980/34255 [04:48<00:20, 110.95it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 32000/34255 [04:48<00:20, 110.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  93% 32020/34255 [04:48<00:20, 111.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32040/34255 [04:48<00:19, 111.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32060/34255 [04:48<00:19, 111.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32080/34255 [04:48<00:19, 111.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32100/34255 [04:48<00:19, 111.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32120/34255 [04:48<00:19, 111.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32140/34255 [04:48<00:19, 111.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32160/34255 [04:49<00:18, 111.24it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32180/34255 [04:49<00:18, 111.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32200/34255 [04:49<00:18, 111.31it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32220/34255 [04:49<00:18, 111.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32240/34255 [04:49<00:18, 111.38it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32260/34255 [04:49<00:17, 111.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32280/34255 [04:49<00:17, 111.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32300/34255 [04:49<00:17, 111.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32320/34255 [04:49<00:17, 111.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32340/34255 [04:49<00:17, 111.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  94% 32360/34255 [04:49<00:16, 111.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32380/34255 [04:50<00:16, 111.63it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32400/34255 [04:50<00:16, 111.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32420/34255 [04:50<00:16, 111.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32440/34255 [04:50<00:16, 111.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32460/34255 [04:50<00:16, 111.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32480/34255 [04:50<00:15, 111.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32500/34255 [04:50<00:15, 111.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32520/34255 [04:50<00:15, 111.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32540/34255 [04:50<00:15, 111.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32560/34255 [04:50<00:15, 111.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32580/34255 [04:50<00:14, 111.98it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32600/34255 [04:51<00:14, 112.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32620/34255 [04:51<00:14, 112.05it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32640/34255 [04:51<00:14, 112.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32660/34255 [04:51<00:14, 112.12it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32680/34255 [04:51<00:14, 112.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  95% 32700/34255 [04:51<00:13, 112.19it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32720/34255 [04:51<00:13, 112.23it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32740/34255 [04:51<00:13, 112.26it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32760/34255 [04:51<00:13, 112.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32780/34255 [04:51<00:13, 112.33it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32800/34255 [04:51<00:12, 112.37it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32820/34255 [04:51<00:12, 112.40it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32840/34255 [04:52<00:12, 112.44it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32860/34255 [04:52<00:12, 112.47it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32880/34255 [04:52<00:12, 112.51it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32900/34255 [04:52<00:12, 112.54it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32920/34255 [04:52<00:11, 112.58it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32940/34255 [04:52<00:11, 112.61it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32960/34255 [04:52<00:11, 112.64it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 32980/34255 [04:52<00:11, 112.67it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 33000/34255 [04:52<00:11, 112.70it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 33020/34255 [04:52<00:10, 112.74it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  96% 33040/34255 [04:52<00:10, 112.77it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33060/34255 [04:53<00:10, 112.81it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33080/34255 [04:53<00:10, 112.84it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33100/34255 [04:53<00:10, 112.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33120/34255 [04:53<00:10, 112.91it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33140/34255 [04:53<00:09, 112.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33160/34255 [04:53<00:09, 112.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33180/34255 [04:53<00:09, 113.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33200/34255 [04:53<00:09, 113.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33220/34255 [04:53<00:09, 113.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33240/34255 [04:53<00:08, 113.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33260/34255 [04:53<00:08, 113.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33280/34255 [04:54<00:08, 113.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33300/34255 [04:54<00:08, 113.21it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33320/34255 [04:54<00:08, 113.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33340/34255 [04:54<00:08, 113.28it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33360/34255 [04:54<00:07, 113.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  97% 33380/34255 [04:54<00:07, 113.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33400/34255 [04:54<00:07, 113.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33420/34255 [04:54<00:07, 113.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33440/34255 [04:54<00:07, 113.45it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33460/34255 [04:54<00:07, 113.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33480/34255 [04:54<00:06, 113.52it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33500/34255 [04:55<00:06, 113.56it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33520/34255 [04:55<00:06, 113.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33540/34255 [04:55<00:06, 113.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33560/34255 [04:55<00:06, 113.66it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33580/34255 [04:55<00:05, 113.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33600/34255 [04:55<00:05, 113.73it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33620/34255 [04:55<00:05, 113.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33640/34255 [04:55<00:05, 113.80it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33660/34255 [04:55<00:05, 113.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33680/34255 [04:55<00:05, 113.87it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33700/34255 [04:55<00:04, 113.90it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33720/34255 [04:55<00:04, 113.94it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  98% 33740/34255 [04:56<00:04, 113.97it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33760/34255 [04:56<00:04, 114.01it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33780/34255 [04:56<00:04, 114.04it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33800/34255 [04:56<00:03, 114.08it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33820/34255 [04:56<00:03, 114.11it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33840/34255 [04:56<00:03, 114.15it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33860/34255 [04:56<00:03, 114.18it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33880/34255 [04:56<00:03, 114.22it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33900/34255 [04:56<00:03, 114.25it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33920/34255 [04:56<00:02, 114.29it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33940/34255 [04:56<00:02, 114.32it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33960/34255 [04:56<00:02, 114.35it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 33980/34255 [04:57<00:02, 114.39it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 34000/34255 [04:57<00:02, 114.42it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 34020/34255 [04:57<00:02, 114.46it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 34040/34255 [04:57<00:01, 114.49it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 34060/34255 [04:57<00:01, 114.53it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14:  99% 34080/34255 [04:57<00:01, 114.55it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34100/34255 [04:57<00:01, 114.59it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34120/34255 [04:57<00:01, 114.62it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34140/34255 [04:57<00:01, 114.65it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34160/34255 [04:57<00:00, 114.69it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34180/34255 [04:57<00:00, 114.72it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34200/34255 [04:58<00:00, 114.76it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34220/34255 [04:58<00:00, 114.79it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 14: 100% 34240/34255 [04:58<00:00, 114.83it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:30<00:00, 227.22it/s]\u001b[A\n",
            "Epoch 14: 100% 34255/34255 [04:58<00:00, 114.85it/s, loss=2.65, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27400/34255 [04:23<01:06, 103.81it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  80% 27420/34255 [04:28<01:06, 102.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27440/34255 [04:28<01:06, 102.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27460/34255 [04:28<01:06, 102.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27480/34255 [04:28<01:06, 102.27it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27500/34255 [04:28<01:06, 102.31it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27520/34255 [04:28<01:05, 102.35it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27540/34255 [04:28<01:05, 102.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  80% 27560/34255 [04:29<01:05, 102.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27580/34255 [04:29<01:05, 102.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27600/34255 [04:29<01:04, 102.51it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27620/34255 [04:29<01:04, 102.55it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27640/34255 [04:29<01:04, 102.59it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27660/34255 [04:29<01:04, 102.63it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27680/34255 [04:29<01:04, 102.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27700/34255 [04:29<01:03, 102.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27720/34255 [04:29<01:03, 102.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27740/34255 [04:29<01:03, 102.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27760/34255 [04:29<01:03, 102.84it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27780/34255 [04:30<01:02, 102.88it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27800/34255 [04:30<01:02, 102.92it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27820/34255 [04:30<01:02, 102.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27840/34255 [04:30<01:02, 103.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27860/34255 [04:30<01:02, 103.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27880/34255 [04:30<01:01, 103.09it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  81% 27900/34255 [04:30<01:01, 103.13it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 27920/34255 [04:30<01:01, 103.17it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 27940/34255 [04:30<01:01, 103.21it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 27960/34255 [04:30<01:00, 103.25it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 27980/34255 [04:30<01:00, 103.29it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28000/34255 [04:30<01:00, 103.34it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28020/34255 [04:31<01:00, 103.38it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28040/34255 [04:31<01:00, 103.42it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28060/34255 [04:31<00:59, 103.46it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28080/34255 [04:31<00:59, 103.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28100/34255 [04:31<00:59, 103.54it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28120/34255 [04:31<00:59, 103.58it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28140/34255 [04:31<00:59, 103.62it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28160/34255 [04:31<00:58, 103.66it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28180/34255 [04:31<00:58, 103.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28200/34255 [04:31<00:58, 103.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28220/34255 [04:31<00:58, 103.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28240/34255 [04:31<00:57, 103.83it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  82% 28260/34255 [04:32<00:57, 103.87it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28280/34255 [04:32<00:57, 103.91it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28300/34255 [04:32<00:57, 103.95it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28320/34255 [04:32<00:57, 103.99it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28340/34255 [04:32<00:56, 104.03it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28360/34255 [04:32<00:56, 104.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28380/34255 [04:32<00:56, 104.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28400/34255 [04:32<00:56, 104.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28420/34255 [04:32<00:56, 104.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28440/34255 [04:32<00:55, 104.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28460/34255 [04:32<00:55, 104.27it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28480/34255 [04:33<00:55, 104.31it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28500/34255 [04:33<00:55, 104.35it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28520/34255 [04:33<00:54, 104.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28540/34255 [04:33<00:54, 104.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28560/34255 [04:33<00:54, 104.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28580/34255 [04:33<00:54, 104.51it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  83% 28600/34255 [04:33<00:54, 104.55it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28620/34255 [04:33<00:53, 104.59it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28640/34255 [04:33<00:53, 104.63it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28660/34255 [04:33<00:53, 104.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28680/34255 [04:33<00:53, 104.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28700/34255 [04:33<00:53, 104.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28720/34255 [04:34<00:52, 104.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28740/34255 [04:34<00:52, 104.83it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28760/34255 [04:34<00:52, 104.87it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28780/34255 [04:34<00:52, 104.91it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28800/34255 [04:34<00:51, 104.95it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28820/34255 [04:34<00:51, 104.99it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28840/34255 [04:34<00:51, 105.03it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28860/34255 [04:34<00:51, 105.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28880/34255 [04:34<00:51, 105.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28900/34255 [04:34<00:50, 105.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28920/34255 [04:34<00:50, 105.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  84% 28940/34255 [04:35<00:50, 105.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 28960/34255 [04:35<00:50, 105.27it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 28980/34255 [04:35<00:50, 105.31it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29000/34255 [04:35<00:49, 105.35it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29020/34255 [04:35<00:49, 105.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29040/34255 [04:35<00:49, 105.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29060/34255 [04:35<00:49, 105.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29080/34255 [04:35<00:49, 105.51it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29100/34255 [04:35<00:48, 105.55it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29120/34255 [04:35<00:48, 105.59it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29140/34255 [04:35<00:48, 105.63it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29160/34255 [04:35<00:48, 105.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29180/34255 [04:36<00:48, 105.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29200/34255 [04:36<00:47, 105.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29220/34255 [04:36<00:47, 105.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29240/34255 [04:36<00:47, 105.83it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29260/34255 [04:36<00:47, 105.87it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  85% 29280/34255 [04:36<00:46, 105.91it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29300/34255 [04:36<00:46, 105.95it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29320/34255 [04:36<00:46, 105.99it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29340/34255 [04:36<00:46, 106.03it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29360/34255 [04:36<00:46, 106.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29380/34255 [04:36<00:45, 106.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29400/34255 [04:36<00:45, 106.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29420/34255 [04:37<00:45, 106.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29440/34255 [04:37<00:45, 106.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29460/34255 [04:37<00:45, 106.26it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29480/34255 [04:37<00:44, 106.30it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29500/34255 [04:37<00:44, 106.34it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29520/34255 [04:37<00:44, 106.38it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29540/34255 [04:37<00:44, 106.42it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29560/34255 [04:37<00:44, 106.46it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29580/34255 [04:37<00:43, 106.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29600/34255 [04:37<00:43, 106.54it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  86% 29620/34255 [04:37<00:43, 106.58it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29640/34255 [04:38<00:43, 106.62it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29660/34255 [04:38<00:43, 106.66it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29680/34255 [04:38<00:42, 106.69it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29700/34255 [04:38<00:42, 106.73it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29720/34255 [04:38<00:42, 106.77it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29740/34255 [04:38<00:42, 106.81it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29760/34255 [04:38<00:42, 106.84it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29780/34255 [04:38<00:41, 106.88it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29800/34255 [04:38<00:41, 106.92it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29820/34255 [04:38<00:41, 106.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29840/34255 [04:38<00:41, 107.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29860/34255 [04:38<00:41, 107.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29880/34255 [04:39<00:40, 107.08it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29900/34255 [04:39<00:40, 107.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29920/34255 [04:39<00:40, 107.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29940/34255 [04:39<00:40, 107.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  87% 29960/34255 [04:39<00:40, 107.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 29980/34255 [04:39<00:39, 107.27it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30000/34255 [04:39<00:39, 107.31it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30020/34255 [04:39<00:39, 107.34it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30040/34255 [04:39<00:39, 107.38it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30060/34255 [04:39<00:39, 107.42it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30080/34255 [04:39<00:38, 107.45it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30100/34255 [04:40<00:38, 107.49it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30120/34255 [04:40<00:38, 107.53it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30140/34255 [04:40<00:38, 107.56it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30160/34255 [04:40<00:38, 107.60it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30180/34255 [04:40<00:37, 107.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30200/34255 [04:40<00:37, 107.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30220/34255 [04:40<00:37, 107.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30240/34255 [04:40<00:37, 107.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30260/34255 [04:40<00:37, 107.78it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30280/34255 [04:40<00:36, 107.82it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  88% 30300/34255 [04:40<00:36, 107.86it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30320/34255 [04:41<00:36, 107.89it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30340/34255 [04:41<00:36, 107.93it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30360/34255 [04:41<00:36, 107.97it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30380/34255 [04:41<00:35, 108.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30400/34255 [04:41<00:35, 108.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30420/34255 [04:41<00:35, 108.08it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30440/34255 [04:41<00:35, 108.12it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30460/34255 [04:41<00:35, 108.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30480/34255 [04:41<00:34, 108.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30500/34255 [04:41<00:34, 108.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30520/34255 [04:41<00:34, 108.26it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30540/34255 [04:41<00:34, 108.30it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30560/34255 [04:42<00:34, 108.34it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30580/34255 [04:42<00:33, 108.38it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30600/34255 [04:42<00:33, 108.41it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30620/34255 [04:42<00:33, 108.45it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  89% 30640/34255 [04:42<00:33, 108.49it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30660/34255 [04:42<00:33, 108.52it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30680/34255 [04:42<00:32, 108.56it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30700/34255 [04:42<00:32, 108.60it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30720/34255 [04:42<00:32, 108.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30740/34255 [04:42<00:32, 108.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30760/34255 [04:42<00:32, 108.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30780/34255 [04:43<00:31, 108.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30800/34255 [04:43<00:31, 108.78it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30820/34255 [04:43<00:31, 108.82it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30840/34255 [04:43<00:31, 108.85it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30860/34255 [04:43<00:31, 108.89it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30880/34255 [04:43<00:30, 108.93it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30900/34255 [04:43<00:30, 108.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30920/34255 [04:43<00:30, 109.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30940/34255 [04:43<00:30, 109.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30960/34255 [04:43<00:30, 109.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 30980/34255 [04:43<00:30, 109.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  90% 31000/34255 [04:44<00:29, 109.14it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31020/34255 [04:44<00:29, 109.18it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31040/34255 [04:44<00:29, 109.21it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31060/34255 [04:44<00:29, 109.25it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31080/34255 [04:44<00:29, 109.28it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31100/34255 [04:44<00:28, 109.32it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31120/34255 [04:44<00:28, 109.35it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31140/34255 [04:44<00:28, 109.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31160/34255 [04:44<00:28, 109.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31180/34255 [04:44<00:28, 109.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31200/34255 [04:44<00:27, 109.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31220/34255 [04:45<00:27, 109.53it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31240/34255 [04:45<00:27, 109.57it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31260/34255 [04:45<00:27, 109.61it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31280/34255 [04:45<00:27, 109.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31300/34255 [04:45<00:26, 109.68it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31320/34255 [04:45<00:26, 109.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  91% 31340/34255 [04:45<00:26, 109.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31360/34255 [04:45<00:26, 109.78it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31380/34255 [04:45<00:26, 109.82it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31400/34255 [04:45<00:25, 109.85it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31420/34255 [04:45<00:25, 109.89it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31440/34255 [04:46<00:25, 109.92it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31460/34255 [04:46<00:25, 109.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31480/34255 [04:46<00:25, 110.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31500/34255 [04:46<00:25, 110.03it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31520/34255 [04:46<00:24, 110.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31540/34255 [04:46<00:24, 110.10it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31560/34255 [04:46<00:24, 110.14it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31580/34255 [04:46<00:24, 110.18it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31600/34255 [04:46<00:24, 110.21it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31620/34255 [04:46<00:23, 110.24it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31640/34255 [04:46<00:23, 110.28it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31660/34255 [04:46<00:23, 110.32it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  92% 31680/34255 [04:47<00:23, 110.35it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31700/34255 [04:47<00:23, 110.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31720/34255 [04:47<00:22, 110.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31740/34255 [04:47<00:22, 110.46it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31760/34255 [04:47<00:22, 110.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31780/34255 [04:47<00:22, 110.54it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31800/34255 [04:47<00:22, 110.57it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31820/34255 [04:47<00:22, 110.61it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31840/34255 [04:47<00:21, 110.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31860/34255 [04:47<00:21, 110.68it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31880/34255 [04:47<00:21, 110.72it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31900/34255 [04:48<00:21, 110.75it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31920/34255 [04:48<00:21, 110.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31940/34255 [04:48<00:20, 110.82it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31960/34255 [04:48<00:20, 110.86it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 31980/34255 [04:48<00:20, 110.89it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 32000/34255 [04:48<00:20, 110.93it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  93% 32020/34255 [04:48<00:20, 110.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32040/34255 [04:48<00:19, 111.00it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32060/34255 [04:48<00:19, 111.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32080/34255 [04:48<00:19, 111.07it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32100/34255 [04:48<00:19, 111.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32120/34255 [04:48<00:19, 111.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32140/34255 [04:49<00:19, 111.18it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32160/34255 [04:49<00:18, 111.22it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32180/34255 [04:49<00:18, 111.26it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32200/34255 [04:49<00:18, 111.29it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32220/34255 [04:49<00:18, 111.33it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32240/34255 [04:49<00:18, 111.37it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32260/34255 [04:49<00:17, 111.40it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32280/34255 [04:49<00:17, 111.44it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32300/34255 [04:49<00:17, 111.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32320/34255 [04:49<00:17, 111.51it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32340/34255 [04:49<00:17, 111.55it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  94% 32360/34255 [04:50<00:16, 111.58it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32380/34255 [04:50<00:16, 111.62it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32400/34255 [04:50<00:16, 111.65it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32420/34255 [04:50<00:16, 111.69it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32440/34255 [04:50<00:16, 111.72it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32460/34255 [04:50<00:16, 111.76it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32480/34255 [04:50<00:15, 111.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32500/34255 [04:50<00:15, 111.83it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32520/34255 [04:50<00:15, 111.86it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32540/34255 [04:50<00:15, 111.90it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32560/34255 [04:50<00:15, 111.94it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32580/34255 [04:50<00:14, 111.97it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32600/34255 [04:51<00:14, 112.01it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32620/34255 [04:51<00:14, 112.04it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32640/34255 [04:51<00:14, 112.08it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32660/34255 [04:51<00:14, 112.11it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32680/34255 [04:51<00:14, 112.15it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  95% 32700/34255 [04:51<00:13, 112.18it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32720/34255 [04:51<00:13, 112.22it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32740/34255 [04:51<00:13, 112.25it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32760/34255 [04:51<00:13, 112.29it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32780/34255 [04:51<00:13, 112.32it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32800/34255 [04:51<00:12, 112.36it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32820/34255 [04:52<00:12, 112.40it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32840/34255 [04:52<00:12, 112.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32860/34255 [04:52<00:12, 112.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32880/34255 [04:52<00:12, 112.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32900/34255 [04:52<00:12, 112.54it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32920/34255 [04:52<00:11, 112.57it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32940/34255 [04:52<00:11, 112.61it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32960/34255 [04:52<00:11, 112.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 32980/34255 [04:52<00:11, 112.68it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 33000/34255 [04:52<00:11, 112.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 33020/34255 [04:52<00:10, 112.74it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  96% 33040/34255 [04:52<00:10, 112.78it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33060/34255 [04:53<00:10, 112.81it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33080/34255 [04:53<00:10, 112.85it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33100/34255 [04:53<00:10, 112.88it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33120/34255 [04:53<00:10, 112.91it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33140/34255 [04:53<00:09, 112.94it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33160/34255 [04:53<00:09, 112.98it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33180/34255 [04:53<00:09, 113.01it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33200/34255 [04:53<00:09, 113.05it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33220/34255 [04:53<00:09, 113.08it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33240/34255 [04:53<00:08, 113.12it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33260/34255 [04:53<00:08, 113.16it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33280/34255 [04:54<00:08, 113.19it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33300/34255 [04:54<00:08, 113.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33320/34255 [04:54<00:08, 113.26it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33340/34255 [04:54<00:08, 113.30it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33360/34255 [04:54<00:07, 113.33it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  97% 33380/34255 [04:54<00:07, 113.37it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33400/34255 [04:54<00:07, 113.40it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33420/34255 [04:54<00:07, 113.44it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33440/34255 [04:54<00:07, 113.47it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33460/34255 [04:54<00:07, 113.50it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33480/34255 [04:54<00:06, 113.54it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33500/34255 [04:54<00:06, 113.58it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33520/34255 [04:55<00:06, 113.61it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33540/34255 [04:55<00:06, 113.64it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33560/34255 [04:55<00:06, 113.67it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33580/34255 [04:55<00:05, 113.70it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33600/34255 [04:55<00:05, 113.73it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33620/34255 [04:55<00:05, 113.76it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33640/34255 [04:55<00:05, 113.79it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33660/34255 [04:55<00:05, 113.83it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33680/34255 [04:55<00:05, 113.86it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33700/34255 [04:55<00:04, 113.89it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33720/34255 [04:55<00:04, 113.92it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  98% 33740/34255 [04:56<00:04, 113.96it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33760/34255 [04:56<00:04, 113.99it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33780/34255 [04:56<00:04, 114.03it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33800/34255 [04:56<00:03, 114.06it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33820/34255 [04:56<00:03, 114.10it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33840/34255 [04:56<00:03, 114.13it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33860/34255 [04:56<00:03, 114.16it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33880/34255 [04:56<00:03, 114.20it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33900/34255 [04:56<00:03, 114.23it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33920/34255 [04:56<00:02, 114.26it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33940/34255 [04:56<00:02, 114.30it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33960/34255 [04:57<00:02, 114.33it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 33980/34255 [04:57<00:02, 114.36it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 34000/34255 [04:57<00:02, 114.39it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 34020/34255 [04:57<00:02, 114.43it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 34040/34255 [04:57<00:01, 114.46it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 34060/34255 [04:57<00:01, 114.49it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15:  99% 34080/34255 [04:57<00:01, 114.52it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34100/34255 [04:57<00:01, 114.56it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34120/34255 [04:57<00:01, 114.59it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34140/34255 [04:57<00:01, 114.62it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34160/34255 [04:57<00:00, 114.65it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34180/34255 [04:58<00:00, 114.68it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34200/34255 [04:58<00:00, 114.71it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34220/34255 [04:58<00:00, 114.74it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Epoch 15: 100% 34240/34255 [04:58<00:00, 114.78it/s, loss=2.64, v_num=53, val_loss=2.690, avg_val_loss=2.690, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 228.18it/s]\u001b[A\n",
            "Epoch 15: 100% 34255/34255 [04:58<00:00, 114.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27400/34255 [04:23<01:05, 103.97it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/6851 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  80% 27420/34255 [04:27<01:06, 102.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27440/34255 [04:27<01:06, 102.42it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27460/34255 [04:28<01:06, 102.46it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27480/34255 [04:28<01:06, 102.50it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27500/34255 [04:28<01:05, 102.54it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27520/34255 [04:28<01:05, 102.58it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27540/34255 [04:28<01:05, 102.62it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  80% 27560/34255 [04:28<01:05, 102.66it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27580/34255 [04:28<01:04, 102.70it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27600/34255 [04:28<01:04, 102.74it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27620/34255 [04:28<01:04, 102.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27640/34255 [04:28<01:04, 102.83it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27660/34255 [04:28<01:04, 102.87it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27680/34255 [04:28<01:03, 102.91it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27700/34255 [04:29<01:03, 102.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27720/34255 [04:29<01:03, 102.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27740/34255 [04:29<01:03, 103.02it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27760/34255 [04:29<01:03, 103.06it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27780/34255 [04:29<01:02, 103.10it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27800/34255 [04:29<01:02, 103.14it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27820/34255 [04:29<01:02, 103.18it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27840/34255 [04:29<01:02, 103.21it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27860/34255 [04:29<01:01, 103.25it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27880/34255 [04:29<01:01, 103.29it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  81% 27900/34255 [04:30<01:01, 103.33it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 27920/34255 [04:30<01:01, 103.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 27940/34255 [04:30<01:01, 103.41it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 27960/34255 [04:30<01:00, 103.45it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 27980/34255 [04:30<01:00, 103.49it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28000/34255 [04:30<01:00, 103.53it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28020/34255 [04:30<01:00, 103.57it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28040/34255 [04:30<00:59, 103.61it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28060/34255 [04:30<00:59, 103.65it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28080/34255 [04:30<00:59, 103.69it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28100/34255 [04:30<00:59, 103.73it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28120/34255 [04:30<00:59, 103.77it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28140/34255 [04:31<00:58, 103.81it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28160/34255 [04:31<00:58, 103.85it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28180/34255 [04:31<00:58, 103.89it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28200/34255 [04:31<00:58, 103.93it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28220/34255 [04:31<00:58, 103.97it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28240/34255 [04:31<00:57, 104.01it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  82% 28260/34255 [04:31<00:57, 104.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28280/34255 [04:31<00:57, 104.09it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28300/34255 [04:31<00:57, 104.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28320/34255 [04:31<00:56, 104.17it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28340/34255 [04:31<00:56, 104.20it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28360/34255 [04:32<00:56, 104.24it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28380/34255 [04:32<00:56, 104.28it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28400/34255 [04:32<00:56, 104.32it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28420/34255 [04:32<00:55, 104.36it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28440/34255 [04:32<00:55, 104.40it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28460/34255 [04:32<00:55, 104.44it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28480/34255 [04:32<00:55, 104.48it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28500/34255 [04:32<00:55, 104.52it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28520/34255 [04:32<00:54, 104.56it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28540/34255 [04:32<00:54, 104.60it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28560/34255 [04:32<00:54, 104.64it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28580/34255 [04:33<00:54, 104.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  83% 28600/34255 [04:33<00:54, 104.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28620/34255 [04:33<00:53, 104.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28640/34255 [04:33<00:53, 104.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28660/34255 [04:33<00:53, 104.83it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28680/34255 [04:33<00:53, 104.87it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28700/34255 [04:33<00:52, 104.91it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28720/34255 [04:33<00:52, 104.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28740/34255 [04:33<00:52, 104.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28760/34255 [04:33<00:52, 105.01it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28780/34255 [04:33<00:52, 105.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28800/34255 [04:34<00:51, 105.09it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28820/34255 [04:34<00:51, 105.14it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28840/34255 [04:34<00:51, 105.18it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28860/34255 [04:34<00:51, 105.22it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28880/34255 [04:34<00:51, 105.26it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28900/34255 [04:34<00:50, 105.30it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28920/34255 [04:34<00:50, 105.34it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  84% 28940/34255 [04:34<00:50, 105.38it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 28960/34255 [04:34<00:50, 105.42it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 28980/34255 [04:34<00:50, 105.46it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29000/34255 [04:34<00:49, 105.50it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29020/34255 [04:34<00:49, 105.54it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29040/34255 [04:35<00:49, 105.57it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29060/34255 [04:35<00:49, 105.61it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29080/34255 [04:35<00:48, 105.65it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29100/34255 [04:35<00:48, 105.69it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29120/34255 [04:35<00:48, 105.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29140/34255 [04:35<00:48, 105.76it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29160/34255 [04:35<00:48, 105.80it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29180/34255 [04:35<00:47, 105.84it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29200/34255 [04:35<00:47, 105.88it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29220/34255 [04:35<00:47, 105.92it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29240/34255 [04:35<00:47, 105.95it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29260/34255 [04:36<00:47, 105.99it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  85% 29280/34255 [04:36<00:46, 106.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29300/34255 [04:36<00:46, 106.07it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29320/34255 [04:36<00:46, 106.10it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29340/34255 [04:36<00:46, 106.14it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29360/34255 [04:36<00:46, 106.18it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29380/34255 [04:36<00:45, 106.21it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29400/34255 [04:36<00:45, 106.25it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29420/34255 [04:36<00:45, 106.29it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29440/34255 [04:36<00:45, 106.33it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29460/34255 [04:36<00:45, 106.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29480/34255 [04:37<00:44, 106.40it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29500/34255 [04:37<00:44, 106.44it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29520/34255 [04:37<00:44, 106.48it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29540/34255 [04:37<00:44, 106.52it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29560/34255 [04:37<00:44, 106.56it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29580/34255 [04:37<00:43, 106.60it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29600/34255 [04:37<00:43, 106.64it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  86% 29620/34255 [04:37<00:43, 106.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29640/34255 [04:37<00:43, 106.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29660/34255 [04:37<00:43, 106.76it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29680/34255 [04:37<00:42, 106.80it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29700/34255 [04:37<00:42, 106.84it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29720/34255 [04:38<00:42, 106.88it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29740/34255 [04:38<00:42, 106.91it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29760/34255 [04:38<00:42, 106.95it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29780/34255 [04:38<00:41, 106.99it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29800/34255 [04:38<00:41, 107.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29820/34255 [04:38<00:41, 107.07it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29840/34255 [04:38<00:41, 107.11it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29860/34255 [04:38<00:41, 107.15it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29880/34255 [04:38<00:40, 107.18it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29900/34255 [04:38<00:40, 107.22it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29920/34255 [04:38<00:40, 107.26it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29940/34255 [04:39<00:40, 107.30it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  87% 29960/34255 [04:39<00:40, 107.34it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 29980/34255 [04:39<00:39, 107.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30000/34255 [04:39<00:39, 107.41it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30020/34255 [04:39<00:39, 107.45it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30040/34255 [04:39<00:39, 107.49it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30060/34255 [04:39<00:39, 107.52it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30080/34255 [04:39<00:38, 107.56it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30100/34255 [04:39<00:38, 107.60it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30120/34255 [04:39<00:38, 107.64it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30140/34255 [04:39<00:38, 107.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30160/34255 [04:39<00:38, 107.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30180/34255 [04:40<00:37, 107.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30200/34255 [04:40<00:37, 107.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30220/34255 [04:40<00:37, 107.83it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30240/34255 [04:40<00:37, 107.86it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30260/34255 [04:40<00:37, 107.90it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30280/34255 [04:40<00:36, 107.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  88% 30300/34255 [04:40<00:36, 107.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30320/34255 [04:40<00:36, 108.01it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30340/34255 [04:40<00:36, 108.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30360/34255 [04:40<00:36, 108.09it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30380/34255 [04:40<00:35, 108.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30400/34255 [04:41<00:35, 108.17it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30420/34255 [04:41<00:35, 108.21it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30440/34255 [04:41<00:35, 108.25it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30460/34255 [04:41<00:35, 108.29it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30480/34255 [04:41<00:34, 108.32it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30500/34255 [04:41<00:34, 108.36it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30520/34255 [04:41<00:34, 108.40it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30540/34255 [04:41<00:34, 108.44it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30560/34255 [04:41<00:34, 108.48it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30580/34255 [04:41<00:33, 108.52it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30600/34255 [04:41<00:33, 108.56it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30620/34255 [04:41<00:33, 108.59it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  89% 30640/34255 [04:42<00:33, 108.63it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30660/34255 [04:42<00:33, 108.67it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30680/34255 [04:42<00:32, 108.71it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30700/34255 [04:42<00:32, 108.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30720/34255 [04:42<00:32, 108.78it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30740/34255 [04:42<00:32, 108.82it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30760/34255 [04:42<00:32, 108.86it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30780/34255 [04:42<00:31, 108.90it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30800/34255 [04:42<00:31, 108.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30820/34255 [04:42<00:31, 108.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30840/34255 [04:42<00:31, 109.02it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30860/34255 [04:42<00:31, 109.06it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30880/34255 [04:43<00:30, 109.10it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30900/34255 [04:43<00:30, 109.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30920/34255 [04:43<00:30, 109.17it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30940/34255 [04:43<00:30, 109.21it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30960/34255 [04:43<00:30, 109.24it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 30980/34255 [04:43<00:29, 109.28it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  90% 31000/34255 [04:43<00:29, 109.32it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31020/34255 [04:43<00:29, 109.35it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31040/34255 [04:43<00:29, 109.39it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31060/34255 [04:43<00:29, 109.43it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31080/34255 [04:43<00:29, 109.47it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31100/34255 [04:44<00:28, 109.50it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31120/34255 [04:44<00:28, 109.54it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31140/34255 [04:44<00:28, 109.58it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31160/34255 [04:44<00:28, 109.62it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31180/34255 [04:44<00:28, 109.65it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31200/34255 [04:44<00:27, 109.69it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31220/34255 [04:44<00:27, 109.73it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31240/34255 [04:44<00:27, 109.76it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31260/34255 [04:44<00:27, 109.80it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31280/34255 [04:44<00:27, 109.84it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31300/34255 [04:44<00:26, 109.87it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31320/34255 [04:44<00:26, 109.91it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  91% 31340/34255 [04:45<00:26, 109.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31360/34255 [04:45<00:26, 109.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31380/34255 [04:45<00:26, 110.02it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31400/34255 [04:45<00:25, 110.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31420/34255 [04:45<00:25, 110.09it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31440/34255 [04:45<00:25, 110.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31460/34255 [04:45<00:25, 110.16it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31480/34255 [04:45<00:25, 110.20it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31500/34255 [04:45<00:24, 110.23it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31520/34255 [04:45<00:24, 110.27it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31540/34255 [04:45<00:24, 110.31it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31560/34255 [04:46<00:24, 110.34it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31580/34255 [04:46<00:24, 110.38it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31600/34255 [04:46<00:24, 110.42it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31620/34255 [04:46<00:23, 110.45it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31640/34255 [04:46<00:23, 110.49it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31660/34255 [04:46<00:23, 110.53it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  92% 31680/34255 [04:46<00:23, 110.57it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31700/34255 [04:46<00:23, 110.60it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31720/34255 [04:46<00:22, 110.64it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31740/34255 [04:46<00:22, 110.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31760/34255 [04:46<00:22, 110.71it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31780/34255 [04:46<00:22, 110.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31800/34255 [04:47<00:22, 110.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31820/34255 [04:47<00:21, 110.82it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31840/34255 [04:47<00:21, 110.86it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31860/34255 [04:47<00:21, 110.89it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31880/34255 [04:47<00:21, 110.93it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31900/34255 [04:47<00:21, 110.96it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31920/34255 [04:47<00:21, 111.00it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31940/34255 [04:47<00:20, 111.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31960/34255 [04:47<00:20, 111.07it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 31980/34255 [04:47<00:20, 111.11it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 32000/34255 [04:47<00:20, 111.14it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  93% 32020/34255 [04:48<00:20, 111.18it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32040/34255 [04:48<00:19, 111.21it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32060/34255 [04:48<00:19, 111.25it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32080/34255 [04:48<00:19, 111.28it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32100/34255 [04:48<00:19, 111.31it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32120/34255 [04:48<00:19, 111.35it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32140/34255 [04:48<00:18, 111.38it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32160/34255 [04:48<00:18, 111.42it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32180/34255 [04:48<00:18, 111.46it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32200/34255 [04:48<00:18, 111.49it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32220/34255 [04:48<00:18, 111.53it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32240/34255 [04:48<00:18, 111.56it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32260/34255 [04:49<00:17, 111.60it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32280/34255 [04:49<00:17, 111.63it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32300/34255 [04:49<00:17, 111.66it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32320/34255 [04:49<00:17, 111.69it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32340/34255 [04:49<00:17, 111.73it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  94% 32360/34255 [04:49<00:16, 111.76it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32380/34255 [04:49<00:16, 111.80it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32400/34255 [04:49<00:16, 111.83it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32420/34255 [04:49<00:16, 111.87it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32440/34255 [04:49<00:16, 111.90it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32460/34255 [04:49<00:16, 111.94it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32480/34255 [04:50<00:15, 111.98it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32500/34255 [04:50<00:15, 112.01it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32520/34255 [04:50<00:15, 112.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32540/34255 [04:50<00:15, 112.08it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32560/34255 [04:50<00:15, 112.12it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32580/34255 [04:50<00:14, 112.15it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32600/34255 [04:50<00:14, 112.19it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32620/34255 [04:50<00:14, 112.23it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32640/34255 [04:50<00:14, 112.26it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32660/34255 [04:50<00:14, 112.30it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32680/34255 [04:50<00:14, 112.33it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  95% 32700/34255 [04:51<00:13, 112.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32720/34255 [04:51<00:13, 112.40it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32740/34255 [04:51<00:13, 112.44it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32760/34255 [04:51<00:13, 112.47it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32780/34255 [04:51<00:13, 112.51it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32800/34255 [04:51<00:12, 112.54it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32820/34255 [04:51<00:12, 112.58it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32840/34255 [04:51<00:12, 112.61it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32860/34255 [04:51<00:12, 112.65it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32880/34255 [04:51<00:12, 112.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32900/34255 [04:51<00:12, 112.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32920/34255 [04:51<00:11, 112.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32940/34255 [04:52<00:11, 112.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32960/34255 [04:52<00:11, 112.82it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 32980/34255 [04:52<00:11, 112.86it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 33000/34255 [04:52<00:11, 112.89it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 33020/34255 [04:52<00:10, 112.93it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  96% 33040/34255 [04:52<00:10, 112.96it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33060/34255 [04:52<00:10, 113.00it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33080/34255 [04:52<00:10, 113.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33100/34255 [04:52<00:10, 113.07it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33120/34255 [04:52<00:10, 113.10it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33140/34255 [04:52<00:09, 113.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33160/34255 [04:53<00:09, 113.16it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33180/34255 [04:53<00:09, 113.20it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33200/34255 [04:53<00:09, 113.23it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33220/34255 [04:53<00:09, 113.27it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33240/34255 [04:53<00:08, 113.30it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33260/34255 [04:53<00:08, 113.34it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33280/34255 [04:53<00:08, 113.37it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33300/34255 [04:53<00:08, 113.40it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33320/34255 [04:53<00:08, 113.44it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33340/34255 [04:53<00:08, 113.47it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33360/34255 [04:53<00:07, 113.50it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  97% 33380/34255 [04:54<00:07, 113.54it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33400/34255 [04:54<00:07, 113.57it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33420/34255 [04:54<00:07, 113.61it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33440/34255 [04:54<00:07, 113.64it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33460/34255 [04:54<00:06, 113.68it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33480/34255 [04:54<00:06, 113.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33500/34255 [04:54<00:06, 113.75it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33520/34255 [04:54<00:06, 113.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33540/34255 [04:54<00:06, 113.82it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33560/34255 [04:54<00:06, 113.85it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33580/34255 [04:54<00:05, 113.88it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33600/34255 [04:54<00:05, 113.92it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33620/34255 [04:55<00:05, 113.95it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33640/34255 [04:55<00:05, 113.99it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33660/34255 [04:55<00:05, 114.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33680/34255 [04:55<00:05, 114.06it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33700/34255 [04:55<00:04, 114.10it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33720/34255 [04:55<00:04, 114.13it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  98% 33740/34255 [04:55<00:04, 114.17it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33760/34255 [04:55<00:04, 114.20it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33780/34255 [04:55<00:04, 114.24it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33800/34255 [04:55<00:03, 114.28it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33820/34255 [04:55<00:03, 114.31it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33840/34255 [04:55<00:03, 114.35it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33860/34255 [04:56<00:03, 114.38it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33880/34255 [04:56<00:03, 114.41it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33900/34255 [04:56<00:03, 114.45it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33920/34255 [04:56<00:02, 114.48it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33940/34255 [04:56<00:02, 114.51it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33960/34255 [04:56<00:02, 114.55it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 33980/34255 [04:56<00:02, 114.58it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 34000/34255 [04:56<00:02, 114.62it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 34020/34255 [04:56<00:02, 114.65it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 34040/34255 [04:56<00:01, 114.69it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 34060/34255 [04:56<00:01, 114.72it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16:  99% 34080/34255 [04:56<00:01, 114.76it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34100/34255 [04:57<00:01, 114.79it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34120/34255 [04:57<00:01, 114.83it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34140/34255 [04:57<00:01, 114.86it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34160/34255 [04:57<00:00, 114.89it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34180/34255 [04:57<00:00, 114.93it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34200/34255 [04:57<00:00, 114.96it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34220/34255 [04:57<00:00, 115.00it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Epoch 16: 100% 34240/34255 [04:57<00:00, 115.03it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "Validation DataLoader 0: 100% 6840/6851 [00:29<00:00, 228.68it/s]\u001b[A\n",
            "Epoch 16: 100% 34255/34255 [04:57<00:00, 115.05it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.650]\n",
            "                                                                 \u001b[AMonitored metric avg_val_loss did not improve in the last 10 records. Best score: 2.691. Signaling Trainer to stop.\n",
            "Epoch 16: 100% 34255/34255 [04:57<00:00, 114.96it/s, loss=2.64, v_num=53, val_loss=2.700, avg_val_loss=2.700, train_loss=2.640]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 119\n",
            "Shape of the feature representation generated by the base encoder: (5921, 64)\n",
            "end time: 1729476066.6552942\n",
            "Execution time: 1.43 hours\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_train_lineage.h5ad\" \\\n",
        "                                              --batch_size 50 \\\n",
        "                                              --size_factor 0.04 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs50_sf004_biddy_train_test_lineage\" \\\n",
        "                                              --train_test 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXLxalfnumD"
      },
      "source": [
        "**5. extract the features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8HHCBIOWxgQ",
        "outputId": "a0872116-04c8-4905-a28d-2301bc00787f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "usage: feature_extraction.py [-h] [--inputFilePath INPUTFILEPATH] [--batch_size BATCH_SIZE]\n",
            "                             [--size_factor SIZE_FACTOR] [--temperature TEMPERATURE]\n",
            "                             [--patience PATIENCE] [--min_delta MIN_DELTA] [--max_epoch MAX_EPOCH]\n",
            "                             --output_dir OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                             [--hidden_dims HIDDEN_DIMS] [--embedding_size EMBEDDING_SIZE]\n",
            "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGQKLhnAYTs4",
        "outputId": "9c29f1f1-2d31-4553-e4e0-360b27ca29a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "/content/drive/My Drive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs50_sf004_biddy_train_test_lineage/saved_models/scContrastiveLearn_last.ckpt was loaded\n",
            "Shape of the feature representation generated by the base encoder: (613, 64)\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_test_lineage.h5ad\" \\\n",
        "                                              --batch_size 50 \\\n",
        "                                              --size_factor 0.04 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs50_sf004_biddy_train_test_lineage\" \\\n",
        "                                              --train_test 1 \\\n",
        "                                              --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs50_sf004_biddy_train_test_lineage/saved_models/scContrastiveLearn_last.ckpt\"\n",
        "\n",
        "\n",
        "\n",
        "# \"/content/drive/MyDrive/Colab Notebooks/data/Larry_41093_2000_norm_log.h5ad\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "f2dce50a-deab-4a03-c0be-fc97bf4af34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgFzoXTPCVub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5746f90-cb0e-4bda-9bc8-253168baa3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "aa2321e5-01d9-46de-9aaa-3e24d346fa81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.8 pytorch-lightning-2.4.0 torchmetrics-1.5.0\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata)\n",
            "  Downloading array_api_compat-1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.11.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.1)\n",
            "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n",
            "Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: array-api-compat, anndata\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.26.4)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (1.5.0)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.11.8)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (0.19.1+cu121)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from lightning-bolts) (2.17.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.0->lightning-bolts) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (3.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.2.0)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.4.0\n",
            "    Uninstalling pytorch-lightning-2.4.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.4.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.10.9)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.11.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.1)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.5)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.9)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=aa073963800f8006169d96be38f8a8a3589478d82a17dfd0da878d3f66d7ac39\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, session-info, pynndescent, umap-learn, scanpy\n",
            "Successfully installed legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "b8dfd86a-b9f9-4854-9ae6-c610bba5a41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1729473291.6443107\n",
            "usage: scContrastiveLearning_Main_709_ckpt_epoch.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                                                    [--batch_size BATCH_SIZE]\n",
            "                                                    [--size_factor SIZE_FACTOR]\n",
            "                                                    [--temperature TEMPERATURE]\n",
            "                                                    [--patience PATIENCE] [--min_delta MIN_DELTA]\n",
            "                                                    [--max_epoch MAX_EPOCH] --output_dir\n",
            "                                                    OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                                                    [--hidden_dims HIDDEN_DIMS]\n",
            "                                                    [--embedding_size EMBEDDING_SIZE]\n",
            "                                                    [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3V30WCiJDs9I",
        "outputId": "aab73437-b2b9-4397-a9a4-22010018f21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1729474598.9885752\n",
            "-------------------------------INFO-------------------------------\n",
            "Anndata Info:  /content/drive/MyDrive/Colab Notebooks/data/Larry_200_train_lineage.h5ad\n",
            "batch_size:  140\n",
            "size_factor:  0.8\n",
            "temperature:  0.5\n",
            "number of epochs:  220\n",
            "train_test_ratio:  0.8\n",
            "input_dim:  2000\n",
            "hidden_dims:  [1024, 256, 64]\n",
            "embedding_size:  32\n",
            "The range of number of cells in a lineage: (34, 177), average of number of cells in a lineage 56.96\n",
            "number of batches:  12613\n",
            "total number of pairs:  1765820\n",
            "num_workers(number of available CPU cores):  12\n",
            "Training the data with validation set\n",
            "-------------------------------Dataloading-------------------------------\n",
            "number of total batch: 12613\n",
            "number of training batch: 10090\n",
            "number of validation batch: 2523\n",
            "\n",
            "lineage_info shape: (1765820, 1)\n",
            "lineage_info shape of training data: (1412600, 1)\n",
            "lineage_info shape of validation data: (353220, 1)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/content/drive/MyDrive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:105: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=self.config.epochs)\n",
            "\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | AddProjectionMLP | 2.3 M \n",
            "1 | loss  | ContrastiveLoss  | 0     \n",
            "-------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.348     Total estimated model params size (MB)\n",
            "2024-10-21 01:37:46.722793: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-10-21 01:37:47.185746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-21 01:37:47.405082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-21 01:37:47.459873: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-21 01:37:47.793973: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-21 01:37:49.340526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.22it/s]/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 0:  80% 10080/12613 [01:51<00:27, 90.53it/s, loss=5.6, v_num=56] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 10100/12613 [01:55<00:28, 87.50it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  80% 10120/12613 [01:55<00:28, 87.60it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  80% 10140/12613 [01:55<00:28, 87.69it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10160/12613 [01:55<00:27, 87.79it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10180/12613 [01:55<00:27, 87.88it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10200/12613 [01:55<00:27, 87.98it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10220/12613 [01:56<00:27, 88.07it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10240/12613 [01:56<00:26, 88.16it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  81% 10260/12613 [01:56<00:26, 88.25it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10280/12613 [01:56<00:26, 88.35it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10300/12613 [01:56<00:26, 88.44it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10320/12613 [01:56<00:25, 88.53it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10340/12613 [01:56<00:25, 88.63it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10360/12613 [01:56<00:25, 88.72it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10380/12613 [01:56<00:25, 88.81it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  82% 10400/12613 [01:56<00:24, 88.90it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10420/12613 [01:57<00:24, 89.00it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10440/12613 [01:57<00:24, 89.09it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10460/12613 [01:57<00:24, 89.19it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10480/12613 [01:57<00:23, 89.28it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10500/12613 [01:57<00:23, 89.37it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  83% 10520/12613 [01:57<00:23, 89.46it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10540/12613 [01:57<00:23, 89.55it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10560/12613 [01:57<00:22, 89.64it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10580/12613 [01:57<00:22, 89.73it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10600/12613 [01:58<00:22, 89.83it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10620/12613 [01:58<00:22, 89.92it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  84% 10640/12613 [01:58<00:21, 90.01it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10660/12613 [01:58<00:21, 90.10it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10680/12613 [01:58<00:21, 90.18it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10700/12613 [01:58<00:21, 90.27it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10720/12613 [01:58<00:20, 90.36it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10740/12613 [01:58<00:20, 90.45it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10760/12613 [01:58<00:20, 90.54it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  85% 10780/12613 [01:58<00:20, 90.63it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10800/12613 [01:59<00:19, 90.72it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10820/12613 [01:59<00:19, 90.81it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10840/12613 [01:59<00:19, 90.90it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10860/12613 [01:59<00:19, 90.99it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10880/12613 [01:59<00:19, 91.08it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  86% 10900/12613 [01:59<00:18, 91.16it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 10920/12613 [01:59<00:18, 91.25it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 10940/12613 [01:59<00:18, 91.34it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 10960/12613 [01:59<00:18, 91.42it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 10980/12613 [01:59<00:17, 91.51it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 11000/12613 [02:00<00:17, 91.60it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  87% 11020/12613 [02:00<00:17, 91.69it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11040/12613 [02:00<00:17, 91.78it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11060/12613 [02:00<00:16, 91.86it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11080/12613 [02:00<00:16, 91.95it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11100/12613 [02:00<00:16, 92.04it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11120/12613 [02:00<00:16, 92.12it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11140/12613 [02:00<00:15, 92.21it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  88% 11160/12613 [02:00<00:15, 92.30it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11180/12613 [02:01<00:15, 92.39it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11200/12613 [02:01<00:15, 92.47it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11220/12613 [02:01<00:15, 92.56it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11240/12613 [02:01<00:14, 92.65it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11260/12613 [02:01<00:14, 92.73it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  89% 11280/12613 [02:01<00:14, 92.82it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11300/12613 [02:01<00:14, 92.90it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11320/12613 [02:01<00:13, 92.99it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11340/12613 [02:01<00:13, 93.07it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11360/12613 [02:01<00:13, 93.16it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11380/12613 [02:02<00:13, 93.25it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  90% 11400/12613 [02:02<00:12, 93.34it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11420/12613 [02:02<00:12, 93.42it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11440/12613 [02:02<00:12, 93.51it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11460/12613 [02:02<00:12, 93.59it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11480/12613 [02:02<00:12, 93.68it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11500/12613 [02:02<00:11, 93.76it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11520/12613 [02:02<00:11, 93.85it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  91% 11540/12613 [02:02<00:11, 93.93it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11560/12613 [02:02<00:11, 94.02it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11580/12613 [02:03<00:10, 94.10it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11600/12613 [02:03<00:10, 94.18it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11620/12613 [02:03<00:10, 94.26it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11640/12613 [02:03<00:10, 94.34it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  92% 11660/12613 [02:03<00:10, 94.42it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11680/12613 [02:03<00:09, 94.50it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11700/12613 [02:03<00:09, 94.57it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11720/12613 [02:03<00:09, 94.65it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11740/12613 [02:03<00:09, 94.73it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11760/12613 [02:04<00:08, 94.81it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  93% 11780/12613 [02:04<00:08, 94.89it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11800/12613 [02:04<00:08, 94.96it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11820/12613 [02:04<00:08, 95.04it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11840/12613 [02:04<00:08, 95.12it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11860/12613 [02:04<00:07, 95.20it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11880/12613 [02:04<00:07, 95.28it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  94% 11900/12613 [02:04<00:07, 95.36it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 11920/12613 [02:04<00:07, 95.44it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 11940/12613 [02:05<00:07, 95.51it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 11960/12613 [02:05<00:06, 95.59it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 11980/12613 [02:05<00:06, 95.67it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 12000/12613 [02:05<00:06, 95.74it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 12020/12613 [02:05<00:06, 95.82it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  95% 12040/12613 [02:05<00:05, 95.89it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12060/12613 [02:05<00:05, 95.97it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12080/12613 [02:05<00:05, 96.04it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12100/12613 [02:05<00:05, 96.12it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12120/12613 [02:06<00:05, 96.19it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12140/12613 [02:06<00:04, 96.26it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  96% 12160/12613 [02:06<00:04, 96.34it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12180/12613 [02:06<00:04, 96.41it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12200/12613 [02:06<00:04, 96.49it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12220/12613 [02:06<00:04, 96.56it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12240/12613 [02:06<00:03, 96.63it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12260/12613 [02:06<00:03, 96.71it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  97% 12280/12613 [02:06<00:03, 96.79it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12300/12613 [02:06<00:03, 96.86it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12320/12613 [02:07<00:03, 96.92it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12340/12613 [02:07<00:02, 97.00it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12360/12613 [02:07<00:02, 97.07it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12380/12613 [02:07<00:02, 97.13it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12400/12613 [02:07<00:02, 97.20it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  98% 12420/12613 [02:07<00:01, 97.27it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12440/12613 [02:07<00:01, 97.35it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12460/12613 [02:07<00:01, 97.42it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12480/12613 [02:08<00:01, 97.49it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12500/12613 [02:08<00:01, 97.57it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12520/12613 [02:08<00:00, 97.64it/s, loss=5.6, v_num=56]\n",
            "Epoch 0:  99% 12540/12613 [02:08<00:00, 97.72it/s, loss=5.6, v_num=56]\n",
            "Epoch 0: 100% 12560/12613 [02:08<00:00, 97.79it/s, loss=5.6, v_num=56]\n",
            "Epoch 0: 100% 12580/12613 [02:08<00:00, 97.87it/s, loss=5.6, v_num=56]\n",
            "Epoch 0: 100% 12600/12613 [02:08<00:00, 97.94it/s, loss=5.6, v_num=56]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 188.98it/s]\u001b[A\n",
            "Epoch 0: 100% 12613/12613 [02:08<00:00, 97.98it/s, loss=5.59, v_num=56, val_loss=5.610, avg_val_loss=5.610]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved. New best score: 5.606\n",
            "Epoch 1:  80% 10080/12613 [01:55<00:29, 87.22it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 10100/12613 [01:59<00:29, 84.30it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  80% 10120/12613 [01:59<00:29, 84.40it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  80% 10140/12613 [02:00<00:29, 84.49it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10160/12613 [02:00<00:29, 84.58it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10180/12613 [02:00<00:28, 84.67it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10200/12613 [02:00<00:28, 84.76it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10220/12613 [02:00<00:28, 84.86it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10240/12613 [02:00<00:27, 84.95it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  81% 10260/12613 [02:00<00:27, 85.04it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10280/12613 [02:00<00:27, 85.12it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10300/12613 [02:00<00:27, 85.21it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10320/12613 [02:00<00:26, 85.29it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10340/12613 [02:01<00:26, 85.38it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10360/12613 [02:01<00:26, 85.46it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10380/12613 [02:01<00:26, 85.55it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  82% 10400/12613 [02:01<00:25, 85.64it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10420/12613 [02:01<00:25, 85.73it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10440/12613 [02:01<00:25, 85.82it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10460/12613 [02:01<00:25, 85.91it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10480/12613 [02:01<00:24, 86.00it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10500/12613 [02:01<00:24, 86.08it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  83% 10520/12613 [02:02<00:24, 86.16it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10540/12613 [02:02<00:24, 86.25it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10560/12613 [02:02<00:23, 86.34it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10580/12613 [02:02<00:23, 86.43it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10600/12613 [02:02<00:23, 86.51it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10620/12613 [02:02<00:23, 86.60it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  84% 10640/12613 [02:02<00:22, 86.69it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10660/12613 [02:02<00:22, 86.78it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10680/12613 [02:02<00:22, 86.86it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10700/12613 [02:03<00:22, 86.95it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10720/12613 [02:03<00:21, 87.04it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10740/12613 [02:03<00:21, 87.12it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10760/12613 [02:03<00:21, 87.21it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  85% 10780/12613 [02:03<00:20, 87.29it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10800/12613 [02:03<00:20, 87.37it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10820/12613 [02:03<00:20, 87.45it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10840/12613 [02:03<00:20, 87.53it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10860/12613 [02:03<00:20, 87.61it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10880/12613 [02:04<00:19, 87.69it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  86% 10900/12613 [02:04<00:19, 87.77it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 10920/12613 [02:04<00:19, 87.86it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 10940/12613 [02:04<00:19, 87.93it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 10960/12613 [02:04<00:18, 88.01it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 10980/12613 [02:04<00:18, 88.09it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 11000/12613 [02:04<00:18, 88.17it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  87% 11020/12613 [02:04<00:18, 88.26it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11040/12613 [02:04<00:17, 88.34it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11060/12613 [02:05<00:17, 88.42it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11080/12613 [02:05<00:17, 88.50it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11100/12613 [02:05<00:17, 88.58it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11120/12613 [02:05<00:16, 88.67it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11140/12613 [02:05<00:16, 88.73it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  88% 11160/12613 [02:05<00:16, 88.81it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11180/12613 [02:05<00:16, 88.90it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11200/12613 [02:05<00:15, 88.98it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11220/12613 [02:05<00:15, 89.06it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11240/12613 [02:06<00:15, 89.14it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11260/12613 [02:06<00:15, 89.23it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  89% 11280/12613 [02:06<00:14, 89.31it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11300/12613 [02:06<00:14, 89.39it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11320/12613 [02:06<00:14, 89.46it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11340/12613 [02:06<00:14, 89.54it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11360/12613 [02:06<00:13, 89.62it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11380/12613 [02:06<00:13, 89.70it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  90% 11400/12613 [02:06<00:13, 89.78it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11420/12613 [02:07<00:13, 89.86it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11440/12613 [02:07<00:13, 89.94it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11460/12613 [02:07<00:12, 90.02it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11480/12613 [02:07<00:12, 90.10it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11500/12613 [02:07<00:12, 90.18it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11520/12613 [02:07<00:12, 90.25it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  91% 11540/12613 [02:07<00:11, 90.33it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11560/12613 [02:07<00:11, 90.41it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11580/12613 [02:07<00:11, 90.49it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11600/12613 [02:08<00:11, 90.56it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11620/12613 [02:08<00:10, 90.64it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11640/12613 [02:08<00:10, 90.72it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  92% 11660/12613 [02:08<00:10, 90.80it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11680/12613 [02:08<00:10, 90.88it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11700/12613 [02:08<00:10, 90.96it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11720/12613 [02:08<00:09, 91.04it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11740/12613 [02:08<00:09, 91.12it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11760/12613 [02:08<00:09, 91.20it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  93% 11780/12613 [02:09<00:09, 91.28it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11800/12613 [02:09<00:08, 91.36it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11820/12613 [02:09<00:08, 91.43it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11840/12613 [02:09<00:08, 91.51it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11860/12613 [02:09<00:08, 91.59it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11880/12613 [02:09<00:07, 91.67it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  94% 11900/12613 [02:09<00:07, 91.75it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 11920/12613 [02:09<00:07, 91.83it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 11940/12613 [02:09<00:07, 91.90it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 11960/12613 [02:10<00:07, 91.98it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 11980/12613 [02:10<00:06, 92.06it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 12000/12613 [02:10<00:06, 92.14it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 12020/12613 [02:10<00:06, 92.21it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  95% 12040/12613 [02:10<00:06, 92.29it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12060/12613 [02:10<00:05, 92.37it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12080/12613 [02:10<00:05, 92.44it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12100/12613 [02:10<00:05, 92.52it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12120/12613 [02:10<00:05, 92.60it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12140/12613 [02:10<00:05, 92.67it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  96% 12160/12613 [02:11<00:04, 92.75it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12180/12613 [02:11<00:04, 92.83it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12200/12613 [02:11<00:04, 92.91it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12220/12613 [02:11<00:04, 92.99it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12240/12613 [02:11<00:04, 93.06it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12260/12613 [02:11<00:03, 93.14it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  97% 12280/12613 [02:11<00:03, 93.22it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12300/12613 [02:11<00:03, 93.30it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12320/12613 [02:11<00:03, 93.37it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12340/12613 [02:12<00:02, 93.44it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12360/12613 [02:12<00:02, 93.52it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12380/12613 [02:12<00:02, 93.59it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12400/12613 [02:12<00:02, 93.67it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  98% 12420/12613 [02:12<00:02, 93.75it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12440/12613 [02:12<00:01, 93.82it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12460/12613 [02:12<00:01, 93.90it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12480/12613 [02:12<00:01, 93.97it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12500/12613 [02:12<00:01, 94.05it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12520/12613 [02:13<00:00, 94.12it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1:  99% 12540/12613 [02:13<00:00, 94.20it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1: 100% 12560/12613 [02:13<00:00, 94.27it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1: 100% 12580/12613 [02:13<00:00, 94.34it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Epoch 1: 100% 12600/12613 [02:13<00:00, 94.42it/s, loss=4, v_num=56, val_loss=5.610, avg_val_loss=5.610, train_loss=5.600]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.19it/s]\u001b[A\n",
            "Epoch 1: 100% 12613/12613 [02:13<00:00, 94.46it/s, loss=4, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=5.600]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 1.617 >= min_delta = 0.001. New best score: 3.988\n",
            "Epoch 2:  80% 10080/12613 [01:54<00:28, 88.04it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 10100/12613 [01:58<00:29, 84.96it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  80% 10120/12613 [01:58<00:29, 85.06it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  80% 10140/12613 [01:59<00:29, 85.14it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10160/12613 [01:59<00:28, 85.23it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10180/12613 [01:59<00:28, 85.32it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10200/12613 [01:59<00:28, 85.41it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10220/12613 [01:59<00:27, 85.50it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10240/12613 [01:59<00:27, 85.59it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  81% 10260/12613 [01:59<00:27, 85.68it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10280/12613 [01:59<00:27, 85.76it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10300/12613 [01:59<00:26, 85.85it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10320/12613 [02:00<00:26, 85.94it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10340/12613 [02:00<00:26, 86.03it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10360/12613 [02:00<00:26, 86.11it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10380/12613 [02:00<00:25, 86.20it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  82% 10400/12613 [02:00<00:25, 86.28it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10420/12613 [02:00<00:25, 86.37it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10440/12613 [02:00<00:25, 86.45it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10460/12613 [02:00<00:24, 86.54it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10480/12613 [02:00<00:24, 86.62it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10500/12613 [02:01<00:24, 86.71it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  83% 10520/12613 [02:01<00:24, 86.80it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10540/12613 [02:01<00:23, 86.89it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10560/12613 [02:01<00:23, 86.98it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10580/12613 [02:01<00:23, 87.07it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10600/12613 [02:01<00:23, 87.16it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10620/12613 [02:01<00:22, 87.24it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  84% 10640/12613 [02:01<00:22, 87.33it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10660/12613 [02:01<00:22, 87.42it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10680/12613 [02:02<00:22, 87.51it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10700/12613 [02:02<00:21, 87.60it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10720/12613 [02:02<00:21, 87.68it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10740/12613 [02:02<00:21, 87.77it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10760/12613 [02:02<00:21, 87.85it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  85% 10780/12613 [02:02<00:20, 87.94it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10800/12613 [02:02<00:20, 88.03it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10820/12613 [02:02<00:20, 88.11it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10840/12613 [02:02<00:20, 88.20it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10860/12613 [02:03<00:19, 88.29it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10880/12613 [02:03<00:19, 88.37it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  86% 10900/12613 [02:03<00:19, 88.46it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 10920/12613 [02:03<00:19, 88.55it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 10940/12613 [02:03<00:18, 88.63it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 10960/12613 [02:03<00:18, 88.71it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 10980/12613 [02:03<00:18, 88.80it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 11000/12613 [02:03<00:18, 88.88it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  87% 11020/12613 [02:03<00:17, 88.97it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11040/12613 [02:03<00:17, 89.05it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11060/12613 [02:04<00:17, 89.14it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11080/12613 [02:04<00:17, 89.22it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11100/12613 [02:04<00:16, 89.31it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11120/12613 [02:04<00:16, 89.39it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11140/12613 [02:04<00:16, 89.46it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  88% 11160/12613 [02:04<00:16, 89.54it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11180/12613 [02:04<00:15, 89.63it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11200/12613 [02:04<00:15, 89.71it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11220/12613 [02:04<00:15, 89.79it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11240/12613 [02:05<00:15, 89.87it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11260/12613 [02:05<00:15, 89.95it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  89% 11280/12613 [02:05<00:14, 90.03it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11300/12613 [02:05<00:14, 90.11it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11320/12613 [02:05<00:14, 90.19it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11340/12613 [02:05<00:14, 90.27it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11360/12613 [02:05<00:13, 90.35it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11380/12613 [02:05<00:13, 90.44it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  90% 11400/12613 [02:05<00:13, 90.52it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11420/12613 [02:06<00:13, 90.61it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11440/12613 [02:06<00:12, 90.69it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11460/12613 [02:06<00:12, 90.77it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11480/12613 [02:06<00:12, 90.85it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11500/12613 [02:06<00:12, 90.94it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11520/12613 [02:06<00:12, 91.02it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  91% 11540/12613 [02:06<00:11, 91.10it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11560/12613 [02:06<00:11, 91.18it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11580/12613 [02:06<00:11, 91.26it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11600/12613 [02:06<00:11, 91.35it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11620/12613 [02:07<00:10, 91.43it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11640/12613 [02:07<00:10, 91.51it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  92% 11660/12613 [02:07<00:10, 91.59it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11680/12613 [02:07<00:10, 91.68it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11700/12613 [02:07<00:09, 91.76it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11720/12613 [02:07<00:09, 91.85it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11740/12613 [02:07<00:09, 91.93it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11760/12613 [02:07<00:09, 92.01it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  93% 11780/12613 [02:07<00:09, 92.10it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11800/12613 [02:08<00:08, 92.18it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11820/12613 [02:08<00:08, 92.26it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11840/12613 [02:08<00:08, 92.34it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11860/12613 [02:08<00:08, 92.42it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11880/12613 [02:08<00:07, 92.50it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  94% 11900/12613 [02:08<00:07, 92.57it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 11920/12613 [02:08<00:07, 92.65it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 11940/12613 [02:08<00:07, 92.72it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 11960/12613 [02:08<00:07, 92.80it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 11980/12613 [02:08<00:06, 92.87it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 12000/12613 [02:09<00:06, 92.95it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 12020/12613 [02:09<00:06, 93.02it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  95% 12040/12613 [02:09<00:06, 93.09it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12060/12613 [02:09<00:05, 93.16it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12080/12613 [02:09<00:05, 93.24it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12100/12613 [02:09<00:05, 93.31it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12120/12613 [02:09<00:05, 93.38it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12140/12613 [02:09<00:05, 93.45it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  96% 12160/12613 [02:10<00:04, 93.52it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12180/12613 [02:10<00:04, 93.60it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12200/12613 [02:10<00:04, 93.67it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12220/12613 [02:10<00:04, 93.67it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12240/12613 [02:10<00:03, 93.74it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12260/12613 [02:10<00:03, 93.82it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  97% 12280/12613 [02:10<00:03, 93.90it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12300/12613 [02:10<00:03, 93.98it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12320/12613 [02:10<00:03, 94.06it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12340/12613 [02:11<00:02, 94.14it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12360/12613 [02:11<00:02, 94.21it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12380/12613 [02:11<00:02, 94.29it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12400/12613 [02:11<00:02, 94.37it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  98% 12420/12613 [02:11<00:02, 94.45it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12440/12613 [02:11<00:01, 94.52it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12460/12613 [02:11<00:01, 94.60it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12480/12613 [02:11<00:01, 94.68it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12500/12613 [02:11<00:01, 94.75it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12520/12613 [02:12<00:00, 94.83it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2:  99% 12540/12613 [02:12<00:00, 94.90it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2: 100% 12560/12613 [02:12<00:00, 94.98it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2: 100% 12580/12613 [02:12<00:00, 95.05it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Epoch 2: 100% 12600/12613 [02:12<00:00, 95.13it/s, loss=3.82, v_num=56, val_loss=3.990, avg_val_loss=3.990, train_loss=4.310]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.93it/s]\u001b[A\n",
            "Epoch 2: 100% 12613/12613 [02:12<00:00, 95.17it/s, loss=3.82, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=4.310]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.174 >= min_delta = 0.001. New best score: 3.814\n",
            "Epoch 3:  80% 10080/12613 [01:53<00:28, 88.58it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 10100/12613 [01:58<00:29, 85.34it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  80% 10120/12613 [01:58<00:29, 85.44it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  80% 10140/12613 [01:58<00:28, 85.53it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10160/12613 [01:58<00:28, 85.62it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10180/12613 [01:58<00:28, 85.71it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10200/12613 [01:58<00:28, 85.80it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10220/12613 [01:58<00:27, 85.89it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10240/12613 [01:59<00:27, 85.98it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  81% 10260/12613 [01:59<00:27, 86.07it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10280/12613 [01:59<00:27, 86.16it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10300/12613 [01:59<00:26, 86.25it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10320/12613 [01:59<00:26, 86.34it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10340/12613 [01:59<00:26, 86.43it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10360/12613 [01:59<00:26, 86.52it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10380/12613 [01:59<00:25, 86.61it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  82% 10400/12613 [01:59<00:25, 86.70it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10420/12613 [02:00<00:25, 86.78it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10440/12613 [02:00<00:25, 86.87it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10460/12613 [02:00<00:24, 86.87it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10480/12613 [02:00<00:24, 86.96it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10500/12613 [02:00<00:24, 87.05it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  83% 10520/12613 [02:00<00:24, 87.13it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10540/12613 [02:00<00:23, 87.22it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10560/12613 [02:00<00:23, 87.31it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10580/12613 [02:01<00:23, 87.40it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10600/12613 [02:01<00:23, 87.49it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10620/12613 [02:01<00:22, 87.58it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  84% 10640/12613 [02:01<00:22, 87.67it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10660/12613 [02:01<00:22, 87.75it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10680/12613 [02:01<00:22, 87.84it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10700/12613 [02:01<00:21, 87.93it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10720/12613 [02:01<00:21, 88.01it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10740/12613 [02:01<00:21, 88.10it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10760/12613 [02:02<00:21, 88.18it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  85% 10780/12613 [02:02<00:20, 88.27it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10800/12613 [02:02<00:20, 88.36it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10820/12613 [02:02<00:20, 88.44it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10840/12613 [02:02<00:20, 88.52it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10860/12613 [02:02<00:19, 88.61it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10880/12613 [02:02<00:19, 88.69it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  86% 10900/12613 [02:02<00:19, 88.77it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 10920/12613 [02:02<00:19, 88.85it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 10940/12613 [02:03<00:18, 88.93it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 10960/12613 [02:03<00:18, 89.02it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 10980/12613 [02:03<00:18, 89.11it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 11000/12613 [02:03<00:18, 89.20it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  87% 11020/12613 [02:03<00:17, 89.29it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11040/12613 [02:03<00:17, 89.37it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11060/12613 [02:03<00:17, 89.45it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11080/12613 [02:03<00:17, 89.54it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11100/12613 [02:03<00:16, 89.62it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11120/12613 [02:03<00:16, 89.71it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11140/12613 [02:04<00:16, 89.79it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  88% 11160/12613 [02:04<00:16, 89.87it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11180/12613 [02:04<00:15, 89.95it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11200/12613 [02:04<00:15, 90.04it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11220/12613 [02:04<00:15, 90.12it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11240/12613 [02:04<00:15, 90.21it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11260/12613 [02:04<00:14, 90.29it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  89% 11280/12613 [02:04<00:14, 90.38it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11300/12613 [02:04<00:14, 90.46it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11320/12613 [02:05<00:14, 90.55it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11340/12613 [02:05<00:14, 90.63it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11360/12613 [02:05<00:13, 90.72it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11380/12613 [02:05<00:13, 90.80it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  90% 11400/12613 [02:05<00:13, 90.89it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11420/12613 [02:05<00:13, 90.98it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11440/12613 [02:05<00:12, 91.06it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11460/12613 [02:05<00:12, 91.15it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11480/12613 [02:05<00:12, 91.23it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11500/12613 [02:05<00:12, 91.32it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11520/12613 [02:06<00:11, 91.40it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  91% 11540/12613 [02:06<00:11, 91.48it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11560/12613 [02:06<00:11, 91.57it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11580/12613 [02:06<00:11, 91.65it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11600/12613 [02:06<00:11, 91.73it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11620/12613 [02:06<00:10, 91.82it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11640/12613 [02:06<00:10, 91.90it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  92% 11660/12613 [02:06<00:10, 91.98it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11680/12613 [02:06<00:10, 92.06it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11700/12613 [02:06<00:09, 92.14it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11720/12613 [02:07<00:09, 92.23it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11740/12613 [02:07<00:09, 92.31it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11760/12613 [02:07<00:09, 92.39it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  93% 11780/12613 [02:07<00:09, 92.47it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11800/12613 [02:07<00:08, 92.55it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11820/12613 [02:07<00:08, 92.63it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11840/12613 [02:07<00:08, 92.72it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11860/12613 [02:07<00:08, 92.80it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11880/12613 [02:07<00:07, 92.88it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  94% 11900/12613 [02:08<00:07, 92.95it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 11920/12613 [02:08<00:07, 93.02it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 11940/12613 [02:08<00:07, 93.10it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 11960/12613 [02:08<00:07, 93.18it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 11980/12613 [02:08<00:06, 93.26it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 12000/12613 [02:08<00:06, 93.34it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 12020/12613 [02:08<00:06, 93.42it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  95% 12040/12613 [02:08<00:06, 93.50it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12060/12613 [02:08<00:05, 93.58it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12080/12613 [02:08<00:05, 93.65it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12100/12613 [02:09<00:05, 93.73it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12120/12613 [02:09<00:05, 93.81it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12140/12613 [02:09<00:05, 93.89it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  96% 12160/12613 [02:09<00:04, 93.96it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12180/12613 [02:09<00:04, 94.04it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12200/12613 [02:09<00:04, 94.12it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12220/12613 [02:09<00:04, 94.19it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12240/12613 [02:09<00:03, 94.27it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12260/12613 [02:09<00:03, 94.35it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  97% 12280/12613 [02:10<00:03, 94.42it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12300/12613 [02:10<00:03, 94.49it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12320/12613 [02:10<00:03, 94.56it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12340/12613 [02:10<00:02, 94.64it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12360/12613 [02:10<00:02, 94.71it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12380/12613 [02:10<00:02, 94.79it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12400/12613 [02:10<00:02, 94.86it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  98% 12420/12613 [02:10<00:02, 94.93it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12440/12613 [02:10<00:01, 95.00it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12460/12613 [02:11<00:01, 95.06it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12480/12613 [02:11<00:01, 95.13it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12500/12613 [02:11<00:01, 95.21it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12520/12613 [02:11<00:00, 95.28it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3:  99% 12540/12613 [02:11<00:00, 95.36it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3: 100% 12560/12613 [02:11<00:00, 95.43it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3: 100% 12580/12613 [02:11<00:00, 95.50it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Epoch 3: 100% 12600/12613 [02:11<00:00, 95.58it/s, loss=3.75, v_num=56, val_loss=3.810, avg_val_loss=3.810, train_loss=3.890]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 185.19it/s]\u001b[A\n",
            "Epoch 3: 100% 12613/12613 [02:11<00:00, 95.62it/s, loss=3.75, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.890]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.066 >= min_delta = 0.001. New best score: 3.748\n",
            "Epoch 4:  80% 10080/12613 [01:55<00:28, 87.47it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 10100/12613 [01:59<00:29, 84.37it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  80% 10120/12613 [01:59<00:29, 84.47it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  80% 10140/12613 [01:59<00:29, 84.56it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10160/12613 [02:00<00:28, 84.64it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10180/12613 [02:00<00:28, 84.72it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10200/12613 [02:00<00:28, 84.81it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10220/12613 [02:00<00:28, 84.90it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10240/12613 [02:00<00:27, 84.98it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  81% 10260/12613 [02:00<00:27, 85.07it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10280/12613 [02:00<00:27, 85.16it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10300/12613 [02:00<00:27, 85.25it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10320/12613 [02:00<00:26, 85.34it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10340/12613 [02:01<00:26, 85.43it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10360/12613 [02:01<00:26, 85.52it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10380/12613 [02:01<00:26, 85.60it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  82% 10400/12613 [02:01<00:25, 85.69it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10420/12613 [02:01<00:25, 85.78it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10440/12613 [02:01<00:25, 85.87it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10460/12613 [02:01<00:25, 85.96it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10480/12613 [02:01<00:24, 86.05it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10500/12613 [02:01<00:24, 86.14it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  83% 10520/12613 [02:02<00:24, 86.23it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10540/12613 [02:02<00:24, 86.32it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10560/12613 [02:02<00:23, 86.40it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10580/12613 [02:02<00:23, 86.49it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10600/12613 [02:02<00:23, 86.58it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10620/12613 [02:02<00:22, 86.67it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  84% 10640/12613 [02:02<00:22, 86.76it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10660/12613 [02:02<00:22, 86.84it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10680/12613 [02:02<00:22, 86.93it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10700/12613 [02:02<00:21, 87.01it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10720/12613 [02:03<00:21, 87.10it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10740/12613 [02:03<00:21, 87.18it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10760/12613 [02:03<00:21, 87.27it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  85% 10780/12613 [02:03<00:20, 87.35it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10800/12613 [02:03<00:20, 87.43it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10820/12613 [02:03<00:20, 87.52it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10840/12613 [02:03<00:20, 87.60it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10860/12613 [02:03<00:19, 87.68it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10880/12613 [02:03<00:19, 87.77it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  86% 10900/12613 [02:04<00:19, 87.85it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 10920/12613 [02:04<00:19, 87.93it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 10940/12613 [02:04<00:19, 88.02it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 10960/12613 [02:04<00:18, 88.10it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 10980/12613 [02:04<00:18, 88.19it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 11000/12613 [02:04<00:18, 88.27it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  87% 11020/12613 [02:04<00:18, 88.35it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11040/12613 [02:04<00:17, 88.44it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11060/12613 [02:04<00:17, 88.52it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11080/12613 [02:05<00:17, 88.61it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11100/12613 [02:05<00:17, 88.69it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11120/12613 [02:05<00:16, 88.77it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11140/12613 [02:05<00:16, 88.85it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  88% 11160/12613 [02:05<00:16, 88.93it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11180/12613 [02:05<00:16, 89.00it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11200/12613 [02:05<00:15, 89.08it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11220/12613 [02:05<00:15, 89.16it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11240/12613 [02:05<00:15, 89.24it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11260/12613 [02:06<00:15, 89.32it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  89% 11280/12613 [02:06<00:14, 89.40it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11300/12613 [02:06<00:14, 89.48it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11320/12613 [02:06<00:14, 89.56it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11340/12613 [02:06<00:14, 89.64it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11360/12613 [02:06<00:13, 89.72it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11380/12613 [02:06<00:13, 89.80it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  90% 11400/12613 [02:06<00:13, 89.88it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11420/12613 [02:06<00:13, 89.96it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11440/12613 [02:07<00:13, 90.04it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11460/12613 [02:07<00:12, 90.11it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11480/12613 [02:07<00:12, 90.19it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11500/12613 [02:07<00:12, 90.27it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11520/12613 [02:07<00:12, 90.35it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  91% 11540/12613 [02:07<00:11, 90.43it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11560/12613 [02:07<00:11, 90.52it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11580/12613 [02:07<00:11, 90.59it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11600/12613 [02:07<00:11, 90.67it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11620/12613 [02:08<00:10, 90.75it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11640/12613 [02:08<00:10, 90.82it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  92% 11660/12613 [02:08<00:10, 90.89it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11680/12613 [02:08<00:10, 90.97it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11700/12613 [02:08<00:10, 91.04it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11720/12613 [02:08<00:09, 91.12it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11740/12613 [02:08<00:09, 91.20it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11760/12613 [02:08<00:09, 91.28it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  93% 11780/12613 [02:08<00:09, 91.36it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11800/12613 [02:09<00:08, 91.43it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11820/12613 [02:09<00:08, 91.51it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11840/12613 [02:09<00:08, 91.59it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11860/12613 [02:09<00:08, 91.67it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11880/12613 [02:09<00:07, 91.75it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  94% 11900/12613 [02:09<00:07, 91.82it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 11920/12613 [02:09<00:07, 91.88it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 11940/12613 [02:09<00:07, 91.95it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 11960/12613 [02:09<00:07, 92.02it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 11980/12613 [02:10<00:06, 92.10it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 12000/12613 [02:10<00:06, 92.18it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 12020/12613 [02:10<00:06, 92.25it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  95% 12040/12613 [02:10<00:06, 92.33it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12060/12613 [02:10<00:05, 92.40it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12080/12613 [02:10<00:05, 92.47it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12100/12613 [02:10<00:05, 92.54it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12120/12613 [02:10<00:05, 92.62it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12140/12613 [02:10<00:05, 92.70it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  96% 12160/12613 [02:11<00:04, 92.77it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12180/12613 [02:11<00:04, 92.85it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12200/12613 [02:11<00:04, 92.93it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12220/12613 [02:11<00:04, 93.01it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12240/12613 [02:11<00:04, 93.09it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12260/12613 [02:11<00:03, 93.16it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  97% 12280/12613 [02:11<00:03, 93.23it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12300/12613 [02:11<00:03, 93.31it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12320/12613 [02:11<00:03, 93.38it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12340/12613 [02:12<00:02, 93.45it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12360/12613 [02:12<00:02, 93.53it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12380/12613 [02:12<00:02, 93.61it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12400/12613 [02:12<00:02, 93.69it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  98% 12420/12613 [02:12<00:02, 93.77it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12440/12613 [02:12<00:01, 93.84it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12460/12613 [02:12<00:01, 93.92it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12480/12613 [02:12<00:01, 94.00it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12500/12613 [02:12<00:01, 94.07it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12520/12613 [02:12<00:00, 94.15it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4:  99% 12540/12613 [02:13<00:00, 94.22it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4: 100% 12560/12613 [02:13<00:00, 94.30it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4: 100% 12580/12613 [02:13<00:00, 94.37it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Epoch 4: 100% 12600/12613 [02:13<00:00, 94.45it/s, loss=3.72, v_num=56, val_loss=3.750, avg_val_loss=3.750, train_loss=3.780]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 182.30it/s]\u001b[A\n",
            "Epoch 4: 100% 12613/12613 [02:13<00:00, 94.49it/s, loss=3.72, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.780]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.029 >= min_delta = 0.001. New best score: 3.720\n",
            "Epoch 5:  80% 10080/12613 [01:55<00:29, 87.33it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 10100/12613 [01:59<00:29, 84.21it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  80% 10120/12613 [02:00<00:29, 84.30it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  80% 10140/12613 [02:00<00:29, 84.39it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10160/12613 [02:00<00:29, 84.47it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10180/12613 [02:00<00:28, 84.55it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10200/12613 [02:00<00:28, 84.64it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10220/12613 [02:00<00:28, 84.72it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10240/12613 [02:00<00:27, 84.80it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  81% 10260/12613 [02:00<00:27, 84.89it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10280/12613 [02:00<00:27, 84.97it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10300/12613 [02:01<00:27, 85.06it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10320/12613 [02:01<00:26, 85.16it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10340/12613 [02:01<00:26, 85.25it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10360/12613 [02:01<00:26, 85.34it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10380/12613 [02:01<00:26, 85.43it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  82% 10400/12613 [02:01<00:25, 85.52it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10420/12613 [02:01<00:25, 85.61it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10440/12613 [02:01<00:25, 85.69it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10460/12613 [02:01<00:25, 85.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10480/12613 [02:02<00:24, 85.84it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10500/12613 [02:02<00:24, 85.92it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  83% 10520/12613 [02:02<00:24, 86.01it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10540/12613 [02:02<00:24, 86.09it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10560/12613 [02:02<00:23, 86.17it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10580/12613 [02:02<00:23, 86.26it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10600/12613 [02:02<00:23, 86.34it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10620/12613 [02:02<00:23, 86.42it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  84% 10640/12613 [02:02<00:22, 86.51it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10660/12613 [02:03<00:22, 86.60it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10680/12613 [02:03<00:22, 86.68it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10700/12613 [02:03<00:22, 86.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10720/12613 [02:03<00:21, 86.85it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10740/12613 [02:03<00:21, 86.94it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10760/12613 [02:03<00:21, 87.02it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  85% 10780/12613 [02:03<00:21, 87.11it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10800/12613 [02:03<00:20, 87.20it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10820/12613 [02:03<00:20, 87.28it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10840/12613 [02:04<00:20, 87.36it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10860/12613 [02:04<00:20, 87.44it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10880/12613 [02:04<00:19, 87.53it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  86% 10900/12613 [02:04<00:19, 87.61it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 10920/12613 [02:04<00:19, 87.69it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 10940/12613 [02:04<00:19, 87.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 10960/12613 [02:04<00:18, 87.85it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 10980/12613 [02:04<00:18, 87.94it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 11000/12613 [02:04<00:18, 88.02it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  87% 11020/12613 [02:05<00:18, 88.10it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11040/12613 [02:05<00:17, 88.19it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11060/12613 [02:05<00:17, 88.27it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11080/12613 [02:05<00:17, 88.35it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11100/12613 [02:05<00:17, 88.43it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11120/12613 [02:05<00:16, 88.52it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11140/12613 [02:05<00:16, 88.60it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  88% 11160/12613 [02:05<00:16, 88.69it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11180/12613 [02:05<00:16, 88.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11200/12613 [02:06<00:15, 88.85it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11220/12613 [02:06<00:15, 88.93it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11240/12613 [02:06<00:15, 89.02it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11260/12613 [02:06<00:15, 89.10it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  89% 11280/12613 [02:06<00:14, 89.18it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11300/12613 [02:06<00:14, 89.26it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11320/12613 [02:06<00:14, 89.34it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11340/12613 [02:06<00:14, 89.42it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11360/12613 [02:06<00:13, 89.50it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11380/12613 [02:07<00:13, 89.58it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  90% 11400/12613 [02:07<00:13, 89.67it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11420/12613 [02:07<00:13, 89.75it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11440/12613 [02:07<00:13, 89.83it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11460/12613 [02:07<00:12, 89.91it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11480/12613 [02:07<00:12, 89.99it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11500/12613 [02:07<00:12, 90.07it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11520/12613 [02:07<00:12, 90.15it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  91% 11540/12613 [02:07<00:11, 90.23it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11560/12613 [02:07<00:11, 90.32it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11580/12613 [02:08<00:11, 90.40it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11600/12613 [02:08<00:11, 90.48it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11620/12613 [02:08<00:10, 90.56it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11640/12613 [02:08<00:10, 90.64it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  92% 11660/12613 [02:08<00:10, 90.72it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11680/12613 [02:08<00:10, 90.80it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11700/12613 [02:08<00:10, 90.88it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11720/12613 [02:08<00:09, 90.97it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11740/12613 [02:08<00:09, 91.05it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11760/12613 [02:09<00:09, 91.13it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  93% 11780/12613 [02:09<00:09, 91.21it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11800/12613 [02:09<00:08, 91.29it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11820/12613 [02:09<00:08, 91.37it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11840/12613 [02:09<00:08, 91.45it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11860/12613 [02:09<00:08, 91.53it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11880/12613 [02:09<00:08, 91.61it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  94% 11900/12613 [02:09<00:07, 91.69it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 11920/12613 [02:09<00:07, 91.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 11940/12613 [02:09<00:07, 91.85it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 11960/12613 [02:10<00:07, 91.93it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 11980/12613 [02:10<00:06, 92.01it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 12000/12613 [02:10<00:06, 92.09it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 12020/12613 [02:10<00:06, 92.16it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  95% 12040/12613 [02:10<00:06, 92.24it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12060/12613 [02:10<00:05, 92.32it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12080/12613 [02:10<00:05, 92.40it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12100/12613 [02:10<00:05, 92.48it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12120/12613 [02:10<00:05, 92.56it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12140/12613 [02:11<00:05, 92.64it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  96% 12160/12613 [02:11<00:04, 92.72it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12180/12613 [02:11<00:04, 92.80it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12200/12613 [02:11<00:04, 92.87it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12220/12613 [02:11<00:04, 92.95it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12240/12613 [02:11<00:04, 93.03it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12260/12613 [02:11<00:03, 93.10it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  97% 12280/12613 [02:11<00:03, 93.18it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12300/12613 [02:11<00:03, 93.25it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12320/12613 [02:12<00:03, 93.33it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12340/12613 [02:12<00:02, 93.40it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12360/12613 [02:12<00:02, 93.48it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12380/12613 [02:12<00:02, 93.55it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12400/12613 [02:12<00:02, 93.62it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  98% 12420/12613 [02:12<00:02, 93.70it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12440/12613 [02:12<00:01, 93.77it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12460/12613 [02:12<00:01, 93.85it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12480/12613 [02:12<00:01, 93.92it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12500/12613 [02:12<00:01, 93.99it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12520/12613 [02:13<00:00, 94.07it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5:  99% 12540/12613 [02:13<00:00, 94.14it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5: 100% 12560/12613 [02:13<00:00, 94.21it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5: 100% 12580/12613 [02:13<00:00, 94.29it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Epoch 5: 100% 12600/12613 [02:13<00:00, 94.36it/s, loss=3.7, v_num=56, val_loss=3.720, avg_val_loss=3.720, train_loss=3.730]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.79it/s]\u001b[A\n",
            "Epoch 5: 100% 12613/12613 [02:13<00:00, 94.29it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.730]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.015 >= min_delta = 0.001. New best score: 3.705\n",
            "Epoch 6:  80% 10080/12613 [01:55<00:29, 87.18it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 10100/12613 [02:00<00:29, 84.04it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  80% 10120/12613 [02:00<00:29, 84.13it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  80% 10140/12613 [02:00<00:29, 84.22it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10160/12613 [02:00<00:29, 84.31it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10180/12613 [02:00<00:28, 84.40it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10200/12613 [02:00<00:28, 84.49it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10220/12613 [02:00<00:28, 84.58it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10240/12613 [02:00<00:28, 84.67it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  81% 10260/12613 [02:01<00:27, 84.76it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10280/12613 [02:01<00:27, 84.85it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10300/12613 [02:01<00:27, 84.94it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10320/12613 [02:01<00:26, 85.02it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10340/12613 [02:01<00:26, 85.11it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10360/12613 [02:01<00:26, 85.19it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10380/12613 [02:01<00:26, 85.29it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  82% 10400/12613 [02:01<00:25, 85.38it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10420/12613 [02:01<00:25, 85.47it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10440/12613 [02:02<00:25, 85.55it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10460/12613 [02:02<00:25, 85.64it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10480/12613 [02:02<00:24, 85.73it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10500/12613 [02:02<00:24, 85.82it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  83% 10520/12613 [02:02<00:24, 85.91it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10540/12613 [02:02<00:24, 86.00it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10560/12613 [02:02<00:23, 86.08it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10580/12613 [02:02<00:23, 86.17it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10600/12613 [02:02<00:23, 86.26it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10620/12613 [02:03<00:23, 86.34it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  84% 10640/12613 [02:03<00:22, 86.42it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10660/12613 [02:03<00:22, 86.51it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10680/12613 [02:03<00:22, 86.60it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10700/12613 [02:03<00:22, 86.68it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10720/12613 [02:03<00:21, 86.77it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10740/12613 [02:03<00:21, 86.85it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10760/12613 [02:03<00:21, 86.94it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  85% 10780/12613 [02:03<00:21, 87.02it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10800/12613 [02:03<00:20, 87.10it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10820/12613 [02:04<00:20, 87.19it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10840/12613 [02:04<00:20, 87.27it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10860/12613 [02:04<00:20, 87.35it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10880/12613 [02:04<00:19, 87.44it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  86% 10900/12613 [02:04<00:19, 87.52it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 10920/12613 [02:04<00:19, 87.60it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 10940/12613 [02:04<00:19, 87.68it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 10960/12613 [02:04<00:18, 87.77it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 10980/12613 [02:04<00:18, 87.85it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 11000/12613 [02:05<00:18, 87.93it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  87% 11020/12613 [02:05<00:18, 88.01it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11040/12613 [02:05<00:17, 88.10it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11060/12613 [02:05<00:17, 88.18it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11080/12613 [02:05<00:17, 88.26it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11100/12613 [02:05<00:17, 88.34it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11120/12613 [02:05<00:16, 88.41it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11140/12613 [02:05<00:16, 88.50it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  88% 11160/12613 [02:05<00:16, 88.58it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11180/12613 [02:06<00:16, 88.66it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11200/12613 [02:06<00:15, 88.75it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11220/12613 [02:06<00:15, 88.83it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11240/12613 [02:06<00:15, 88.91it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11260/12613 [02:06<00:15, 88.99it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  89% 11280/12613 [02:06<00:14, 89.07it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11300/12613 [02:06<00:14, 89.15it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11320/12613 [02:06<00:14, 89.23it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11340/12613 [02:06<00:14, 89.32it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11360/12613 [02:07<00:14, 89.40it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11380/12613 [02:07<00:13, 89.49it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  90% 11400/12613 [02:07<00:13, 89.57it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11420/12613 [02:07<00:13, 89.65it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11440/12613 [02:07<00:13, 89.74it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11460/12613 [02:07<00:12, 89.82it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11480/12613 [02:07<00:12, 89.90it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11500/12613 [02:07<00:12, 89.98it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11520/12613 [02:07<00:12, 90.07it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  91% 11540/12613 [02:08<00:11, 90.15it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11560/12613 [02:08<00:11, 90.23it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11580/12613 [02:08<00:11, 90.31it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11600/12613 [02:08<00:11, 90.39it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11620/12613 [02:08<00:10, 90.47it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11640/12613 [02:08<00:10, 90.55it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  92% 11660/12613 [02:08<00:10, 90.63it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11680/12613 [02:08<00:10, 90.71it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11700/12613 [02:08<00:10, 90.79it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11720/12613 [02:08<00:09, 90.87it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11740/12613 [02:09<00:09, 90.96it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11760/12613 [02:09<00:09, 91.04it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  93% 11780/12613 [02:09<00:09, 91.12it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11800/12613 [02:09<00:08, 91.20it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11820/12613 [02:09<00:08, 91.28it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11840/12613 [02:09<00:08, 91.37it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11860/12613 [02:09<00:08, 91.45it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11880/12613 [02:09<00:08, 91.53it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  94% 11900/12613 [02:09<00:07, 91.60it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 11920/12613 [02:10<00:07, 91.68it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 11940/12613 [02:10<00:07, 91.76it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 11960/12613 [02:10<00:07, 91.84it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 11980/12613 [02:10<00:06, 91.91it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 12000/12613 [02:10<00:06, 91.99it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 12020/12613 [02:10<00:06, 92.06it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  95% 12040/12613 [02:10<00:06, 92.13it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12060/12613 [02:10<00:05, 92.21it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12080/12613 [02:10<00:05, 92.28it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12100/12613 [02:11<00:05, 92.36it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12120/12613 [02:11<00:05, 92.43it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12140/12613 [02:11<00:05, 92.51it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  96% 12160/12613 [02:11<00:04, 92.58it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12180/12613 [02:11<00:04, 92.66it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12200/12613 [02:11<00:04, 92.74it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12220/12613 [02:11<00:04, 92.81it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12240/12613 [02:11<00:04, 92.89it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12260/12613 [02:11<00:03, 92.96it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  97% 12280/12613 [02:11<00:03, 93.04it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12300/12613 [02:12<00:03, 93.11it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12320/12613 [02:12<00:03, 93.19it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12340/12613 [02:12<00:02, 93.26it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12360/12613 [02:12<00:02, 93.34it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12380/12613 [02:12<00:02, 93.41it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12400/12613 [02:12<00:02, 93.49it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  98% 12420/12613 [02:12<00:02, 93.56it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12440/12613 [02:12<00:01, 93.63it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12460/12613 [02:12<00:01, 93.71it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12480/12613 [02:13<00:01, 93.78it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12500/12613 [02:13<00:01, 93.85it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12520/12613 [02:13<00:00, 93.92it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6:  99% 12540/12613 [02:13<00:00, 93.99it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6: 100% 12560/12613 [02:13<00:00, 94.06it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6: 100% 12580/12613 [02:13<00:00, 94.14it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Epoch 6: 100% 12600/12613 [02:13<00:00, 94.21it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 184.24it/s]\u001b[A\n",
            "Epoch 6: 100% 12613/12613 [02:13<00:00, 94.25it/s, loss=3.7, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.710]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.006 >= min_delta = 0.001. New best score: 3.698\n",
            "Epoch 7:  80% 10080/12613 [01:56<00:29, 86.89it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 10100/12613 [02:00<00:29, 83.80it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  80% 10120/12613 [02:00<00:29, 83.90it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  80% 10140/12613 [02:00<00:29, 83.99it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10160/12613 [02:00<00:29, 84.08it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10180/12613 [02:00<00:28, 84.18it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10200/12613 [02:01<00:28, 84.27it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10220/12613 [02:01<00:28, 84.36it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10240/12613 [02:01<00:28, 84.45it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  81% 10260/12613 [02:01<00:27, 84.54it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10280/12613 [02:01<00:27, 84.62it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10300/12613 [02:01<00:27, 84.71it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10320/12613 [02:01<00:27, 84.80it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10340/12613 [02:01<00:26, 84.89it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10360/12613 [02:01<00:26, 84.98it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10380/12613 [02:02<00:26, 85.07it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  82% 10400/12613 [02:02<00:25, 85.16it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10420/12613 [02:02<00:25, 85.24it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10440/12613 [02:02<00:25, 85.33it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10460/12613 [02:02<00:25, 85.42it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10480/12613 [02:02<00:24, 85.50it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10500/12613 [02:02<00:24, 85.59it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  83% 10520/12613 [02:02<00:24, 85.68it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10540/12613 [02:02<00:24, 85.76it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10560/12613 [02:03<00:23, 85.85it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10580/12613 [02:03<00:23, 85.94it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10600/12613 [02:03<00:23, 86.03it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10620/12613 [02:03<00:23, 86.12it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  84% 10640/12613 [02:03<00:22, 86.20it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10660/12613 [02:03<00:22, 86.29it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10680/12613 [02:03<00:22, 86.38it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10700/12613 [02:03<00:22, 86.46it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10720/12613 [02:03<00:21, 86.55it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10740/12613 [02:03<00:21, 86.62it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10760/12613 [02:04<00:21, 86.70it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  85% 10780/12613 [02:04<00:21, 86.78it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10800/12613 [02:04<00:20, 86.85it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10820/12613 [02:04<00:20, 86.93it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10840/12613 [02:04<00:20, 87.01it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10860/12613 [02:04<00:20, 87.09it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10880/12613 [02:04<00:19, 87.16it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  86% 10900/12613 [02:04<00:19, 87.25it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 10920/12613 [02:05<00:19, 87.33it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 10940/12613 [02:05<00:19, 87.41it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 10960/12613 [02:05<00:18, 87.50it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 10980/12613 [02:05<00:18, 87.58it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 11000/12613 [02:05<00:18, 87.66it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  87% 11020/12613 [02:05<00:18, 87.75it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11040/12613 [02:05<00:17, 87.83it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11060/12613 [02:05<00:17, 87.90it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11080/12613 [02:05<00:17, 87.99it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11100/12613 [02:06<00:17, 88.07it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11120/12613 [02:06<00:16, 88.15it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11140/12613 [02:06<00:16, 88.23it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  88% 11160/12613 [02:06<00:16, 88.31it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11180/12613 [02:06<00:16, 88.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11200/12613 [02:06<00:15, 88.47it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11220/12613 [02:06<00:15, 88.55it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11240/12613 [02:06<00:15, 88.63it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11260/12613 [02:06<00:15, 88.71it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  89% 11280/12613 [02:07<00:15, 88.79it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11300/12613 [02:07<00:14, 88.87it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11320/12613 [02:07<00:14, 88.95it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11340/12613 [02:07<00:14, 89.03it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11360/12613 [02:07<00:14, 89.10it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11380/12613 [02:07<00:13, 89.18it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  90% 11400/12613 [02:07<00:13, 89.26it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11420/12613 [02:07<00:13, 89.33it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11440/12613 [02:07<00:13, 89.41it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11460/12613 [02:08<00:12, 89.49it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11480/12613 [02:08<00:12, 89.57it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11500/12613 [02:08<00:12, 89.65it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11520/12613 [02:08<00:12, 89.73it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  91% 11540/12613 [02:08<00:11, 89.81it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11560/12613 [02:08<00:11, 89.89it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11580/12613 [02:08<00:11, 89.97it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11600/12613 [02:08<00:11, 90.05it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11620/12613 [02:08<00:11, 90.13it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11640/12613 [02:09<00:10, 90.20it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  92% 11660/12613 [02:09<00:10, 90.28it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11680/12613 [02:09<00:10, 90.36it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11700/12613 [02:09<00:10, 90.45it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11720/12613 [02:09<00:09, 90.53it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11740/12613 [02:09<00:09, 90.61it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11760/12613 [02:09<00:09, 90.69it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  93% 11780/12613 [02:09<00:09, 90.76it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11800/12613 [02:09<00:08, 90.84it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11820/12613 [02:10<00:08, 90.92it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11840/12613 [02:10<00:08, 91.00it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11860/12613 [02:10<00:08, 91.08it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11880/12613 [02:10<00:08, 91.16it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  94% 11900/12613 [02:10<00:07, 91.24it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 11920/12613 [02:10<00:07, 91.32it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 11940/12613 [02:10<00:07, 91.40it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 11960/12613 [02:10<00:07, 91.48it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 11980/12613 [02:10<00:06, 91.56it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 12000/12613 [02:10<00:06, 91.64it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 12020/12613 [02:11<00:06, 91.72it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  95% 12040/12613 [02:11<00:06, 91.80it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12060/12613 [02:11<00:06, 91.88it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12080/12613 [02:11<00:05, 91.96it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12100/12613 [02:11<00:05, 92.03it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12120/12613 [02:11<00:05, 92.11it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12140/12613 [02:11<00:05, 92.19it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  96% 12160/12613 [02:11<00:04, 92.27it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12180/12613 [02:11<00:04, 92.34it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12200/12613 [02:12<00:04, 92.42it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12220/12613 [02:12<00:04, 92.50it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12240/12613 [02:12<00:04, 92.57it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12260/12613 [02:12<00:03, 92.65it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  97% 12280/12613 [02:12<00:03, 92.72it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12300/12613 [02:12<00:03, 92.80it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12320/12613 [02:12<00:03, 92.87it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12340/12613 [02:12<00:02, 92.95it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12360/12613 [02:12<00:02, 93.02it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12380/12613 [02:12<00:02, 93.10it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12400/12613 [02:13<00:02, 93.18it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  98% 12420/12613 [02:13<00:02, 93.26it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12440/12613 [02:13<00:01, 93.33it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12460/12613 [02:13<00:01, 93.41it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12480/12613 [02:13<00:01, 93.49it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12500/12613 [02:13<00:01, 93.57it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12520/12613 [02:13<00:00, 93.64it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7:  99% 12540/12613 [02:13<00:00, 93.72it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7: 100% 12560/12613 [02:13<00:00, 93.79it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7: 100% 12580/12613 [02:14<00:00, 93.86it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 7: 100% 12600/12613 [02:14<00:00, 93.94it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.56it/s]\u001b[A\n",
            "Epoch 7: 100% 12613/12613 [02:14<00:00, 93.98it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 3.695\n",
            "Epoch 8:  80% 10080/12613 [01:56<00:29, 86.51it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 10100/12613 [02:01<00:30, 83.33it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  80% 10120/12613 [02:01<00:29, 83.42it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  80% 10140/12613 [02:01<00:29, 83.51it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10160/12613 [02:01<00:29, 83.60it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10180/12613 [02:01<00:29, 83.69it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10200/12613 [02:01<00:28, 83.78it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10220/12613 [02:01<00:28, 83.87it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10240/12613 [02:01<00:28, 83.95it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  81% 10260/12613 [02:02<00:27, 84.04it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10280/12613 [02:02<00:27, 84.13it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10300/12613 [02:02<00:27, 84.22it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10320/12613 [02:02<00:27, 84.30it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10340/12613 [02:02<00:26, 84.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10360/12613 [02:02<00:26, 84.48it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10380/12613 [02:02<00:26, 84.56it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  82% 10400/12613 [02:02<00:26, 84.65it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10420/12613 [02:02<00:25, 84.73it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10440/12613 [02:03<00:25, 84.82it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10460/12613 [02:03<00:25, 84.90it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10480/12613 [02:03<00:25, 84.99it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10500/12613 [02:03<00:24, 85.07it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  83% 10520/12613 [02:03<00:24, 85.16it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10540/12613 [02:03<00:24, 85.24it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10560/12613 [02:03<00:24, 85.32it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10580/12613 [02:03<00:23, 85.41it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10600/12613 [02:03<00:23, 85.50it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10620/12613 [02:04<00:23, 85.59it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  84% 10640/12613 [02:04<00:23, 85.67it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10660/12613 [02:04<00:22, 85.76it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10680/12613 [02:04<00:22, 85.84it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10700/12613 [02:04<00:22, 85.93it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10720/12613 [02:04<00:22, 86.02it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10740/12613 [02:04<00:21, 86.10it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10760/12613 [02:04<00:21, 86.11it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  85% 10780/12613 [02:05<00:21, 86.19it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10800/12613 [02:05<00:21, 86.28it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10820/12613 [02:05<00:20, 86.36it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10840/12613 [02:05<00:20, 86.45it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10860/12613 [02:05<00:20, 86.53it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10880/12613 [02:05<00:20, 86.62it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  86% 10900/12613 [02:05<00:19, 86.71it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 10920/12613 [02:05<00:19, 86.79it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 10940/12613 [02:05<00:19, 86.88it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 10960/12613 [02:06<00:19, 86.96it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 10980/12613 [02:06<00:18, 87.05it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 11000/12613 [02:06<00:18, 87.14it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  87% 11020/12613 [02:06<00:18, 87.22it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11040/12613 [02:06<00:18, 87.31it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11060/12613 [02:06<00:17, 87.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11080/12613 [02:06<00:17, 87.48it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11100/12613 [02:06<00:17, 87.56it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11120/12613 [02:06<00:17, 87.64it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11140/12613 [02:06<00:16, 87.72it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  88% 11160/12613 [02:07<00:16, 87.80it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11180/12613 [02:07<00:16, 87.89it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11200/12613 [02:07<00:16, 87.97it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11220/12613 [02:07<00:15, 88.05it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11240/12613 [02:07<00:15, 88.14it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11260/12613 [02:07<00:15, 88.22it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  89% 11280/12613 [02:07<00:15, 88.30it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11300/12613 [02:07<00:14, 88.38it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11320/12613 [02:07<00:14, 88.47it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11340/12613 [02:08<00:14, 88.55it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11360/12613 [02:08<00:14, 88.62it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11380/12613 [02:08<00:13, 88.70it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  90% 11400/12613 [02:08<00:13, 88.78it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11420/12613 [02:08<00:13, 88.86it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11440/12613 [02:08<00:13, 88.94it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11460/12613 [02:08<00:12, 89.03it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11480/12613 [02:08<00:12, 89.11it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11500/12613 [02:08<00:12, 89.18it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11520/12613 [02:09<00:12, 89.26it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  91% 11540/12613 [02:09<00:12, 89.35it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11560/12613 [02:09<00:11, 89.42it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11580/12613 [02:09<00:11, 89.50it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11600/12613 [02:09<00:11, 89.58it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11620/12613 [02:09<00:11, 89.66it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11640/12613 [02:09<00:10, 89.74it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  92% 11660/12613 [02:09<00:10, 89.82it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11680/12613 [02:09<00:10, 89.90it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11700/12613 [02:10<00:10, 89.98it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11720/12613 [02:10<00:09, 90.06it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11740/12613 [02:10<00:09, 90.14it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11760/12613 [02:10<00:09, 90.22it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  93% 11780/12613 [02:10<00:09, 90.31it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11800/12613 [02:10<00:08, 90.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11820/12613 [02:10<00:08, 90.47it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11840/12613 [02:10<00:08, 90.55it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11860/12613 [02:10<00:08, 90.63it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11880/12613 [02:10<00:08, 90.70it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  94% 11900/12613 [02:11<00:07, 90.78it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 11920/12613 [02:11<00:07, 90.85it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 11940/12613 [02:11<00:07, 90.93it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 11960/12613 [02:11<00:07, 91.01it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 11980/12613 [02:11<00:06, 91.08it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 12000/12613 [02:11<00:06, 91.16it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 12020/12613 [02:11<00:06, 91.23it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  95% 12040/12613 [02:11<00:06, 91.31it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12060/12613 [02:11<00:06, 91.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12080/12613 [02:12<00:05, 91.46it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12100/12613 [02:12<00:05, 91.54it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12120/12613 [02:12<00:05, 91.62it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12140/12613 [02:12<00:05, 91.70it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  96% 12160/12613 [02:12<00:04, 91.78it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12180/12613 [02:12<00:04, 91.86it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12200/12613 [02:12<00:04, 91.94it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12220/12613 [02:12<00:04, 92.02it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12240/12613 [02:12<00:04, 92.09it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12260/12613 [02:13<00:03, 92.17it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  97% 12280/12613 [02:13<00:03, 92.24it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12300/12613 [02:13<00:03, 92.31it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12320/12613 [02:13<00:03, 92.39it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12340/12613 [02:13<00:02, 92.47it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12360/12613 [02:13<00:02, 92.54it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12380/12613 [02:13<00:02, 92.61it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12400/12613 [02:13<00:02, 92.69it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  98% 12420/12613 [02:13<00:02, 92.76it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12440/12613 [02:14<00:01, 92.82it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12460/12613 [02:14<00:01, 92.90it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12480/12613 [02:14<00:01, 92.97it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12500/12613 [02:14<00:01, 93.04it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12520/12613 [02:14<00:00, 93.11it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8:  99% 12540/12613 [02:14<00:00, 93.19it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8: 100% 12560/12613 [02:14<00:00, 93.27it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8: 100% 12580/12613 [02:14<00:00, 93.34it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Epoch 8: 100% 12600/12613 [02:14<00:00, 93.42it/s, loss=3.69, v_num=56, val_loss=3.700, avg_val_loss=3.700, train_loss=3.700]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 182.68it/s]\u001b[A\n",
            "Epoch 8: 100% 12613/12613 [02:14<00:00, 93.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.700]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 3.694\n",
            "Epoch 9:  80% 10080/12613 [01:55<00:29, 87.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 10100/12613 [02:00<00:29, 84.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  80% 10120/12613 [02:00<00:29, 84.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  80% 10140/12613 [02:00<00:29, 84.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10160/12613 [02:00<00:29, 84.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10180/12613 [02:00<00:28, 84.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10200/12613 [02:00<00:28, 84.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10220/12613 [02:00<00:28, 84.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10240/12613 [02:00<00:27, 84.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  81% 10260/12613 [02:00<00:27, 84.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10280/12613 [02:01<00:27, 84.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10300/12613 [02:01<00:27, 85.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10320/12613 [02:01<00:26, 85.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10340/12613 [02:01<00:26, 85.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10360/12613 [02:01<00:26, 85.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10380/12613 [02:01<00:26, 85.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  82% 10400/12613 [02:01<00:25, 85.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10420/12613 [02:01<00:25, 85.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10440/12613 [02:01<00:25, 85.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10460/12613 [02:01<00:25, 85.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10480/12613 [02:02<00:24, 85.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10500/12613 [02:02<00:24, 85.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  83% 10520/12613 [02:02<00:24, 86.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10540/12613 [02:02<00:24, 86.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10560/12613 [02:02<00:23, 86.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10580/12613 [02:02<00:23, 86.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10600/12613 [02:02<00:23, 86.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10620/12613 [02:02<00:23, 86.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  84% 10640/12613 [02:02<00:22, 86.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10660/12613 [02:03<00:22, 86.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10680/12613 [02:03<00:22, 86.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10700/12613 [02:03<00:22, 86.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10720/12613 [02:03<00:21, 86.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10740/12613 [02:03<00:21, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10760/12613 [02:03<00:21, 87.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  85% 10780/12613 [02:03<00:21, 87.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10800/12613 [02:03<00:20, 87.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10820/12613 [02:03<00:20, 87.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10840/12613 [02:04<00:20, 87.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10860/12613 [02:04<00:20, 87.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10880/12613 [02:04<00:19, 87.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  86% 10900/12613 [02:04<00:19, 87.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 10920/12613 [02:04<00:19, 87.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 10940/12613 [02:04<00:19, 87.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 10960/12613 [02:04<00:18, 87.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 10980/12613 [02:04<00:18, 87.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 11000/12613 [02:04<00:18, 88.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  87% 11020/12613 [02:05<00:18, 88.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11040/12613 [02:05<00:17, 88.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11060/12613 [02:05<00:17, 88.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11080/12613 [02:05<00:17, 88.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11100/12613 [02:05<00:17, 88.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11120/12613 [02:05<00:16, 88.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11140/12613 [02:05<00:16, 88.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  88% 11160/12613 [02:05<00:16, 88.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11180/12613 [02:05<00:16, 88.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11200/12613 [02:06<00:15, 88.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11220/12613 [02:06<00:15, 88.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11240/12613 [02:06<00:15, 89.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11260/12613 [02:06<00:15, 89.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  89% 11280/12613 [02:06<00:14, 89.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11300/12613 [02:06<00:14, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11320/12613 [02:06<00:14, 89.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11340/12613 [02:06<00:14, 89.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11360/12613 [02:06<00:14, 89.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11380/12613 [02:07<00:13, 89.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  90% 11400/12613 [02:07<00:13, 89.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11420/12613 [02:07<00:13, 89.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11440/12613 [02:07<00:13, 89.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11460/12613 [02:07<00:12, 89.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11480/12613 [02:07<00:12, 89.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11500/12613 [02:07<00:12, 90.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11520/12613 [02:07<00:12, 90.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  91% 11540/12613 [02:07<00:11, 90.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11560/12613 [02:08<00:11, 90.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11580/12613 [02:08<00:11, 90.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11600/12613 [02:08<00:11, 90.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11620/12613 [02:08<00:10, 90.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11640/12613 [02:08<00:10, 90.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  92% 11660/12613 [02:08<00:10, 90.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11680/12613 [02:08<00:10, 90.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11700/12613 [02:08<00:10, 90.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11720/12613 [02:08<00:09, 90.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11740/12613 [02:09<00:09, 90.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11760/12613 [02:09<00:09, 91.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  93% 11780/12613 [02:09<00:09, 91.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11800/12613 [02:09<00:08, 91.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11820/12613 [02:09<00:08, 91.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11840/12613 [02:09<00:08, 91.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11860/12613 [02:09<00:08, 91.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11880/12613 [02:09<00:08, 91.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  94% 11900/12613 [02:09<00:07, 91.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 11920/12613 [02:10<00:07, 91.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 11940/12613 [02:10<00:07, 91.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 11960/12613 [02:10<00:07, 91.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 11980/12613 [02:10<00:06, 91.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 12000/12613 [02:10<00:06, 91.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 12020/12613 [02:10<00:06, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  95% 12040/12613 [02:10<00:06, 92.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12060/12613 [02:10<00:05, 92.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12080/12613 [02:10<00:05, 92.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12100/12613 [02:11<00:05, 92.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12120/12613 [02:11<00:05, 92.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12140/12613 [02:11<00:05, 92.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  96% 12160/12613 [02:11<00:04, 92.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12180/12613 [02:11<00:04, 92.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12200/12613 [02:11<00:04, 92.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12220/12613 [02:11<00:04, 92.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12240/12613 [02:11<00:04, 92.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12260/12613 [02:11<00:03, 92.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  97% 12280/12613 [02:11<00:03, 93.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12300/12613 [02:12<00:03, 93.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12320/12613 [02:12<00:03, 93.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12340/12613 [02:12<00:02, 93.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12360/12613 [02:12<00:02, 93.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12380/12613 [02:12<00:02, 93.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12400/12613 [02:12<00:02, 93.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  98% 12420/12613 [02:12<00:02, 93.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12440/12613 [02:12<00:01, 93.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12460/12613 [02:12<00:01, 93.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12480/12613 [02:12<00:01, 93.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12500/12613 [02:13<00:01, 93.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12520/12613 [02:13<00:00, 94.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9:  99% 12540/12613 [02:13<00:00, 94.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9: 100% 12560/12613 [02:13<00:00, 94.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9: 100% 12580/12613 [02:13<00:00, 94.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 9: 100% 12600/12613 [02:13<00:00, 94.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 184.19it/s]\u001b[A\n",
            "Epoch 9: 100% 12613/12613 [02:13<00:00, 94.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.693\n",
            "Epoch 10:  80% 10080/12613 [01:56<00:29, 86.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 10100/12613 [02:00<00:30, 83.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  80% 10120/12613 [02:00<00:29, 83.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  80% 10140/12613 [02:00<00:29, 83.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10160/12613 [02:00<00:29, 83.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10180/12613 [02:01<00:28, 84.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10200/12613 [02:01<00:28, 84.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10220/12613 [02:01<00:28, 84.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10240/12613 [02:01<00:28, 84.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  81% 10260/12613 [02:01<00:27, 84.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10280/12613 [02:01<00:27, 84.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10300/12613 [02:01<00:27, 84.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10320/12613 [02:01<00:27, 84.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10340/12613 [02:01<00:26, 84.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10360/12613 [02:02<00:26, 84.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10380/12613 [02:02<00:26, 84.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  82% 10400/12613 [02:02<00:26, 85.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10420/12613 [02:02<00:25, 85.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10440/12613 [02:02<00:25, 85.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10460/12613 [02:02<00:25, 85.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10480/12613 [02:02<00:24, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10500/12613 [02:02<00:24, 85.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  83% 10520/12613 [02:03<00:24, 85.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10540/12613 [02:03<00:24, 85.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10560/12613 [02:03<00:23, 85.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10580/12613 [02:03<00:23, 85.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10600/12613 [02:03<00:23, 85.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10620/12613 [02:03<00:23, 85.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  84% 10640/12613 [02:03<00:22, 86.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10660/12613 [02:03<00:22, 86.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10680/12613 [02:03<00:22, 86.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10700/12613 [02:03<00:22, 86.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10720/12613 [02:04<00:21, 86.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10740/12613 [02:04<00:21, 86.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10760/12613 [02:04<00:21, 86.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  85% 10780/12613 [02:04<00:21, 86.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10800/12613 [02:04<00:20, 86.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10820/12613 [02:04<00:20, 86.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10840/12613 [02:04<00:20, 86.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10860/12613 [02:04<00:20, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10880/12613 [02:05<00:19, 87.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  86% 10900/12613 [02:05<00:19, 87.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 10920/12613 [02:05<00:19, 87.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 10940/12613 [02:05<00:19, 87.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 10960/12613 [02:05<00:18, 87.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 10980/12613 [02:05<00:18, 87.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 11000/12613 [02:05<00:18, 87.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  87% 11020/12613 [02:05<00:18, 87.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11040/12613 [02:05<00:17, 87.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11060/12613 [02:06<00:17, 87.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11080/12613 [02:06<00:17, 87.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11100/12613 [02:06<00:17, 87.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11120/12613 [02:06<00:16, 87.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11140/12613 [02:06<00:16, 88.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  88% 11160/12613 [02:06<00:16, 88.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11180/12613 [02:06<00:16, 88.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11200/12613 [02:06<00:15, 88.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11220/12613 [02:06<00:15, 88.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11240/12613 [02:07<00:15, 88.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11260/12613 [02:07<00:15, 88.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  89% 11280/12613 [02:07<00:15, 88.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11300/12613 [02:07<00:14, 88.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11320/12613 [02:07<00:14, 88.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11340/12613 [02:07<00:14, 88.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11360/12613 [02:07<00:14, 89.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11380/12613 [02:07<00:13, 89.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  90% 11400/12613 [02:07<00:13, 89.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11420/12613 [02:07<00:13, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11440/12613 [02:08<00:13, 89.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11460/12613 [02:08<00:12, 89.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11480/12613 [02:08<00:12, 89.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11500/12613 [02:08<00:12, 89.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11520/12613 [02:08<00:12, 89.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  91% 11540/12613 [02:08<00:11, 89.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11560/12613 [02:08<00:11, 89.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11580/12613 [02:08<00:11, 89.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11600/12613 [02:08<00:11, 89.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11620/12613 [02:09<00:11, 90.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11640/12613 [02:09<00:10, 90.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  92% 11660/12613 [02:09<00:10, 90.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11680/12613 [02:09<00:10, 90.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11700/12613 [02:09<00:10, 90.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11720/12613 [02:09<00:09, 90.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11740/12613 [02:09<00:09, 90.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11760/12613 [02:09<00:09, 90.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  93% 11780/12613 [02:09<00:09, 90.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11800/12613 [02:09<00:08, 90.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11820/12613 [02:10<00:08, 90.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11840/12613 [02:10<00:08, 90.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11860/12613 [02:10<00:08, 91.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11880/12613 [02:10<00:08, 91.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  94% 11900/12613 [02:10<00:07, 91.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 11920/12613 [02:10<00:07, 91.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 11940/12613 [02:10<00:07, 91.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 11960/12613 [02:10<00:07, 91.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 11980/12613 [02:10<00:06, 91.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 12000/12613 [02:11<00:06, 91.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 12020/12613 [02:11<00:06, 91.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  95% 12040/12613 [02:11<00:06, 91.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12060/12613 [02:11<00:06, 91.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12080/12613 [02:11<00:05, 91.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12100/12613 [02:11<00:05, 91.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12120/12613 [02:11<00:05, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12140/12613 [02:11<00:05, 92.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  96% 12160/12613 [02:11<00:04, 92.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12180/12613 [02:11<00:04, 92.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12200/12613 [02:12<00:04, 92.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12220/12613 [02:12<00:04, 92.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12240/12613 [02:12<00:04, 92.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12260/12613 [02:12<00:03, 92.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  97% 12280/12613 [02:12<00:03, 92.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12300/12613 [02:12<00:03, 92.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12320/12613 [02:12<00:03, 92.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12340/12613 [02:12<00:02, 92.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12360/12613 [02:12<00:02, 92.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12380/12613 [02:13<00:02, 93.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12400/12613 [02:13<00:02, 93.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  98% 12420/12613 [02:13<00:02, 93.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12440/12613 [02:13<00:01, 93.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12460/12613 [02:13<00:01, 93.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12480/12613 [02:13<00:01, 93.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12500/12613 [02:13<00:01, 93.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12520/12613 [02:13<00:00, 93.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10:  99% 12540/12613 [02:13<00:00, 93.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10: 100% 12560/12613 [02:14<00:00, 93.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10: 100% 12580/12613 [02:14<00:00, 93.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 10: 100% 12600/12613 [02:14<00:00, 93.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 182.53it/s]\u001b[A\n",
            "Epoch 10: 100% 12613/12613 [02:14<00:00, 93.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  80% 10080/12613 [01:55<00:29, 87.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 10100/12613 [02:00<00:29, 83.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  80% 10120/12613 [02:00<00:29, 84.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  80% 10140/12613 [02:00<00:29, 84.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10160/12613 [02:00<00:29, 84.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10180/12613 [02:00<00:28, 84.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10200/12613 [02:00<00:28, 84.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10220/12613 [02:00<00:28, 84.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10240/12613 [02:01<00:28, 84.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  81% 10260/12613 [02:01<00:27, 84.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10280/12613 [02:01<00:27, 84.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10300/12613 [02:01<00:27, 84.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10320/12613 [02:01<00:26, 84.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10340/12613 [02:01<00:26, 85.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10360/12613 [02:01<00:26, 85.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10380/12613 [02:01<00:26, 85.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  82% 10400/12613 [02:01<00:25, 85.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10420/12613 [02:01<00:25, 85.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10440/12613 [02:02<00:25, 85.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10460/12613 [02:02<00:25, 85.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10480/12613 [02:02<00:24, 85.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10500/12613 [02:02<00:24, 85.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  83% 10520/12613 [02:02<00:24, 85.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10540/12613 [02:02<00:24, 85.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10560/12613 [02:02<00:23, 86.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10580/12613 [02:02<00:23, 86.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10600/12613 [02:02<00:23, 86.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10620/12613 [02:03<00:23, 86.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  84% 10640/12613 [02:03<00:22, 86.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10660/12613 [02:03<00:22, 86.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10680/12613 [02:03<00:22, 86.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10700/12613 [02:03<00:22, 86.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10720/12613 [02:03<00:21, 86.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10740/12613 [02:03<00:21, 86.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10760/12613 [02:03<00:21, 86.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  85% 10780/12613 [02:03<00:21, 87.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10800/12613 [02:03<00:20, 87.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10820/12613 [02:04<00:20, 87.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10840/12613 [02:04<00:20, 87.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10860/12613 [02:04<00:20, 87.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10880/12613 [02:04<00:19, 87.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  86% 10900/12613 [02:04<00:19, 87.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 10920/12613 [02:04<00:19, 87.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 10940/12613 [02:04<00:19, 87.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 10960/12613 [02:04<00:18, 87.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 10980/12613 [02:04<00:18, 87.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 11000/12613 [02:05<00:18, 87.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  87% 11020/12613 [02:05<00:18, 88.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11040/12613 [02:05<00:17, 88.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11060/12613 [02:05<00:17, 88.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11080/12613 [02:05<00:17, 88.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11100/12613 [02:05<00:17, 88.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11120/12613 [02:05<00:16, 88.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11140/12613 [02:05<00:16, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  88% 11160/12613 [02:05<00:16, 88.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11180/12613 [02:05<00:16, 88.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11200/12613 [02:06<00:15, 88.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11220/12613 [02:06<00:15, 88.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11240/12613 [02:06<00:15, 89.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11260/12613 [02:06<00:15, 89.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  89% 11280/12613 [02:06<00:14, 89.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11300/12613 [02:06<00:14, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11320/12613 [02:06<00:14, 89.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11340/12613 [02:06<00:14, 89.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11360/12613 [02:06<00:14, 89.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11380/12613 [02:07<00:13, 89.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  90% 11400/12613 [02:07<00:13, 89.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11420/12613 [02:07<00:13, 89.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11440/12613 [02:07<00:13, 89.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11460/12613 [02:07<00:12, 89.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11480/12613 [02:07<00:12, 90.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11500/12613 [02:07<00:12, 90.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11520/12613 [02:07<00:12, 90.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  91% 11540/12613 [02:07<00:11, 90.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11560/12613 [02:07<00:11, 90.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11580/12613 [02:08<00:11, 90.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11600/12613 [02:08<00:11, 90.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11620/12613 [02:08<00:10, 90.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11640/12613 [02:08<00:10, 90.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  92% 11660/12613 [02:08<00:10, 90.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11680/12613 [02:08<00:10, 90.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11700/12613 [02:08<00:10, 90.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11720/12613 [02:08<00:09, 90.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11740/12613 [02:08<00:09, 91.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11760/12613 [02:09<00:09, 91.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  93% 11780/12613 [02:09<00:09, 91.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11800/12613 [02:09<00:08, 91.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11820/12613 [02:09<00:08, 91.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11840/12613 [02:09<00:08, 91.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11860/12613 [02:09<00:08, 91.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11880/12613 [02:09<00:08, 91.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  94% 11900/12613 [02:09<00:07, 91.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 11920/12613 [02:09<00:07, 91.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 11940/12613 [02:10<00:07, 91.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 11960/12613 [02:10<00:07, 91.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 11980/12613 [02:10<00:06, 92.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 12000/12613 [02:10<00:06, 92.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 12020/12613 [02:10<00:06, 92.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  95% 12040/12613 [02:10<00:06, 92.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12060/12613 [02:10<00:05, 92.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12080/12613 [02:10<00:05, 92.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12100/12613 [02:10<00:05, 92.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12120/12613 [02:10<00:05, 92.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12140/12613 [02:11<00:05, 92.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  96% 12160/12613 [02:11<00:04, 92.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12180/12613 [02:11<00:04, 92.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12200/12613 [02:11<00:04, 92.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12220/12613 [02:11<00:04, 92.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12240/12613 [02:11<00:04, 92.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12260/12613 [02:11<00:03, 93.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  97% 12280/12613 [02:11<00:03, 93.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12300/12613 [02:12<00:03, 93.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12320/12613 [02:12<00:03, 93.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12340/12613 [02:12<00:02, 93.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12360/12613 [02:12<00:02, 93.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12380/12613 [02:12<00:02, 93.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12400/12613 [02:12<00:02, 93.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  98% 12420/12613 [02:12<00:02, 93.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12440/12613 [02:12<00:01, 93.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12460/12613 [02:12<00:01, 93.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12480/12613 [02:13<00:01, 93.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12500/12613 [02:13<00:01, 93.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12520/12613 [02:13<00:00, 93.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11:  99% 12540/12613 [02:13<00:00, 94.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11: 100% 12560/12613 [02:13<00:00, 94.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11: 100% 12580/12613 [02:13<00:00, 94.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 11: 100% 12600/12613 [02:13<00:00, 94.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 185.84it/s]\u001b[A\n",
            "Epoch 11: 100% 12613/12613 [02:13<00:00, 94.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 3.691\n",
            "Epoch 12:  80% 10080/12613 [01:56<00:29, 86.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 10100/12613 [02:00<00:30, 83.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  80% 10120/12613 [02:00<00:29, 83.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  80% 10140/12613 [02:01<00:29, 83.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10160/12613 [02:01<00:29, 83.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10180/12613 [02:01<00:28, 83.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10200/12613 [02:01<00:28, 84.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10220/12613 [02:01<00:28, 84.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10240/12613 [02:01<00:28, 84.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  81% 10260/12613 [02:01<00:27, 84.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10280/12613 [02:01<00:27, 84.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10300/12613 [02:01<00:27, 84.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10320/12613 [02:02<00:27, 84.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10340/12613 [02:02<00:26, 84.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10360/12613 [02:02<00:26, 84.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10380/12613 [02:02<00:26, 84.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  82% 10400/12613 [02:02<00:26, 84.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10420/12613 [02:02<00:25, 84.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10440/12613 [02:02<00:25, 85.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10460/12613 [02:02<00:25, 85.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10480/12613 [02:03<00:25, 85.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10500/12613 [02:03<00:24, 85.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  83% 10520/12613 [02:03<00:24, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10540/12613 [02:03<00:24, 85.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10560/12613 [02:03<00:24, 85.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10580/12613 [02:03<00:23, 85.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10600/12613 [02:03<00:23, 85.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10620/12613 [02:03<00:23, 85.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  84% 10640/12613 [02:03<00:22, 85.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10660/12613 [02:04<00:22, 85.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10680/12613 [02:04<00:22, 86.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10700/12613 [02:04<00:22, 86.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10720/12613 [02:04<00:21, 86.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10740/12613 [02:04<00:21, 86.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10760/12613 [02:04<00:21, 86.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  85% 10780/12613 [02:04<00:21, 86.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10800/12613 [02:04<00:20, 86.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10820/12613 [02:04<00:20, 86.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10840/12613 [02:05<00:20, 86.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10860/12613 [02:05<00:20, 86.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10880/12613 [02:05<00:19, 86.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  86% 10900/12613 [02:05<00:19, 86.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 10920/12613 [02:05<00:19, 86.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 10940/12613 [02:05<00:19, 87.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 10960/12613 [02:05<00:18, 87.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 10980/12613 [02:05<00:18, 87.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 11000/12613 [02:05<00:18, 87.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  87% 11020/12613 [02:06<00:18, 87.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11040/12613 [02:06<00:17, 87.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11060/12613 [02:06<00:17, 87.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11080/12613 [02:06<00:17, 87.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11100/12613 [02:06<00:17, 87.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11120/12613 [02:06<00:17, 87.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11140/12613 [02:06<00:16, 87.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  88% 11160/12613 [02:06<00:16, 87.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11180/12613 [02:06<00:16, 88.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11200/12613 [02:07<00:16, 88.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11220/12613 [02:07<00:15, 88.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11240/12613 [02:07<00:15, 88.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11260/12613 [02:07<00:15, 88.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  89% 11280/12613 [02:07<00:15, 88.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11300/12613 [02:07<00:14, 88.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11320/12613 [02:07<00:14, 88.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11340/12613 [02:07<00:14, 88.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11360/12613 [02:07<00:14, 88.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11380/12613 [02:08<00:13, 88.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  90% 11400/12613 [02:08<00:13, 88.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11420/12613 [02:08<00:13, 89.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11440/12613 [02:08<00:13, 89.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11460/12613 [02:08<00:12, 89.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11480/12613 [02:08<00:12, 89.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11500/12613 [02:08<00:12, 89.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11520/12613 [02:08<00:12, 89.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  91% 11540/12613 [02:08<00:11, 89.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11560/12613 [02:09<00:11, 89.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11580/12613 [02:09<00:11, 89.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11600/12613 [02:09<00:11, 89.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11620/12613 [02:09<00:11, 89.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11640/12613 [02:09<00:10, 89.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  92% 11660/12613 [02:09<00:10, 89.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11680/12613 [02:09<00:10, 90.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11700/12613 [02:09<00:10, 90.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11720/12613 [02:09<00:09, 90.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11740/12613 [02:10<00:09, 90.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11760/12613 [02:10<00:09, 90.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  93% 11780/12613 [02:10<00:09, 90.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11800/12613 [02:10<00:08, 90.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11820/12613 [02:10<00:08, 90.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11840/12613 [02:10<00:08, 90.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11860/12613 [02:10<00:08, 90.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11880/12613 [02:10<00:08, 90.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  94% 11900/12613 [02:10<00:07, 90.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 11920/12613 [02:11<00:07, 90.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 11940/12613 [02:11<00:07, 91.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 11960/12613 [02:11<00:07, 91.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 11980/12613 [02:11<00:06, 91.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 12000/12613 [02:11<00:06, 91.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 12020/12613 [02:11<00:06, 91.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  95% 12040/12613 [02:11<00:06, 91.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12060/12613 [02:11<00:06, 91.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12080/12613 [02:11<00:05, 91.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12100/12613 [02:12<00:05, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12120/12613 [02:12<00:05, 91.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12140/12613 [02:12<00:05, 91.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  96% 12160/12613 [02:12<00:04, 91.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12180/12613 [02:12<00:04, 91.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12200/12613 [02:12<00:04, 92.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12220/12613 [02:12<00:04, 92.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12240/12613 [02:12<00:04, 92.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12260/12613 [02:12<00:03, 92.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  97% 12280/12613 [02:13<00:03, 92.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12300/12613 [02:13<00:03, 92.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12320/12613 [02:13<00:03, 92.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12340/12613 [02:13<00:02, 92.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12360/12613 [02:13<00:02, 92.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12380/12613 [02:13<00:02, 92.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12400/12613 [02:13<00:02, 92.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  98% 12420/12613 [02:13<00:02, 92.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12440/12613 [02:13<00:01, 92.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12460/12613 [02:13<00:01, 92.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12480/12613 [02:14<00:01, 93.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12500/12613 [02:14<00:01, 93.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12520/12613 [02:14<00:00, 93.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12:  99% 12540/12613 [02:14<00:00, 93.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12: 100% 12560/12613 [02:14<00:00, 93.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12: 100% 12580/12613 [02:14<00:00, 93.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 12: 100% 12600/12613 [02:14<00:00, 93.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 180.02it/s]\u001b[A\n",
            "Epoch 12: 100% 12613/12613 [02:14<00:00, 93.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  80% 10080/12613 [01:56<00:29, 86.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  80% 10100/12613 [02:00<00:30, 83.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  80% 10120/12613 [02:01<00:29, 83.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  80% 10140/12613 [02:01<00:29, 83.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10160/12613 [02:01<00:29, 83.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10180/12613 [02:01<00:29, 83.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10200/12613 [02:01<00:28, 83.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10220/12613 [02:01<00:28, 84.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10240/12613 [02:01<00:28, 84.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  81% 10260/12613 [02:01<00:27, 84.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10280/12613 [02:01<00:27, 84.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10300/12613 [02:02<00:27, 84.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10320/12613 [02:02<00:27, 84.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10340/12613 [02:02<00:26, 84.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10360/12613 [02:02<00:26, 84.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10380/12613 [02:02<00:26, 84.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  82% 10400/12613 [02:02<00:26, 84.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10420/12613 [02:02<00:25, 84.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10440/12613 [02:02<00:25, 84.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10460/12613 [02:02<00:25, 85.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10480/12613 [02:03<00:25, 85.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10500/12613 [02:03<00:24, 85.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  83% 10520/12613 [02:03<00:24, 85.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10540/12613 [02:03<00:24, 85.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10560/12613 [02:03<00:24, 85.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10580/12613 [02:03<00:23, 85.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10600/12613 [02:03<00:23, 85.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10620/12613 [02:03<00:23, 85.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  84% 10640/12613 [02:03<00:22, 85.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10660/12613 [02:04<00:22, 85.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10680/12613 [02:04<00:22, 86.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10700/12613 [02:04<00:22, 86.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10720/12613 [02:04<00:21, 86.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10740/12613 [02:04<00:21, 86.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10760/12613 [02:04<00:21, 86.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  85% 10780/12613 [02:04<00:21, 86.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10800/12613 [02:04<00:20, 86.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10820/12613 [02:04<00:20, 86.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10840/12613 [02:05<00:20, 86.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10860/12613 [02:05<00:20, 86.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10880/12613 [02:05<00:19, 86.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  86% 10900/12613 [02:05<00:19, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 10920/12613 [02:05<00:19, 87.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 10940/12613 [02:05<00:19, 87.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 10960/12613 [02:05<00:18, 87.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 10980/12613 [02:05<00:18, 87.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 11000/12613 [02:05<00:18, 87.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  87% 11020/12613 [02:06<00:18, 87.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11040/12613 [02:06<00:17, 87.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11060/12613 [02:06<00:17, 87.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11080/12613 [02:06<00:17, 87.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11100/12613 [02:06<00:17, 87.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11120/12613 [02:06<00:16, 87.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11140/12613 [02:06<00:16, 87.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  88% 11160/12613 [02:06<00:16, 88.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11180/12613 [02:06<00:16, 88.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11200/12613 [02:06<00:16, 88.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11220/12613 [02:07<00:15, 88.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11240/12613 [02:07<00:15, 88.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11260/12613 [02:07<00:15, 88.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  89% 11280/12613 [02:07<00:15, 88.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11300/12613 [02:07<00:14, 88.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11320/12613 [02:07<00:14, 88.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11340/12613 [02:07<00:14, 88.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11360/12613 [02:07<00:14, 88.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11380/12613 [02:08<00:13, 88.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  90% 11400/12613 [02:08<00:13, 88.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11420/12613 [02:08<00:13, 89.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11440/12613 [02:08<00:13, 89.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11460/12613 [02:08<00:12, 89.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11480/12613 [02:08<00:12, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11500/12613 [02:08<00:12, 89.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11520/12613 [02:08<00:12, 89.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  91% 11540/12613 [02:08<00:11, 89.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11560/12613 [02:09<00:11, 89.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11580/12613 [02:09<00:11, 89.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11600/12613 [02:09<00:11, 89.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11620/12613 [02:09<00:11, 89.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11640/12613 [02:09<00:10, 89.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  92% 11660/12613 [02:09<00:10, 89.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11680/12613 [02:09<00:10, 90.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11700/12613 [02:09<00:10, 90.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11720/12613 [02:09<00:09, 90.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11740/12613 [02:10<00:09, 90.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11760/12613 [02:10<00:09, 90.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  93% 11780/12613 [02:10<00:09, 90.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11800/12613 [02:10<00:08, 90.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11820/12613 [02:10<00:08, 90.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11840/12613 [02:10<00:08, 90.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11860/12613 [02:10<00:08, 90.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11880/12613 [02:10<00:08, 90.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  94% 11900/12613 [02:10<00:07, 90.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 11920/12613 [02:10<00:07, 91.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 11940/12613 [02:11<00:07, 91.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 11960/12613 [02:11<00:07, 91.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 11980/12613 [02:11<00:06, 91.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 12000/12613 [02:11<00:06, 91.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 12020/12613 [02:11<00:06, 91.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  95% 12040/12613 [02:11<00:06, 91.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12060/12613 [02:11<00:06, 91.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12080/12613 [02:11<00:05, 91.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12100/12613 [02:11<00:05, 91.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12120/12613 [02:12<00:05, 91.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12140/12613 [02:12<00:05, 91.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  96% 12160/12613 [02:12<00:04, 91.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12180/12613 [02:12<00:04, 91.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12200/12613 [02:12<00:04, 92.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12220/12613 [02:12<00:04, 92.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12240/12613 [02:12<00:04, 92.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12260/12613 [02:12<00:03, 92.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  97% 12280/12613 [02:12<00:03, 92.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12300/12613 [02:13<00:03, 92.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12320/12613 [02:13<00:03, 92.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12340/12613 [02:13<00:02, 92.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12360/12613 [02:13<00:02, 92.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12380/12613 [02:13<00:02, 92.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12400/12613 [02:13<00:02, 92.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  98% 12420/12613 [02:13<00:02, 92.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12440/12613 [02:13<00:01, 92.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12460/12613 [02:13<00:01, 93.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12480/12613 [02:14<00:01, 93.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12500/12613 [02:14<00:01, 93.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12520/12613 [02:14<00:00, 93.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13:  99% 12540/12613 [02:14<00:00, 93.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13: 100% 12560/12613 [02:14<00:00, 93.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13: 100% 12580/12613 [02:14<00:00, 93.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 13: 100% 12600/12613 [02:14<00:00, 93.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 181.26it/s]\u001b[A\n",
            "Epoch 13: 100% 12613/12613 [02:14<00:00, 93.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  80% 10080/12613 [01:56<00:29, 86.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  80% 10100/12613 [02:01<00:30, 83.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  80% 10120/12613 [02:01<00:29, 83.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  80% 10140/12613 [02:01<00:29, 83.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10160/12613 [02:01<00:29, 83.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10180/12613 [02:02<00:29, 83.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10200/12613 [02:02<00:28, 83.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10220/12613 [02:02<00:28, 83.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10240/12613 [02:02<00:28, 83.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  81% 10260/12613 [02:02<00:28, 83.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10280/12613 [02:02<00:27, 83.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10300/12613 [02:02<00:27, 83.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10320/12613 [02:02<00:27, 84.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10340/12613 [02:02<00:27, 84.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10360/12613 [02:03<00:26, 84.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10380/12613 [02:03<00:26, 84.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  82% 10400/12613 [02:03<00:26, 84.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10420/12613 [02:03<00:25, 84.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10440/12613 [02:03<00:25, 84.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10460/12613 [02:03<00:25, 84.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10480/12613 [02:03<00:25, 84.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10500/12613 [02:03<00:24, 84.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  83% 10520/12613 [02:03<00:24, 84.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10540/12613 [02:04<00:24, 84.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10560/12613 [02:04<00:24, 85.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10580/12613 [02:04<00:23, 85.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10600/12613 [02:04<00:23, 85.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10620/12613 [02:04<00:23, 85.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  84% 10640/12613 [02:04<00:23, 85.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10660/12613 [02:04<00:22, 85.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10680/12613 [02:04<00:22, 85.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10700/12613 [02:04<00:22, 85.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10720/12613 [02:05<00:22, 85.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10740/12613 [02:05<00:21, 85.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10760/12613 [02:05<00:21, 85.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  85% 10780/12613 [02:05<00:21, 85.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10800/12613 [02:05<00:21, 86.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10820/12613 [02:05<00:20, 86.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10840/12613 [02:05<00:20, 86.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10860/12613 [02:05<00:20, 86.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10880/12613 [02:05<00:20, 86.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  86% 10900/12613 [02:06<00:19, 86.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 10920/12613 [02:06<00:19, 86.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 10940/12613 [02:06<00:19, 86.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 10960/12613 [02:06<00:19, 86.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 10980/12613 [02:06<00:18, 86.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 11000/12613 [02:06<00:18, 86.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  87% 11020/12613 [02:06<00:18, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11040/12613 [02:06<00:18, 87.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11060/12613 [02:06<00:17, 87.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11080/12613 [02:07<00:17, 87.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11100/12613 [02:07<00:17, 87.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11120/12613 [02:07<00:17, 87.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11140/12613 [02:07<00:16, 87.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  88% 11160/12613 [02:07<00:16, 87.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11180/12613 [02:07<00:16, 87.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11200/12613 [02:07<00:16, 87.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11220/12613 [02:07<00:15, 87.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11240/12613 [02:07<00:15, 87.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11260/12613 [02:08<00:15, 87.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  89% 11280/12613 [02:08<00:15, 88.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11300/12613 [02:08<00:14, 88.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11320/12613 [02:08<00:14, 88.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11340/12613 [02:08<00:14, 88.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11360/12613 [02:08<00:14, 88.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11380/12613 [02:08<00:13, 88.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  90% 11400/12613 [02:08<00:13, 88.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11420/12613 [02:08<00:13, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11440/12613 [02:09<00:13, 88.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11460/12613 [02:09<00:12, 88.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11480/12613 [02:09<00:12, 88.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11500/12613 [02:09<00:12, 88.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11520/12613 [02:09<00:12, 88.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  91% 11540/12613 [02:09<00:12, 89.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11560/12613 [02:09<00:11, 89.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11580/12613 [02:09<00:11, 89.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11600/12613 [02:09<00:11, 89.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11620/12613 [02:10<00:11, 89.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11640/12613 [02:10<00:10, 89.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  92% 11660/12613 [02:10<00:10, 89.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11680/12613 [02:10<00:10, 89.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11700/12613 [02:10<00:10, 89.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11720/12613 [02:10<00:09, 89.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11740/12613 [02:10<00:09, 89.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11760/12613 [02:10<00:09, 89.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  93% 11780/12613 [02:10<00:09, 89.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11800/12613 [02:11<00:09, 90.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11820/12613 [02:11<00:08, 90.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11840/12613 [02:11<00:08, 90.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11860/12613 [02:11<00:08, 90.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11880/12613 [02:11<00:08, 90.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  94% 11900/12613 [02:11<00:07, 90.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 11920/12613 [02:11<00:07, 90.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 11940/12613 [02:11<00:07, 90.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 11960/12613 [02:11<00:07, 90.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 11980/12613 [02:12<00:06, 90.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 12000/12613 [02:12<00:06, 90.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 12020/12613 [02:12<00:06, 90.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  95% 12040/12613 [02:12<00:06, 90.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12060/12613 [02:12<00:06, 91.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12080/12613 [02:12<00:05, 91.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12100/12613 [02:12<00:05, 91.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12120/12613 [02:12<00:05, 91.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12140/12613 [02:12<00:05, 91.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  96% 12160/12613 [02:13<00:04, 91.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12180/12613 [02:13<00:04, 91.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12200/12613 [02:13<00:04, 91.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12220/12613 [02:13<00:04, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12240/12613 [02:13<00:04, 91.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12260/12613 [02:13<00:03, 91.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  97% 12280/12613 [02:13<00:03, 91.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12300/12613 [02:13<00:03, 91.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12320/12613 [02:13<00:03, 91.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12340/12613 [02:14<00:02, 92.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12360/12613 [02:14<00:02, 92.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12380/12613 [02:14<00:02, 92.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12400/12613 [02:14<00:02, 92.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  98% 12420/12613 [02:14<00:02, 92.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12440/12613 [02:14<00:01, 92.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12460/12613 [02:14<00:01, 92.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12480/12613 [02:14<00:01, 92.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12500/12613 [02:14<00:01, 92.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12520/12613 [02:15<00:01, 92.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14:  99% 12540/12613 [02:15<00:00, 92.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14: 100% 12560/12613 [02:15<00:00, 92.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14: 100% 12580/12613 [02:15<00:00, 92.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 14: 100% 12600/12613 [02:15<00:00, 92.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 180.12it/s]\u001b[A\n",
            "Epoch 14: 100% 12613/12613 [02:15<00:00, 93.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "                                                                 \u001b[AMetric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 3.690\n",
            "Epoch 15:  80% 10080/12613 [01:56<00:29, 86.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  80% 10100/12613 [02:01<00:30, 83.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  80% 10120/12613 [02:01<00:29, 83.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  80% 10140/12613 [02:01<00:29, 83.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10160/12613 [02:01<00:29, 83.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10180/12613 [02:01<00:29, 83.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10200/12613 [02:01<00:28, 83.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10220/12613 [02:01<00:28, 83.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10240/12613 [02:01<00:28, 84.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  81% 10260/12613 [02:01<00:27, 84.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10280/12613 [02:02<00:27, 84.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10300/12613 [02:02<00:27, 84.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10320/12613 [02:02<00:27, 84.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10340/12613 [02:02<00:26, 84.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10360/12613 [02:02<00:26, 84.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10380/12613 [02:02<00:26, 84.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  82% 10400/12613 [02:02<00:26, 84.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10420/12613 [02:02<00:25, 84.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10440/12613 [02:03<00:25, 84.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10460/12613 [02:03<00:25, 84.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10480/12613 [02:03<00:25, 85.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10500/12613 [02:03<00:24, 85.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  83% 10520/12613 [02:03<00:24, 85.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10540/12613 [02:03<00:24, 85.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10560/12613 [02:03<00:24, 85.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10580/12613 [02:03<00:23, 85.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10600/12613 [02:03<00:23, 85.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10620/12613 [02:04<00:23, 85.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  84% 10640/12613 [02:04<00:23, 85.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10660/12613 [02:04<00:22, 85.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10680/12613 [02:04<00:22, 85.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10700/12613 [02:04<00:22, 85.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10720/12613 [02:04<00:21, 86.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10740/12613 [02:04<00:21, 86.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10760/12613 [02:04<00:21, 86.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  85% 10780/12613 [02:04<00:21, 86.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10800/12613 [02:04<00:20, 86.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10820/12613 [02:05<00:20, 86.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10840/12613 [02:05<00:20, 86.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10860/12613 [02:05<00:20, 86.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10880/12613 [02:05<00:19, 86.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  86% 10900/12613 [02:05<00:19, 86.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 10920/12613 [02:05<00:19, 86.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 10940/12613 [02:05<00:19, 87.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 10960/12613 [02:05<00:18, 87.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 10980/12613 [02:05<00:18, 87.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 11000/12613 [02:06<00:18, 87.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  87% 11020/12613 [02:06<00:18, 87.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11040/12613 [02:06<00:17, 87.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11060/12613 [02:06<00:17, 87.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11080/12613 [02:06<00:17, 87.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11100/12613 [02:06<00:17, 87.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11120/12613 [02:06<00:17, 87.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11140/12613 [02:06<00:16, 87.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  88% 11160/12613 [02:06<00:16, 87.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11180/12613 [02:07<00:16, 87.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11200/12613 [02:07<00:16, 88.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11220/12613 [02:07<00:15, 88.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11240/12613 [02:07<00:15, 88.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11260/12613 [02:07<00:15, 88.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  89% 11280/12613 [02:07<00:15, 88.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11300/12613 [02:07<00:14, 88.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11320/12613 [02:07<00:14, 88.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11340/12613 [02:07<00:14, 88.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11360/12613 [02:08<00:14, 88.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11380/12613 [02:08<00:13, 88.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  90% 11400/12613 [02:08<00:13, 88.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11420/12613 [02:08<00:13, 88.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11440/12613 [02:08<00:13, 89.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11460/12613 [02:08<00:12, 89.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11480/12613 [02:08<00:12, 89.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11500/12613 [02:08<00:12, 89.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11520/12613 [02:08<00:12, 89.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  91% 11540/12613 [02:09<00:11, 89.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11560/12613 [02:09<00:11, 89.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11580/12613 [02:09<00:11, 89.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11600/12613 [02:09<00:11, 89.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11620/12613 [02:09<00:11, 89.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11640/12613 [02:09<00:10, 89.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  92% 11660/12613 [02:09<00:10, 89.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11680/12613 [02:09<00:10, 89.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11700/12613 [02:09<00:10, 90.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11720/12613 [02:10<00:09, 90.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11740/12613 [02:10<00:09, 90.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11760/12613 [02:10<00:09, 90.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  93% 11780/12613 [02:10<00:09, 90.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11800/12613 [02:10<00:08, 90.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11820/12613 [02:10<00:08, 90.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11840/12613 [02:10<00:08, 90.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11860/12613 [02:10<00:08, 90.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11880/12613 [02:10<00:08, 90.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  94% 11900/12613 [02:10<00:07, 90.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 11920/12613 [02:11<00:07, 90.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 11940/12613 [02:11<00:07, 91.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 11960/12613 [02:11<00:07, 91.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 11980/12613 [02:11<00:06, 91.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 12000/12613 [02:11<00:06, 91.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 12020/12613 [02:11<00:06, 91.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  95% 12040/12613 [02:11<00:06, 91.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12060/12613 [02:11<00:06, 91.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12080/12613 [02:11<00:05, 91.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12100/12613 [02:12<00:05, 91.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12120/12613 [02:12<00:05, 91.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12140/12613 [02:12<00:05, 91.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  96% 12160/12613 [02:12<00:04, 91.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12180/12613 [02:12<00:04, 91.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12200/12613 [02:12<00:04, 91.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12220/12613 [02:12<00:04, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12240/12613 [02:12<00:04, 92.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12260/12613 [02:12<00:03, 92.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  97% 12280/12613 [02:13<00:03, 92.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12300/12613 [02:13<00:03, 92.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12320/12613 [02:13<00:03, 92.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12340/12613 [02:13<00:02, 92.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12360/12613 [02:13<00:02, 92.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12380/12613 [02:13<00:02, 92.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12400/12613 [02:13<00:02, 92.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  98% 12420/12613 [02:13<00:02, 92.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12440/12613 [02:14<00:01, 92.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12460/12613 [02:14<00:01, 92.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12480/12613 [02:14<00:01, 92.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12500/12613 [02:14<00:01, 93.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12520/12613 [02:14<00:00, 93.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15:  99% 12540/12613 [02:14<00:00, 93.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15: 100% 12560/12613 [02:14<00:00, 93.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15: 100% 12580/12613 [02:14<00:00, 93.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 15: 100% 12600/12613 [02:14<00:00, 93.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 181.36it/s]\u001b[A\n",
            "Epoch 15: 100% 12613/12613 [02:14<00:00, 93.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  80% 10080/12613 [01:56<00:29, 86.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  80% 10100/12613 [02:00<00:30, 83.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  80% 10120/12613 [02:00<00:29, 83.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  80% 10140/12613 [02:01<00:29, 83.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10160/12613 [02:01<00:29, 83.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10180/12613 [02:01<00:28, 83.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10200/12613 [02:01<00:28, 84.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10220/12613 [02:01<00:28, 84.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10240/12613 [02:01<00:28, 84.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  81% 10260/12613 [02:01<00:27, 84.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10280/12613 [02:01<00:27, 84.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10300/12613 [02:01<00:27, 84.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10320/12613 [02:02<00:27, 84.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10340/12613 [02:02<00:26, 84.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10360/12613 [02:02<00:26, 84.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10380/12613 [02:02<00:26, 84.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  82% 10400/12613 [02:02<00:26, 84.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10420/12613 [02:02<00:25, 84.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10440/12613 [02:02<00:25, 85.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10460/12613 [02:02<00:25, 85.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10480/12613 [02:02<00:25, 85.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10500/12613 [02:03<00:24, 85.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  83% 10520/12613 [02:03<00:24, 85.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10540/12613 [02:03<00:24, 85.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10560/12613 [02:03<00:23, 85.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10580/12613 [02:03<00:23, 85.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10600/12613 [02:03<00:23, 85.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10620/12613 [02:03<00:23, 85.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  84% 10640/12613 [02:03<00:22, 85.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10660/12613 [02:03<00:22, 86.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10680/12613 [02:04<00:22, 86.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10700/12613 [02:04<00:22, 86.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10720/12613 [02:04<00:21, 86.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10740/12613 [02:04<00:21, 86.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10760/12613 [02:04<00:21, 86.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  85% 10780/12613 [02:04<00:21, 86.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10800/12613 [02:04<00:20, 86.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10820/12613 [02:04<00:20, 86.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10840/12613 [02:04<00:20, 86.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10860/12613 [02:05<00:20, 86.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10880/12613 [02:05<00:19, 86.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  86% 10900/12613 [02:05<00:19, 86.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 10920/12613 [02:05<00:19, 87.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 10940/12613 [02:05<00:19, 87.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 10960/12613 [02:05<00:18, 87.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 10980/12613 [02:05<00:18, 87.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 11000/12613 [02:05<00:18, 87.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  87% 11020/12613 [02:05<00:18, 87.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11040/12613 [02:06<00:17, 87.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11060/12613 [02:06<00:17, 87.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11080/12613 [02:06<00:17, 87.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11100/12613 [02:06<00:17, 87.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11120/12613 [02:06<00:16, 87.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11140/12613 [02:06<00:16, 87.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  88% 11160/12613 [02:06<00:16, 88.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11180/12613 [02:06<00:16, 88.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11200/12613 [02:06<00:16, 88.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11220/12613 [02:07<00:15, 88.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11240/12613 [02:07<00:15, 88.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11260/12613 [02:07<00:15, 88.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  89% 11280/12613 [02:07<00:15, 88.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11300/12613 [02:07<00:14, 88.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11320/12613 [02:07<00:14, 88.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11340/12613 [02:07<00:14, 88.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11360/12613 [02:07<00:14, 88.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11380/12613 [02:07<00:13, 88.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  90% 11400/12613 [02:08<00:13, 89.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11420/12613 [02:08<00:13, 89.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11440/12613 [02:08<00:13, 89.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11460/12613 [02:08<00:12, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11480/12613 [02:08<00:12, 89.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11500/12613 [02:08<00:12, 89.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11520/12613 [02:08<00:12, 89.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  91% 11540/12613 [02:08<00:11, 89.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11560/12613 [02:08<00:11, 89.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11580/12613 [02:09<00:11, 89.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11600/12613 [02:09<00:11, 89.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11620/12613 [02:09<00:11, 89.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11640/12613 [02:09<00:10, 89.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  92% 11660/12613 [02:09<00:10, 90.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11680/12613 [02:09<00:10, 90.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11700/12613 [02:09<00:10, 90.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11720/12613 [02:09<00:09, 90.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11740/12613 [02:09<00:09, 90.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11760/12613 [02:10<00:09, 90.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  93% 11780/12613 [02:10<00:09, 90.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11800/12613 [02:10<00:08, 90.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11820/12613 [02:10<00:08, 90.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11840/12613 [02:10<00:08, 90.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11860/12613 [02:10<00:08, 90.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11880/12613 [02:10<00:08, 90.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  94% 11900/12613 [02:10<00:07, 90.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 11920/12613 [02:10<00:07, 91.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 11940/12613 [02:11<00:07, 91.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 11960/12613 [02:11<00:07, 91.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 11980/12613 [02:11<00:06, 91.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 12000/12613 [02:11<00:06, 91.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 12020/12613 [02:11<00:06, 91.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  95% 12040/12613 [02:11<00:06, 91.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12060/12613 [02:11<00:06, 91.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12080/12613 [02:11<00:05, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12100/12613 [02:11<00:05, 91.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12120/12613 [02:12<00:05, 91.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12140/12613 [02:12<00:05, 91.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  96% 12160/12613 [02:12<00:04, 91.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12180/12613 [02:12<00:04, 92.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12200/12613 [02:12<00:04, 92.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12220/12613 [02:12<00:04, 92.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12240/12613 [02:12<00:04, 92.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12260/12613 [02:12<00:03, 92.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  97% 12280/12613 [02:12<00:03, 92.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12300/12613 [02:13<00:03, 92.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12320/12613 [02:13<00:03, 92.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12340/12613 [02:13<00:02, 92.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12360/12613 [02:13<00:02, 92.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12380/12613 [02:13<00:02, 92.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12400/12613 [02:13<00:02, 92.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  98% 12420/12613 [02:13<00:02, 92.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12440/12613 [02:13<00:01, 92.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12460/12613 [02:13<00:01, 93.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12480/12613 [02:13<00:01, 93.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12500/12613 [02:14<00:01, 93.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12520/12613 [02:14<00:00, 93.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16:  99% 12540/12613 [02:14<00:00, 93.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16: 100% 12560/12613 [02:14<00:00, 93.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16: 100% 12580/12613 [02:14<00:00, 93.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 16: 100% 12600/12613 [02:14<00:00, 93.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 181.34it/s]\u001b[A\n",
            "Epoch 16: 100% 12613/12613 [02:14<00:00, 93.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  80% 10080/12613 [01:55<00:29, 87.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  80% 10100/12613 [02:00<00:29, 83.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  80% 10120/12613 [02:00<00:29, 84.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  80% 10140/12613 [02:00<00:29, 84.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10160/12613 [02:00<00:29, 84.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10180/12613 [02:00<00:28, 84.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10200/12613 [02:00<00:28, 84.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10220/12613 [02:01<00:28, 84.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10240/12613 [02:01<00:28, 84.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  81% 10260/12613 [02:01<00:27, 84.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10280/12613 [02:01<00:27, 84.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10300/12613 [02:01<00:27, 84.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10320/12613 [02:01<00:27, 84.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10340/12613 [02:01<00:26, 84.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10360/12613 [02:01<00:26, 85.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10380/12613 [02:01<00:26, 85.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  82% 10400/12613 [02:02<00:25, 85.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10420/12613 [02:02<00:25, 85.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10440/12613 [02:02<00:25, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10460/12613 [02:02<00:25, 85.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10480/12613 [02:02<00:24, 85.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10500/12613 [02:02<00:24, 85.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  83% 10520/12613 [02:02<00:24, 85.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10540/12613 [02:02<00:24, 85.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10560/12613 [02:02<00:23, 85.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10580/12613 [02:03<00:23, 85.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10600/12613 [02:03<00:23, 86.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10620/12613 [02:03<00:23, 86.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  84% 10640/12613 [02:03<00:22, 86.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10660/12613 [02:03<00:22, 86.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10680/12613 [02:03<00:22, 86.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10700/12613 [02:03<00:22, 86.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10720/12613 [02:03<00:21, 86.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10740/12613 [02:04<00:21, 86.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10760/12613 [02:04<00:21, 86.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  85% 10780/12613 [02:04<00:21, 86.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10800/12613 [02:04<00:20, 86.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10820/12613 [02:04<00:20, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10840/12613 [02:04<00:20, 87.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10860/12613 [02:04<00:20, 87.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10880/12613 [02:04<00:19, 87.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  86% 10900/12613 [02:04<00:19, 87.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 10920/12613 [02:05<00:19, 87.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 10940/12613 [02:05<00:19, 87.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 10960/12613 [02:05<00:18, 87.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 10980/12613 [02:05<00:18, 87.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 11000/12613 [02:05<00:18, 87.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  87% 11020/12613 [02:05<00:18, 87.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11040/12613 [02:05<00:17, 87.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11060/12613 [02:05<00:17, 87.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11080/12613 [02:05<00:17, 88.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11100/12613 [02:05<00:17, 88.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11120/12613 [02:06<00:16, 88.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11140/12613 [02:06<00:16, 88.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  88% 11160/12613 [02:06<00:16, 88.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11180/12613 [02:06<00:16, 88.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11200/12613 [02:06<00:15, 88.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11220/12613 [02:06<00:15, 88.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11240/12613 [02:06<00:15, 88.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11260/12613 [02:06<00:15, 88.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  89% 11280/12613 [02:06<00:15, 88.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11300/12613 [02:07<00:14, 88.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11320/12613 [02:07<00:14, 89.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11340/12613 [02:07<00:14, 89.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11360/12613 [02:07<00:14, 89.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11380/12613 [02:07<00:13, 89.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  90% 11400/12613 [02:07<00:13, 89.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11420/12613 [02:07<00:13, 89.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11440/12613 [02:07<00:13, 89.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11460/12613 [02:07<00:12, 89.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11480/12613 [02:08<00:12, 89.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11500/12613 [02:08<00:12, 89.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11520/12613 [02:08<00:12, 89.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  91% 11540/12613 [02:08<00:11, 89.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11560/12613 [02:08<00:11, 90.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11580/12613 [02:08<00:11, 90.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11600/12613 [02:08<00:11, 90.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11620/12613 [02:08<00:11, 90.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11640/12613 [02:08<00:10, 90.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  92% 11660/12613 [02:08<00:10, 90.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11680/12613 [02:09<00:10, 90.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11700/12613 [02:09<00:10, 90.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11720/12613 [02:09<00:09, 90.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11740/12613 [02:09<00:09, 90.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11760/12613 [02:09<00:09, 90.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  93% 11780/12613 [02:09<00:09, 90.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11800/12613 [02:09<00:08, 90.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11820/12613 [02:09<00:08, 91.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11840/12613 [02:09<00:08, 91.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11860/12613 [02:10<00:08, 91.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11880/12613 [02:10<00:08, 91.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  94% 11900/12613 [02:10<00:07, 91.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 11920/12613 [02:10<00:07, 91.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 11940/12613 [02:10<00:07, 91.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 11960/12613 [02:10<00:07, 91.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 11980/12613 [02:10<00:06, 91.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 12000/12613 [02:10<00:06, 91.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 12020/12613 [02:10<00:06, 91.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  95% 12040/12613 [02:11<00:06, 91.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12060/12613 [02:11<00:06, 91.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12080/12613 [02:11<00:05, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12100/12613 [02:11<00:05, 92.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12120/12613 [02:11<00:05, 92.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12140/12613 [02:11<00:05, 92.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  96% 12160/12613 [02:11<00:04, 92.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12180/12613 [02:11<00:04, 92.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12200/12613 [02:11<00:04, 92.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12220/12613 [02:12<00:04, 92.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12240/12613 [02:12<00:04, 92.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12260/12613 [02:12<00:03, 92.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  97% 12280/12613 [02:12<00:03, 92.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12300/12613 [02:12<00:03, 92.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12320/12613 [02:12<00:03, 92.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12340/12613 [02:12<00:02, 93.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12360/12613 [02:12<00:02, 93.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12380/12613 [02:12<00:02, 93.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12400/12613 [02:12<00:02, 93.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  98% 12420/12613 [02:13<00:02, 93.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12440/12613 [02:13<00:01, 93.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12460/12613 [02:13<00:01, 93.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12480/12613 [02:13<00:01, 93.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12500/12613 [02:13<00:01, 93.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12520/12613 [02:13<00:00, 93.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17:  99% 12540/12613 [02:13<00:00, 93.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17: 100% 12560/12613 [02:13<00:00, 93.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17: 100% 12580/12613 [02:13<00:00, 93.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 17: 100% 12600/12613 [02:13<00:00, 94.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.64it/s]\u001b[A\n",
            "Epoch 17: 100% 12613/12613 [02:14<00:00, 94.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  80% 10080/12613 [01:55<00:29, 87.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  80% 10100/12613 [02:00<00:29, 83.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  80% 10120/12613 [02:00<00:29, 83.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  80% 10140/12613 [02:00<00:29, 84.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10160/12613 [02:00<00:29, 84.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10180/12613 [02:00<00:28, 84.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10200/12613 [02:00<00:28, 84.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10220/12613 [02:01<00:28, 84.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10240/12613 [02:01<00:28, 84.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  81% 10260/12613 [02:01<00:27, 84.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10280/12613 [02:01<00:27, 84.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10300/12613 [02:01<00:27, 84.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10320/12613 [02:01<00:27, 84.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10340/12613 [02:01<00:26, 84.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10360/12613 [02:01<00:26, 85.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10380/12613 [02:01<00:26, 85.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  82% 10400/12613 [02:01<00:25, 85.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10420/12613 [02:02<00:25, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10440/12613 [02:02<00:25, 85.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10460/12613 [02:02<00:25, 85.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10480/12613 [02:02<00:24, 85.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10500/12613 [02:02<00:24, 85.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  83% 10520/12613 [02:02<00:24, 85.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10540/12613 [02:02<00:24, 85.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10560/12613 [02:02<00:23, 85.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10580/12613 [02:02<00:23, 86.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10600/12613 [02:03<00:23, 86.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10620/12613 [02:03<00:23, 86.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  84% 10640/12613 [02:03<00:22, 86.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10660/12613 [02:03<00:22, 86.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10680/12613 [02:03<00:22, 86.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10700/12613 [02:03<00:22, 86.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10720/12613 [02:03<00:21, 86.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10740/12613 [02:03<00:21, 86.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10760/12613 [02:03<00:21, 86.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  85% 10780/12613 [02:03<00:21, 86.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10800/12613 [02:04<00:20, 87.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10820/12613 [02:04<00:20, 87.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10840/12613 [02:04<00:20, 87.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10860/12613 [02:04<00:20, 87.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10880/12613 [02:04<00:19, 87.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  86% 10900/12613 [02:04<00:19, 87.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 10920/12613 [02:04<00:19, 87.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 10940/12613 [02:04<00:19, 87.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 10960/12613 [02:04<00:18, 87.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 10980/12613 [02:05<00:18, 87.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 11000/12613 [02:05<00:18, 87.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  87% 11020/12613 [02:05<00:18, 87.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11040/12613 [02:05<00:17, 88.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11060/12613 [02:05<00:17, 88.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11080/12613 [02:05<00:17, 88.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11100/12613 [02:05<00:17, 88.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11120/12613 [02:05<00:16, 88.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11140/12613 [02:05<00:16, 88.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  88% 11160/12613 [02:05<00:16, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11180/12613 [02:06<00:16, 88.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11200/12613 [02:06<00:15, 88.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11220/12613 [02:06<00:15, 88.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11240/12613 [02:06<00:15, 88.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11260/12613 [02:06<00:15, 89.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  89% 11280/12613 [02:06<00:14, 89.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11300/12613 [02:06<00:14, 89.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11320/12613 [02:06<00:14, 89.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11340/12613 [02:06<00:14, 89.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11360/12613 [02:07<00:14, 89.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11380/12613 [02:07<00:13, 89.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  90% 11400/12613 [02:07<00:13, 89.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11420/12613 [02:07<00:13, 89.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11440/12613 [02:07<00:13, 89.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11460/12613 [02:07<00:12, 89.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11480/12613 [02:07<00:12, 89.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11500/12613 [02:07<00:12, 89.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11520/12613 [02:08<00:12, 90.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  91% 11540/12613 [02:08<00:11, 90.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11560/12613 [02:08<00:11, 90.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11580/12613 [02:08<00:11, 90.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11600/12613 [02:08<00:11, 90.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11620/12613 [02:08<00:10, 90.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11640/12613 [02:08<00:10, 90.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  92% 11660/12613 [02:08<00:10, 90.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11680/12613 [02:08<00:10, 90.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11700/12613 [02:08<00:10, 90.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11720/12613 [02:09<00:09, 90.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11740/12613 [02:09<00:09, 90.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11760/12613 [02:09<00:09, 90.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  93% 11780/12613 [02:09<00:09, 91.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11800/12613 [02:09<00:08, 91.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11820/12613 [02:09<00:08, 91.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11840/12613 [02:09<00:08, 91.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11860/12613 [02:09<00:08, 91.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11880/12613 [02:09<00:08, 91.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  94% 11900/12613 [02:10<00:07, 91.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 11920/12613 [02:10<00:07, 91.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 11940/12613 [02:10<00:07, 91.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 11960/12613 [02:10<00:07, 91.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 11980/12613 [02:10<00:06, 91.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 12000/12613 [02:10<00:06, 91.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 12020/12613 [02:10<00:06, 91.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  95% 12040/12613 [02:10<00:06, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12060/12613 [02:10<00:06, 92.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12080/12613 [02:11<00:05, 92.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12100/12613 [02:11<00:05, 92.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12120/12613 [02:11<00:05, 92.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12140/12613 [02:11<00:05, 92.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  96% 12160/12613 [02:11<00:04, 92.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12180/12613 [02:11<00:04, 92.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12200/12613 [02:11<00:04, 92.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12220/12613 [02:11<00:04, 92.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12240/12613 [02:11<00:04, 92.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12260/12613 [02:11<00:03, 92.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  97% 12280/12613 [02:12<00:03, 92.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12300/12613 [02:12<00:03, 93.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12320/12613 [02:12<00:03, 93.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12340/12613 [02:12<00:02, 93.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12360/12613 [02:12<00:02, 93.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12380/12613 [02:12<00:02, 93.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12400/12613 [02:12<00:02, 93.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  98% 12420/12613 [02:12<00:02, 93.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12440/12613 [02:12<00:01, 93.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12460/12613 [02:13<00:01, 93.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12480/12613 [02:13<00:01, 93.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12500/12613 [02:13<00:01, 93.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12520/12613 [02:13<00:00, 93.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18:  99% 12540/12613 [02:13<00:00, 93.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18: 100% 12560/12613 [02:13<00:00, 93.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18: 100% 12580/12613 [02:13<00:00, 94.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 18: 100% 12600/12613 [02:13<00:00, 94.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 185.66it/s]\u001b[A\n",
            "Epoch 18: 100% 12613/12613 [02:13<00:00, 94.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  80% 10080/12613 [01:55<00:29, 87.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  80% 10100/12613 [02:00<00:29, 83.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  80% 10120/12613 [02:00<00:29, 84.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  80% 10140/12613 [02:00<00:29, 84.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10160/12613 [02:00<00:29, 84.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10180/12613 [02:00<00:28, 84.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10200/12613 [02:00<00:28, 84.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10220/12613 [02:00<00:28, 84.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10240/12613 [02:01<00:28, 84.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  81% 10260/12613 [02:01<00:27, 84.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10280/12613 [02:01<00:27, 84.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10300/12613 [02:01<00:27, 84.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10320/12613 [02:01<00:27, 84.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10340/12613 [02:01<00:26, 85.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10360/12613 [02:01<00:26, 85.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10380/12613 [02:01<00:26, 85.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  82% 10400/12613 [02:01<00:25, 85.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10420/12613 [02:02<00:25, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10440/12613 [02:02<00:25, 85.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10460/12613 [02:02<00:25, 85.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10480/12613 [02:02<00:24, 85.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10500/12613 [02:02<00:24, 85.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  83% 10520/12613 [02:02<00:24, 85.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10540/12613 [02:02<00:24, 85.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10560/12613 [02:02<00:23, 85.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10580/12613 [02:02<00:23, 86.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10600/12613 [02:03<00:23, 86.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10620/12613 [02:03<00:23, 86.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  84% 10640/12613 [02:03<00:22, 86.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10660/12613 [02:03<00:22, 86.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10680/12613 [02:03<00:22, 86.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10700/12613 [02:03<00:22, 86.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10720/12613 [02:03<00:21, 86.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10740/12613 [02:03<00:21, 86.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10760/12613 [02:03<00:21, 86.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  85% 10780/12613 [02:04<00:21, 86.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10800/12613 [02:04<00:20, 86.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10820/12613 [02:04<00:20, 87.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10840/12613 [02:04<00:20, 87.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10860/12613 [02:04<00:20, 87.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10880/12613 [02:04<00:19, 87.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  86% 10900/12613 [02:04<00:19, 87.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 10920/12613 [02:04<00:19, 87.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 10940/12613 [02:04<00:19, 87.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 10960/12613 [02:05<00:18, 87.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 10980/12613 [02:05<00:18, 87.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 11000/12613 [02:05<00:18, 87.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  87% 11020/12613 [02:05<00:18, 87.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11040/12613 [02:05<00:17, 87.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11060/12613 [02:05<00:17, 88.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11080/12613 [02:05<00:17, 88.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11100/12613 [02:05<00:17, 88.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11120/12613 [02:06<00:16, 88.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11140/12613 [02:06<00:16, 88.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  88% 11160/12613 [02:06<00:16, 88.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11180/12613 [02:06<00:16, 88.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11200/12613 [02:06<00:15, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11220/12613 [02:06<00:15, 88.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11240/12613 [02:06<00:15, 88.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11260/12613 [02:06<00:15, 88.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  89% 11280/12613 [02:06<00:14, 88.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11300/12613 [02:07<00:14, 88.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11320/12613 [02:07<00:14, 89.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11340/12613 [02:07<00:14, 89.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11360/12613 [02:07<00:14, 89.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11380/12613 [02:07<00:13, 89.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  90% 11400/12613 [02:07<00:13, 89.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11420/12613 [02:07<00:13, 89.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11440/12613 [02:07<00:13, 89.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11460/12613 [02:07<00:12, 89.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11480/12613 [02:08<00:12, 89.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11500/12613 [02:08<00:12, 89.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11520/12613 [02:08<00:12, 89.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  91% 11540/12613 [02:08<00:11, 89.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11560/12613 [02:08<00:11, 89.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11580/12613 [02:08<00:11, 90.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11600/12613 [02:08<00:11, 90.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11620/12613 [02:08<00:11, 90.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11640/12613 [02:08<00:10, 90.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  92% 11660/12613 [02:09<00:10, 90.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11680/12613 [02:09<00:10, 90.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11700/12613 [02:09<00:10, 90.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11720/12613 [02:09<00:09, 90.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11740/12613 [02:09<00:09, 90.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11760/12613 [02:09<00:09, 90.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  93% 11780/12613 [02:09<00:09, 90.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11800/12613 [02:09<00:08, 90.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11820/12613 [02:09<00:08, 90.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11840/12613 [02:10<00:08, 91.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11860/12613 [02:10<00:08, 91.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11880/12613 [02:10<00:08, 91.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  94% 11900/12613 [02:10<00:07, 91.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 11920/12613 [02:10<00:07, 91.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 11940/12613 [02:10<00:07, 91.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 11960/12613 [02:10<00:07, 91.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 11980/12613 [02:10<00:06, 91.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 12000/12613 [02:10<00:06, 91.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 12020/12613 [02:11<00:06, 91.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  95% 12040/12613 [02:11<00:06, 91.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12060/12613 [02:11<00:06, 91.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12080/12613 [02:11<00:05, 91.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12100/12613 [02:11<00:05, 91.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12120/12613 [02:11<00:05, 92.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12140/12613 [02:11<00:05, 92.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  96% 12160/12613 [02:11<00:04, 92.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12180/12613 [02:12<00:04, 92.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12200/12613 [02:12<00:04, 92.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12220/12613 [02:12<00:04, 92.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12240/12613 [02:12<00:04, 92.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12260/12613 [02:12<00:03, 92.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  97% 12280/12613 [02:12<00:03, 92.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12300/12613 [02:12<00:03, 92.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12320/12613 [02:12<00:03, 92.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12340/12613 [02:12<00:02, 92.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12360/12613 [02:12<00:02, 92.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12380/12613 [02:13<00:02, 93.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12400/12613 [02:13<00:02, 93.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  98% 12420/12613 [02:13<00:02, 93.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12440/12613 [02:13<00:01, 93.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12460/12613 [02:13<00:01, 93.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12480/12613 [02:13<00:01, 93.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12500/12613 [02:13<00:01, 93.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12520/12613 [02:13<00:00, 93.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19:  99% 12540/12613 [02:13<00:00, 93.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19: 100% 12560/12613 [02:14<00:00, 93.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19: 100% 12580/12613 [02:14<00:00, 93.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 19: 100% 12600/12613 [02:14<00:00, 93.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:14<00:00, 179.76it/s]\u001b[A\n",
            "Epoch 19: 100% 12613/12613 [02:14<00:00, 93.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  80% 10080/12613 [01:56<00:29, 86.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  80% 10100/12613 [02:00<00:30, 83.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  80% 10120/12613 [02:01<00:29, 83.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  80% 10140/12613 [02:01<00:29, 83.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10160/12613 [02:01<00:29, 83.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10180/12613 [02:01<00:29, 83.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10200/12613 [02:01<00:28, 83.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10220/12613 [02:01<00:28, 84.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10240/12613 [02:01<00:28, 84.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  81% 10260/12613 [02:01<00:27, 84.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10280/12613 [02:01<00:27, 84.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10300/12613 [02:02<00:27, 84.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10320/12613 [02:02<00:27, 84.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10340/12613 [02:02<00:26, 84.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10360/12613 [02:02<00:26, 84.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10380/12613 [02:02<00:26, 84.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  82% 10400/12613 [02:02<00:26, 84.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10420/12613 [02:02<00:25, 84.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10440/12613 [02:02<00:25, 84.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10460/12613 [02:02<00:25, 85.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10480/12613 [02:03<00:25, 85.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10500/12613 [02:03<00:24, 85.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  83% 10520/12613 [02:03<00:24, 85.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10540/12613 [02:03<00:24, 85.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10560/12613 [02:03<00:24, 85.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10580/12613 [02:03<00:23, 85.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10600/12613 [02:03<00:23, 85.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10620/12613 [02:03<00:23, 85.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  84% 10640/12613 [02:03<00:22, 85.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10660/12613 [02:04<00:22, 85.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10680/12613 [02:04<00:22, 85.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10700/12613 [02:04<00:22, 86.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10720/12613 [02:04<00:21, 86.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10740/12613 [02:04<00:21, 86.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10760/12613 [02:04<00:21, 86.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  85% 10780/12613 [02:04<00:21, 86.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10800/12613 [02:04<00:20, 86.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10820/12613 [02:05<00:20, 86.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10840/12613 [02:05<00:20, 86.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10860/12613 [02:05<00:20, 86.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10880/12613 [02:05<00:19, 86.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  86% 10900/12613 [02:05<00:19, 86.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 10920/12613 [02:05<00:19, 86.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 10940/12613 [02:05<00:19, 87.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 10960/12613 [02:05<00:18, 87.12it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 10980/12613 [02:05<00:18, 87.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 11000/12613 [02:06<00:18, 87.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  87% 11020/12613 [02:06<00:18, 87.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11040/12613 [02:06<00:17, 87.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11060/12613 [02:06<00:17, 87.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11080/12613 [02:06<00:17, 87.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11100/12613 [02:06<00:17, 87.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11120/12613 [02:06<00:17, 87.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11140/12613 [02:06<00:16, 87.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  88% 11160/12613 [02:06<00:16, 87.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11180/12613 [02:07<00:16, 87.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11200/12613 [02:07<00:16, 88.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11220/12613 [02:07<00:15, 88.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11240/12613 [02:07<00:15, 88.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11260/12613 [02:07<00:15, 88.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  89% 11280/12613 [02:07<00:15, 88.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11300/12613 [02:07<00:14, 88.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11320/12613 [02:07<00:14, 88.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11340/12613 [02:07<00:14, 88.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11360/12613 [02:08<00:14, 88.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11380/12613 [02:08<00:13, 88.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  90% 11400/12613 [02:08<00:13, 88.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11420/12613 [02:08<00:13, 88.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11440/12613 [02:08<00:13, 89.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11460/12613 [02:08<00:12, 89.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11480/12613 [02:08<00:12, 89.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11500/12613 [02:08<00:12, 89.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11520/12613 [02:08<00:12, 89.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  91% 11540/12613 [02:09<00:11, 89.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11560/12613 [02:09<00:11, 89.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11580/12613 [02:09<00:11, 89.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11600/12613 [02:09<00:11, 89.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11620/12613 [02:09<00:11, 89.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11640/12613 [02:09<00:10, 89.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  92% 11660/12613 [02:09<00:10, 89.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11680/12613 [02:09<00:10, 89.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11700/12613 [02:09<00:10, 90.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11720/12613 [02:10<00:09, 90.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11740/12613 [02:10<00:09, 90.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11760/12613 [02:10<00:09, 90.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  93% 11780/12613 [02:10<00:09, 90.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11800/12613 [02:10<00:08, 90.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11820/12613 [02:10<00:08, 90.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11840/12613 [02:10<00:08, 90.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11860/12613 [02:10<00:08, 90.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11880/12613 [02:10<00:08, 90.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  94% 11900/12613 [02:11<00:07, 90.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 11920/12613 [02:11<00:07, 90.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 11940/12613 [02:11<00:07, 91.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 11960/12613 [02:11<00:07, 91.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 11980/12613 [02:11<00:06, 91.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 12000/12613 [02:11<00:06, 91.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 12020/12613 [02:11<00:06, 91.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  95% 12040/12613 [02:11<00:06, 91.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12060/12613 [02:11<00:06, 91.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12080/12613 [02:11<00:05, 91.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12100/12613 [02:12<00:05, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12120/12613 [02:12<00:05, 91.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12140/12613 [02:12<00:05, 91.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  96% 12160/12613 [02:12<00:04, 91.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12180/12613 [02:12<00:04, 91.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12200/12613 [02:12<00:04, 92.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12220/12613 [02:12<00:04, 92.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12240/12613 [02:12<00:04, 92.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12260/12613 [02:12<00:03, 92.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  97% 12280/12613 [02:13<00:03, 92.27it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12300/12613 [02:13<00:03, 92.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12320/12613 [02:13<00:03, 92.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12340/12613 [02:13<00:02, 92.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12360/12613 [02:13<00:02, 92.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12380/12613 [02:13<00:02, 92.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12400/12613 [02:13<00:02, 92.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  98% 12420/12613 [02:13<00:02, 92.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12440/12613 [02:14<00:01, 92.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12460/12613 [02:14<00:01, 92.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12480/12613 [02:14<00:01, 92.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12500/12613 [02:14<00:01, 93.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12520/12613 [02:14<00:00, 93.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20:  99% 12540/12613 [02:14<00:00, 93.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20: 100% 12560/12613 [02:14<00:00, 93.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20: 100% 12580/12613 [02:14<00:00, 93.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 20: 100% 12600/12613 [02:14<00:00, 93.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:14<00:00, 178.21it/s]\u001b[A\n",
            "Epoch 20: 100% 12613/12613 [02:14<00:00, 93.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  80% 10080/12613 [01:56<00:29, 86.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  80% 10100/12613 [02:01<00:30, 82.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  80% 10120/12613 [02:01<00:30, 83.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  80% 10140/12613 [02:01<00:29, 83.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10160/12613 [02:02<00:29, 83.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10180/12613 [02:02<00:29, 83.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10200/12613 [02:02<00:28, 83.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10220/12613 [02:02<00:28, 83.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10240/12613 [02:02<00:28, 83.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  81% 10260/12613 [02:02<00:28, 83.68it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10280/12613 [02:02<00:27, 83.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10300/12613 [02:02<00:27, 83.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10320/12613 [02:02<00:27, 83.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10340/12613 [02:03<00:27, 84.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10360/12613 [02:03<00:26, 84.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10380/12613 [02:03<00:26, 84.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  82% 10400/12613 [02:03<00:26, 84.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10420/12613 [02:03<00:25, 84.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10440/12613 [02:03<00:25, 84.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10460/12613 [02:03<00:25, 84.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10480/12613 [02:03<00:25, 84.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10500/12613 [02:03<00:24, 84.72it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  83% 10520/12613 [02:04<00:24, 84.80it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10540/12613 [02:04<00:24, 84.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10560/12613 [02:04<00:24, 84.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10580/12613 [02:04<00:23, 85.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10600/12613 [02:04<00:23, 85.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10620/12613 [02:04<00:23, 85.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  84% 10640/12613 [02:04<00:23, 85.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10660/12613 [02:04<00:22, 85.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10680/12613 [02:04<00:22, 85.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10700/12613 [02:05<00:22, 85.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10720/12613 [02:05<00:22, 85.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10740/12613 [02:05<00:21, 85.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10760/12613 [02:05<00:21, 85.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  85% 10780/12613 [02:05<00:21, 85.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10800/12613 [02:05<00:21, 86.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10820/12613 [02:05<00:20, 86.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10840/12613 [02:05<00:20, 86.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10860/12613 [02:05<00:20, 86.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10880/12613 [02:06<00:20, 86.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  86% 10900/12613 [02:06<00:19, 86.42it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 10920/12613 [02:06<00:19, 86.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 10940/12613 [02:06<00:19, 86.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 10960/12613 [02:06<00:19, 86.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 10980/12613 [02:06<00:18, 86.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 11000/12613 [02:06<00:18, 86.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  87% 11020/12613 [02:06<00:18, 86.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11040/12613 [02:06<00:18, 87.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11060/12613 [02:06<00:17, 87.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11080/12613 [02:07<00:17, 87.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11100/12613 [02:07<00:17, 87.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11120/12613 [02:07<00:17, 87.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11140/12613 [02:07<00:16, 87.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  88% 11160/12613 [02:07<00:16, 87.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11180/12613 [02:07<00:16, 87.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11200/12613 [02:07<00:16, 87.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11220/12613 [02:07<00:15, 87.75it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11240/12613 [02:07<00:15, 87.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11260/12613 [02:08<00:15, 87.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  89% 11280/12613 [02:08<00:15, 88.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11300/12613 [02:08<00:14, 88.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11320/12613 [02:08<00:14, 88.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11340/12613 [02:08<00:14, 88.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11360/12613 [02:08<00:14, 88.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11380/12613 [02:08<00:13, 88.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  90% 11400/12613 [02:08<00:13, 88.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11420/12613 [02:08<00:13, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11440/12613 [02:09<00:13, 88.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11460/12613 [02:09<00:12, 88.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11480/12613 [02:09<00:12, 88.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11500/12613 [02:09<00:12, 88.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11520/12613 [02:09<00:12, 88.98it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  91% 11540/12613 [02:09<00:12, 89.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11560/12613 [02:09<00:11, 89.14it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11580/12613 [02:09<00:11, 89.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11600/12613 [02:09<00:11, 89.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11620/12613 [02:10<00:11, 89.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11640/12613 [02:10<00:10, 89.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  92% 11660/12613 [02:10<00:10, 89.53it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11680/12613 [02:10<00:10, 89.61it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11700/12613 [02:10<00:10, 89.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11720/12613 [02:10<00:09, 89.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11740/12613 [02:10<00:09, 89.84it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11760/12613 [02:10<00:09, 89.92it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  93% 11780/12613 [02:10<00:09, 89.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11800/12613 [02:11<00:09, 90.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11820/12613 [02:11<00:08, 90.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11840/12613 [02:11<00:08, 90.22it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11860/12613 [02:11<00:08, 90.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11880/12613 [02:11<00:08, 90.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  94% 11900/12613 [02:11<00:07, 90.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 11920/12613 [02:11<00:07, 90.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 11940/12613 [02:11<00:07, 90.59it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 11960/12613 [02:11<00:07, 90.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 11980/12613 [02:12<00:06, 90.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 12000/12613 [02:12<00:06, 90.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 12020/12613 [02:12<00:06, 90.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  95% 12040/12613 [02:12<00:06, 90.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12060/12613 [02:12<00:06, 91.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12080/12613 [02:12<00:05, 91.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12100/12613 [02:12<00:05, 91.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12120/12613 [02:12<00:05, 91.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12140/12613 [02:12<00:05, 91.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  96% 12160/12613 [02:13<00:04, 91.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12180/12613 [02:13<00:04, 91.49it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12200/12613 [02:13<00:04, 91.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12220/12613 [02:13<00:04, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12240/12613 [02:13<00:04, 91.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12260/12613 [02:13<00:03, 91.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  97% 12280/12613 [02:13<00:03, 91.85it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12300/12613 [02:13<00:03, 91.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12320/12613 [02:13<00:03, 92.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12340/12613 [02:14<00:02, 92.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12360/12613 [02:14<00:02, 92.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12380/12613 [02:14<00:02, 92.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12400/12613 [02:14<00:02, 92.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  98% 12420/12613 [02:14<00:02, 92.36it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12440/12613 [02:14<00:01, 92.44it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12460/12613 [02:14<00:01, 92.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12480/12613 [02:14<00:01, 92.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12500/12613 [02:14<00:01, 92.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12520/12613 [02:15<00:01, 92.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21:  99% 12540/12613 [02:15<00:00, 92.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21: 100% 12560/12613 [02:15<00:00, 92.88it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21: 100% 12580/12613 [02:15<00:00, 92.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 21: 100% 12600/12613 [02:15<00:00, 93.03it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 182.25it/s]\u001b[A\n",
            "Epoch 21: 100% 12613/12613 [02:15<00:00, 93.06it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  80% 10080/12613 [01:56<00:29, 86.77it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  80% 10100/12613 [02:00<00:30, 83.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  80% 10120/12613 [02:00<00:29, 83.69it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  80% 10140/12613 [02:01<00:29, 83.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10160/12613 [02:01<00:29, 83.87it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10180/12613 [02:01<00:28, 83.95it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10200/12613 [02:01<00:28, 84.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10220/12613 [02:01<00:28, 84.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10240/12613 [02:01<00:28, 84.21it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  81% 10260/12613 [02:01<00:27, 84.29it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10280/12613 [02:01<00:27, 84.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10300/12613 [02:01<00:27, 84.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10320/12613 [02:02<00:27, 84.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10340/12613 [02:02<00:26, 84.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10360/12613 [02:02<00:26, 84.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10380/12613 [02:02<00:26, 84.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  82% 10400/12613 [02:02<00:26, 84.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10420/12613 [02:02<00:25, 85.00it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10440/12613 [02:02<00:25, 85.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10460/12613 [02:02<00:25, 85.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10480/12613 [02:02<00:25, 85.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10500/12613 [02:03<00:24, 85.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  83% 10520/12613 [02:03<00:24, 85.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10540/12613 [02:03<00:24, 85.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10560/12613 [02:03<00:23, 85.60it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10580/12613 [02:03<00:23, 85.67it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10600/12613 [02:03<00:23, 85.76it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10620/12613 [02:03<00:23, 85.83it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  84% 10640/12613 [02:03<00:22, 85.91it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10660/12613 [02:03<00:22, 85.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10680/12613 [02:04<00:22, 86.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10700/12613 [02:04<00:22, 86.16it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10720/12613 [02:04<00:21, 86.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10740/12613 [02:04<00:21, 86.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10760/12613 [02:04<00:21, 86.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  85% 10780/12613 [02:04<00:21, 86.50it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10800/12613 [02:04<00:20, 86.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10820/12613 [02:04<00:20, 86.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10840/12613 [02:04<00:20, 86.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10860/12613 [02:05<00:20, 86.82it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10880/12613 [02:05<00:19, 86.90it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  86% 10900/12613 [02:05<00:19, 86.99it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 10920/12613 [02:05<00:19, 87.07it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 10940/12613 [02:05<00:19, 87.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 10960/12613 [02:05<00:18, 87.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 10980/12613 [02:05<00:18, 87.31it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 11000/12613 [02:05<00:18, 87.38it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  87% 11020/12613 [02:06<00:18, 87.46it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11040/12613 [02:06<00:17, 87.54it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11060/12613 [02:06<00:17, 87.62it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11080/12613 [02:06<00:17, 87.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11100/12613 [02:06<00:17, 87.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11120/12613 [02:06<00:16, 87.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11140/12613 [02:06<00:16, 87.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  88% 11160/12613 [02:06<00:16, 88.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11180/12613 [02:06<00:16, 88.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11200/12613 [02:07<00:16, 88.18it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11220/12613 [02:07<00:15, 88.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11240/12613 [02:07<00:15, 88.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11260/12613 [02:07<00:15, 88.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  89% 11280/12613 [02:07<00:15, 88.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11300/12613 [02:07<00:14, 88.57it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11320/12613 [02:07<00:14, 88.65it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11340/12613 [02:07<00:14, 88.73it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11360/12613 [02:07<00:14, 88.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11380/12613 [02:08<00:13, 88.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  90% 11400/12613 [02:08<00:13, 88.97it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11420/12613 [02:08<00:13, 89.05it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11440/12613 [02:08<00:13, 89.13it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11460/12613 [02:08<00:12, 89.20it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11480/12613 [02:08<00:12, 89.28it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11500/12613 [02:08<00:12, 89.35it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11520/12613 [02:08<00:12, 89.43it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  91% 11540/12613 [02:08<00:11, 89.51it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11560/12613 [02:09<00:11, 89.58it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11580/12613 [02:09<00:11, 89.66it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11600/12613 [02:09<00:11, 89.74it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11620/12613 [02:09<00:11, 89.81it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11640/12613 [02:09<00:10, 89.89it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  92% 11660/12613 [02:09<00:10, 89.96it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11680/12613 [02:09<00:10, 90.04it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11700/12613 [02:09<00:10, 90.11it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11720/12613 [02:09<00:09, 90.19it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11740/12613 [02:10<00:09, 90.26it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11760/12613 [02:10<00:09, 90.34it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  93% 11780/12613 [02:10<00:09, 90.41it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11800/12613 [02:10<00:08, 90.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11820/12613 [02:10<00:08, 90.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11840/12613 [02:10<00:08, 90.64it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11860/12613 [02:10<00:08, 90.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11880/12613 [02:10<00:08, 90.79it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  94% 11900/12613 [02:10<00:07, 90.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 11920/12613 [02:11<00:07, 90.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 11940/12613 [02:11<00:07, 91.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 11960/12613 [02:11<00:07, 91.09it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 11980/12613 [02:11<00:06, 91.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 12000/12613 [02:11<00:06, 91.24it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 12020/12613 [02:11<00:06, 91.32it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  95% 12040/12613 [02:11<00:06, 91.39it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12060/12613 [02:11<00:06, 91.47it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12080/12613 [02:11<00:05, 91.55it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12100/12613 [02:12<00:05, 91.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12120/12613 [02:12<00:05, 91.70it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12140/12613 [02:12<00:05, 91.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  96% 12160/12613 [02:12<00:04, 91.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12180/12613 [02:12<00:04, 91.94it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12200/12613 [02:12<00:04, 92.02it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12220/12613 [02:12<00:04, 92.10it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12240/12613 [02:12<00:04, 92.17it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12260/12613 [02:12<00:03, 92.25it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  97% 12280/12613 [02:13<00:03, 92.33it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12300/12613 [02:13<00:03, 92.40it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12320/12613 [02:13<00:03, 92.48it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12340/12613 [02:13<00:02, 92.56it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12360/12613 [02:13<00:02, 92.63it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12380/12613 [02:13<00:02, 92.71it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12400/12613 [02:13<00:02, 92.78it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  98% 12420/12613 [02:13<00:02, 92.86it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12440/12613 [02:13<00:01, 92.93it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12460/12613 [02:13<00:01, 93.01it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12480/12613 [02:14<00:01, 93.08it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12500/12613 [02:14<00:01, 93.15it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12520/12613 [02:14<00:00, 93.23it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22:  99% 12540/12613 [02:14<00:00, 93.30it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22: 100% 12560/12613 [02:14<00:00, 93.37it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22: 100% 12580/12613 [02:14<00:00, 93.45it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 22: 100% 12600/12613 [02:14<00:00, 93.52it/s, loss=3.69, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:14<00:00, 179.65it/s]\u001b[A\n",
            "Epoch 22: 100% 12613/12613 [02:14<00:00, 93.56it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  80% 10080/12613 [01:56<00:29, 86.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  80% 10100/12613 [02:00<00:30, 83.51it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  80% 10120/12613 [02:01<00:29, 83.61it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  80% 10140/12613 [02:01<00:29, 83.69it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10160/12613 [02:01<00:29, 83.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10180/12613 [02:01<00:29, 83.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10200/12613 [02:01<00:28, 83.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10220/12613 [02:01<00:28, 84.04it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10240/12613 [02:01<00:28, 84.13it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  81% 10260/12613 [02:01<00:27, 84.22it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10280/12613 [02:01<00:27, 84.31it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10300/12613 [02:02<00:27, 84.39it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10320/12613 [02:02<00:27, 84.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10340/12613 [02:02<00:26, 84.56it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10360/12613 [02:02<00:26, 84.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10380/12613 [02:02<00:26, 84.74it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  82% 10400/12613 [02:02<00:26, 84.82it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10420/12613 [02:02<00:25, 84.91it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10440/12613 [02:02<00:25, 85.00it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10460/12613 [02:02<00:25, 85.08it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10480/12613 [02:03<00:25, 85.17it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10500/12613 [02:03<00:24, 85.25it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  83% 10520/12613 [02:03<00:24, 85.34it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10540/12613 [02:03<00:24, 85.42it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10560/12613 [02:03<00:24, 85.51it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10580/12613 [02:03<00:23, 85.59it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10600/12613 [02:03<00:23, 85.68it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10620/12613 [02:03<00:23, 85.77it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  84% 10640/12613 [02:03<00:22, 85.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10660/12613 [02:04<00:22, 85.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10680/12613 [02:04<00:22, 86.04it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10700/12613 [02:04<00:22, 86.13it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10720/12613 [02:04<00:21, 86.21it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10740/12613 [02:04<00:21, 86.30it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10760/12613 [02:04<00:21, 86.39it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  85% 10780/12613 [02:04<00:21, 86.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10800/12613 [02:04<00:20, 86.57it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10820/12613 [02:04<00:20, 86.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10840/12613 [02:04<00:20, 86.74it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10860/12613 [02:05<00:20, 86.83it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10880/12613 [02:05<00:19, 86.92it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  86% 10900/12613 [02:05<00:19, 87.00it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 10920/12613 [02:05<00:19, 87.09it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 10940/12613 [02:05<00:19, 87.18it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 10960/12613 [02:05<00:18, 87.26it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 10980/12613 [02:05<00:18, 87.35it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 11000/12613 [02:05<00:18, 87.44it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  87% 11020/12613 [02:05<00:18, 87.52it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11040/12613 [02:06<00:17, 87.61it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11060/12613 [02:06<00:17, 87.70it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11080/12613 [02:06<00:17, 87.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11100/12613 [02:06<00:17, 87.87it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11120/12613 [02:06<00:16, 87.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11140/12613 [02:06<00:16, 88.04it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  88% 11160/12613 [02:06<00:16, 88.12it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11180/12613 [02:06<00:16, 88.20it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11200/12613 [02:06<00:16, 88.28it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11220/12613 [02:06<00:15, 88.36it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11240/12613 [02:07<00:15, 88.45it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11260/12613 [02:07<00:15, 88.53it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  89% 11280/12613 [02:07<00:15, 88.61it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11300/12613 [02:07<00:14, 88.61it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11320/12613 [02:07<00:14, 88.69it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11340/12613 [02:07<00:14, 88.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11360/12613 [02:07<00:14, 88.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11380/12613 [02:07<00:13, 88.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  90% 11400/12613 [02:08<00:13, 89.03it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11420/12613 [02:08<00:13, 89.11it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11440/12613 [02:08<00:13, 89.20it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11460/12613 [02:08<00:12, 89.28it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11480/12613 [02:08<00:12, 89.36it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11500/12613 [02:08<00:12, 89.45it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11520/12613 [02:08<00:12, 89.53it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  91% 11540/12613 [02:08<00:11, 89.61it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11560/12613 [02:08<00:11, 89.69it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11580/12613 [02:08<00:11, 89.77it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11600/12613 [02:09<00:11, 89.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11620/12613 [02:09<00:11, 89.94it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11640/12613 [02:09<00:10, 90.02it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  92% 11660/12613 [02:09<00:10, 90.10it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11680/12613 [02:09<00:10, 90.18it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11700/12613 [02:09<00:10, 90.26it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11720/12613 [02:09<00:09, 90.34it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11740/12613 [02:09<00:09, 90.42it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11760/12613 [02:09<00:09, 90.50it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  93% 11780/12613 [02:10<00:09, 90.58it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11800/12613 [02:10<00:08, 90.66it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11820/12613 [02:10<00:08, 90.74it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11840/12613 [02:10<00:08, 90.83it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11860/12613 [02:10<00:08, 90.91it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11880/12613 [02:10<00:08, 90.99it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  94% 11900/12613 [02:10<00:07, 91.07it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 11920/12613 [02:10<00:07, 91.15it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 11940/12613 [02:10<00:07, 91.23it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 11960/12613 [02:10<00:07, 91.31it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 11980/12613 [02:11<00:06, 91.39it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 12000/12613 [02:11<00:06, 91.47it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 12020/12613 [02:11<00:06, 91.55it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  95% 12040/12613 [02:11<00:06, 91.63it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12060/12613 [02:11<00:06, 91.71it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12080/12613 [02:11<00:05, 91.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12100/12613 [02:11<00:05, 91.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12120/12613 [02:11<00:05, 91.94it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12140/12613 [02:11<00:05, 92.01it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  96% 12160/12613 [02:12<00:04, 92.08it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12180/12613 [02:12<00:04, 92.16it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12200/12613 [02:12<00:04, 92.22it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12220/12613 [02:12<00:04, 92.30it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12240/12613 [02:12<00:04, 92.37it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12260/12613 [02:12<00:03, 92.44it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  97% 12280/12613 [02:12<00:03, 92.51it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12300/12613 [02:12<00:03, 92.58it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12320/12613 [02:12<00:03, 92.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12340/12613 [02:13<00:02, 92.72it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12360/12613 [02:13<00:02, 92.80it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12380/12613 [02:13<00:02, 92.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12400/12613 [02:13<00:02, 92.94it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  98% 12420/12613 [02:13<00:02, 93.01it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12440/12613 [02:13<00:01, 93.08it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12460/12613 [02:13<00:01, 93.16it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12480/12613 [02:13<00:01, 93.23it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12500/12613 [02:13<00:01, 93.30it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12520/12613 [02:14<00:00, 93.37it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23:  99% 12540/12613 [02:14<00:00, 93.44it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23: 100% 12560/12613 [02:14<00:00, 93.51it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23: 100% 12580/12613 [02:14<00:00, 93.58it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 23: 100% 12600/12613 [02:14<00:00, 93.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 183.64it/s]\u001b[A\n",
            "Epoch 23: 100% 12613/12613 [02:14<00:00, 93.70it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  80% 10080/12613 [01:56<00:29, 86.57it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/2523 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  80% 10100/12613 [02:01<00:30, 83.42it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  80% 10120/12613 [02:01<00:29, 83.52it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  80% 10140/12613 [02:01<00:29, 83.60it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10160/12613 [02:01<00:29, 83.69it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10180/12613 [02:01<00:29, 83.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10200/12613 [02:01<00:28, 83.86it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10220/12613 [02:01<00:28, 83.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10240/12613 [02:01<00:28, 84.03it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  81% 10260/12613 [02:01<00:27, 84.12it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10280/12613 [02:02<00:27, 84.20it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10300/12613 [02:02<00:27, 84.29it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10320/12613 [02:02<00:27, 84.38it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10340/12613 [02:02<00:26, 84.46it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10360/12613 [02:02<00:26, 84.55it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10380/12613 [02:02<00:26, 84.63it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  82% 10400/12613 [02:02<00:26, 84.72it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10420/12613 [02:02<00:25, 84.81it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10440/12613 [02:02<00:25, 84.89it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10460/12613 [02:03<00:25, 84.97it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10480/12613 [02:03<00:25, 85.06it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10500/12613 [02:03<00:24, 85.15it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  83% 10520/12613 [02:03<00:24, 85.23it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10540/12613 [02:03<00:24, 85.31it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10560/12613 [02:03<00:24, 85.39it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10580/12613 [02:03<00:23, 85.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10600/12613 [02:03<00:23, 85.56it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10620/12613 [02:04<00:23, 85.64it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  84% 10640/12613 [02:04<00:23, 85.73it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10660/12613 [02:04<00:22, 85.81it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10680/12613 [02:04<00:22, 85.89it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10700/12613 [02:04<00:22, 85.97it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10720/12613 [02:04<00:21, 86.05it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10740/12613 [02:04<00:21, 86.14it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10760/12613 [02:04<00:21, 86.22it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  85% 10780/12613 [02:04<00:21, 86.31it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10800/12613 [02:05<00:20, 86.40it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10820/12613 [02:05<00:20, 86.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10840/12613 [02:05<00:20, 86.57it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10860/12613 [02:05<00:20, 86.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10880/12613 [02:05<00:19, 86.73it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  86% 10900/12613 [02:05<00:19, 86.81it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 10920/12613 [02:05<00:19, 86.90it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 10940/12613 [02:05<00:19, 86.99it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 10960/12613 [02:05<00:18, 87.08it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 10980/12613 [02:05<00:18, 87.16it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 11000/12613 [02:06<00:18, 87.25it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  87% 11020/12613 [02:06<00:18, 87.33it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11040/12613 [02:06<00:17, 87.41it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11060/12613 [02:06<00:17, 87.49it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11080/12613 [02:06<00:17, 87.57it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11100/12613 [02:06<00:17, 87.65it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11120/12613 [02:06<00:17, 87.73it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11140/12613 [02:06<00:16, 87.81it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  88% 11160/12613 [02:06<00:16, 87.89it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11180/12613 [02:07<00:16, 87.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11200/12613 [02:07<00:16, 88.04it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11220/12613 [02:07<00:15, 88.12it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11240/12613 [02:07<00:15, 88.20it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11260/12613 [02:07<00:15, 88.28it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  89% 11280/12613 [02:07<00:15, 88.36it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11300/12613 [02:07<00:14, 88.44it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11320/12613 [02:07<00:14, 88.52it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11340/12613 [02:07<00:14, 88.60it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11360/12613 [02:08<00:14, 88.68it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11380/12613 [02:08<00:13, 88.76it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  90% 11400/12613 [02:08<00:13, 88.83it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11420/12613 [02:08<00:13, 88.91it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11440/12613 [02:08<00:13, 88.99it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11460/12613 [02:08<00:12, 89.07it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11480/12613 [02:08<00:12, 89.15it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11500/12613 [02:08<00:12, 89.22it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11520/12613 [02:08<00:12, 89.30it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  91% 11540/12613 [02:09<00:12, 89.38it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11560/12613 [02:09<00:11, 89.46it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11580/12613 [02:09<00:11, 89.54it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11600/12613 [02:09<00:11, 89.62it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11620/12613 [02:09<00:11, 89.70it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11640/12613 [02:09<00:10, 89.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  92% 11660/12613 [02:09<00:10, 89.85it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11680/12613 [02:09<00:10, 89.93it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11700/12613 [02:09<00:10, 90.01it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11720/12613 [02:10<00:09, 90.10it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11740/12613 [02:10<00:09, 90.18it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11760/12613 [02:10<00:09, 90.25it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  93% 11780/12613 [02:10<00:09, 90.33it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11800/12613 [02:10<00:08, 90.40it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11820/12613 [02:10<00:08, 90.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11840/12613 [02:10<00:08, 90.56it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11860/12613 [02:10<00:08, 90.63it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11880/12613 [02:10<00:08, 90.70it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  94% 11900/12613 [02:11<00:07, 90.78it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 11920/12613 [02:11<00:07, 90.85it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 11940/12613 [02:11<00:07, 90.93it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 11960/12613 [02:11<00:07, 91.01it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 11980/12613 [02:11<00:06, 91.08it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 12000/12613 [02:11<00:06, 91.16it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 12020/12613 [02:11<00:06, 91.24it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  95% 12040/12613 [02:11<00:06, 91.32it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12060/12613 [02:11<00:06, 91.40it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12080/12613 [02:12<00:05, 91.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12100/12613 [02:12<00:05, 91.56it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12120/12613 [02:12<00:05, 91.64it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12140/12613 [02:12<00:05, 91.72it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  96% 12160/12613 [02:12<00:04, 91.80it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12180/12613 [02:12<00:04, 91.88it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12200/12613 [02:12<00:04, 91.96it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12220/12613 [02:12<00:04, 92.04it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12240/12613 [02:12<00:04, 92.12it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12260/12613 [02:12<00:03, 92.19it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  97% 12280/12613 [02:13<00:03, 92.27it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12300/12613 [02:13<00:03, 92.35it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12320/12613 [02:13<00:03, 92.43it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12340/12613 [02:13<00:02, 92.51it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12360/12613 [02:13<00:02, 92.59it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12380/12613 [02:13<00:02, 92.66it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12400/12613 [02:13<00:02, 92.73it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  98% 12420/12613 [02:13<00:02, 92.81it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12440/12613 [02:13<00:01, 92.88it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12460/12613 [02:14<00:01, 92.95it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12480/12613 [02:14<00:01, 93.03it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12500/12613 [02:14<00:01, 93.11it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12520/12613 [02:14<00:00, 93.18it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24:  99% 12540/12613 [02:14<00:00, 93.25it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24: 100% 12560/12613 [02:14<00:00, 93.33it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24: 100% 12580/12613 [02:14<00:00, 93.40it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Epoch 24: 100% 12600/12613 [02:14<00:00, 93.48it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "Validation DataLoader 0: 100% 2520/2523 [00:13<00:00, 182.05it/s]\u001b[A\n",
            "Epoch 24: 100% 12613/12613 [02:14<00:00, 93.52it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "                                                                 \u001b[AMonitored metric avg_val_loss did not improve in the last 10 records. Best score: 3.690. Signaling Trainer to stop.\n",
            "Epoch 24: 100% 12613/12613 [02:15<00:00, 93.26it/s, loss=3.68, v_num=56, val_loss=3.690, avg_val_loss=3.690, train_loss=3.690]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 74\n",
            "Shape of the feature representation generated by the base encoder: (10252, 64)\n",
            "end time: 1729478042.7139761\n",
            "Execution time: 0.96 hours\n"
          ]
        }
      ],
      "source": [
        "!python scContrastiveLearning_Main_709_ckpt_epoch.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_200_train_lineage.h5ad\" \\\n",
        "                                              --batch_size 140 \\\n",
        "                                              --size_factor 0.8 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs140_sf08_larry200_train_test_lineage\" \\\n",
        "                                              --train_test 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXLxalfnumD"
      },
      "source": [
        "**5. extract the features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8HHCBIOWxgQ",
        "outputId": "3ae91590-d40a-44ef-9b54-ea94a0755520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "usage: feature_extraction.py [-h] [--inputFilePath INPUTFILEPATH] [--batch_size BATCH_SIZE]\n",
            "                             [--size_factor SIZE_FACTOR] [--temperature TEMPERATURE]\n",
            "                             [--patience PATIENCE] [--min_delta MIN_DELTA] [--max_epoch MAX_EPOCH]\n",
            "                             --output_dir OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                             [--hidden_dims HIDDEN_DIMS] [--embedding_size EMBEDDING_SIZE]\n",
            "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Anndata for running the algorithm\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0: otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGQKLhnAYTs4",
        "outputId": "a85340b6-7702-4cd5-e24f-03268a434349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.10/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "/content/drive/My Drive/Colab Notebooks/scCL/main/scContrastiveLearning_Main_709_ckpt_epoch.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n",
            "Checkpoint /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs140_sf08_larry200_train_test_lineage/saved_models/scContrastiveLearn_last.ckpt was loaded\n",
            "Shape of the feature representation generated by the base encoder: (1121, 64)\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_200_test_lineage.h5ad\" \\\n",
        "                                              --batch_size 140 \\\n",
        "                                              --size_factor 0.8 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs140_sf08_larry200_train_test_lineage\" \\\n",
        "                                              --train_test 1 \\\n",
        "                                              --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_train_test_lineage/feat_1020_bs140_sf08_larry200_train_test_lineage/saved_models/scContrastiveLearn_last.ckpt\"\n",
        "\n",
        "\n",
        "\n",
        "# \"/content/drive/MyDrive/Colab Notebooks/data/Larry_41093_2000_norm_log.h5ad\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}